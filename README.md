# Практикум 3. Анализ данных с PCA, SVD и t-SNE

## Описание
Полная реализация задания по анализу данных с использованием методов снижения размерности PCA, SVD и t-SNE.

## Файлы
- `practicum3.ipynb` - основной Jupyter notebook с решением
- `practicum3_executed.ipynb` - выполненный notebook с результатами
- `test_practicum3.py` - тестовый скрипт для проверки работоспособности
- `images_dataset/` - папка с синтетическими изображениями

## Выполненные задачи

### 1. Генерация переменных
- Создано 10 переменных по заданному алгоритму:
  - x1, x2 - независимые нормально распределенные выборки
  - x3, x4 = 5x1 - 5x2
  - x5, x6 = x1² + x2²
  - x7 = log₁₀(x1)
  - x8 = sin(x2)
  - x9 = 10x3
  - x10 = 10^x7

### 2. Анализ PCA и SVD
- Применены методы PCA и SVD для получения 2 главных компонент
- Определены доли объясненной дисперсии:
  - PCA: 99.97% (исходные данные)
  - SVD: 99.97% (исходные данные)
  - После шкалирования: PCA 78.94%, SVD 96.66%

### 3. Минимаксное шкалирование
- Применено шкалирование (x-min)/(max-min)
- Показано влияние на результаты PCA и SVD

### 4. Работа с изображениями
- Создан синтетический датасет из 100 изображений 64x64
- Созданы классы: поза, настроение, солнцезащитные очки
- Применена нормализация к диапазону [0,1]

### 5. Классификация с PCA
- Найдены оптимальные компоненты для каждого класса (3-20 компонент)
- Использована L1 регуляризация для отбора признаков
- Достигнута точность 100% для позы и настроения

### 6. Анализ с t-SNE
- Применен t-SNE для нелинейного снижения размерности
- Сравнены результаты с PCA
- Получены результаты:
  - Поза: 100% точность
  - Солнцезащитные очки: 70% точность
  - Настроение: 100% точность

## Результаты тестирования
Все тесты успешно пройдены:
- ✅ Генерация переменных
- ✅ PCA и SVD анализ
- ✅ Минимаксное шкалирование
- ✅ Создание и обработка изображений
- ✅ Классификация с PCA
- ✅ Анализ с t-SNE

## Технические детали
- Python 3.13
- Библиотеки: numpy, pandas, matplotlib, seaborn, scikit-learn, PIL
- Использованы методы: PCA, SVD, t-SNE, LogisticRegression с L1 регуляризацией
- Созданы визуализации и сравнительные графики

## Запуск
```bash
# Установка зависимостей
pip install numpy pandas matplotlib seaborn scikit-learn pillow jupyter

# Запуск тестов
python3 test_practicum3.py

# Запуск notebook
jupyter notebook practicum3.ipynb
```

## Выводы
1. PCA и SVD показали высокую эффективность для линейно разделимых данных
2. Минимаксное шкалирование значительно влияет на результаты
3. t-SNE лучше подходит для сложных нелинейных зависимостей
4. L1 регуляризация эффективна для отбора важных признаков
5. Синтетические данные позволили продемонстрировать все методы