# Обзор литературы: параметры, настройки и производительность Code LLMs

**Раздел статьи:** обзор_литературы
**Номер артефакта:** 03

## Краткое описание
Систематизированный обзор источников по параметрам, настройкам и производительности больших языковых моделей для кода (Code LLMs), с протоколом поиска, критериями включения/исключения, проверенными ссылками и фиксацией пробелов.

## Основное содержание

### Исследовательские вопросы
- Какие архитектурные параметры и настройки инференса/обучения существенно влияют на производительность Code LLMs?
- Какие бенчмарки и метрики валидны для сопоставимого сравнения моделей?
- Каковы ограничения существующих сравнений и где наблюдаются пробелы/смещения?

### Протокол поиска (воспроизводимый)
- Базы: arXiv, ACL Anthology, IEEE Xplore, ACM DL, SpringerLink, Google Scholar
- Ключевые слова (EN/RU): "code LLM", "code generation benchmark", HumanEval, MBPP, CodeXGLUE, SWE-bench, "pass@k", "temperature top_p", "hyperparameters", "parameter-efficient fine-tuning", "instruction tuning", "benchmark contamination", «кодогенерация», «бенчмарк», «гиперпараметры»
- Диапазон: 2021–2025; прицельно — актуальные версии/репозитории (последние обновления)
- Критерии включения: эмпирические результаты по code tasks; описание параметров/настроек/метрик; открытые или проверяемые ссылки
- Критерии исключения: доменно не-кодовые задачи; отсутствие воспроизводимых метрик; низкая проверяемость

### Ключевые бенчмарки и метрики (проверка ссылок)
- HumanEval (Chen et al., 2021) — exec-based, pass@k. Репозиторий: `https://github.com/openai/human-eval`
- MBPP (Austin et al., 2021) — Python tasks. Репозиторий: `https://github.com/google-research/google-research/tree/master/mbpp`
- CodeXGLUE (Wang et al., 2021) — многозадачный набор. Страница: `https://microsoft.github.io/CodeXGLUE/`
- SWE-bench (OpenDevin/PR-багфиксы) — реальный контекст PR/issue. Репозиторий: `https://github.com/princeton-nlp/SWE-bench`
- EvalPlus (Liu et al., 2023) — строгие тесты для HumanEval/MBPP. Репозиторий: `https://github.com/evalplus/evalplus`
- MultiPL-E — многоязычная оценка. Репозиторий: `https://github.com/nuprl/MultiPL-E`
- LiveCodeBench — предотвращение контаминации. Репозиторий: `https://github.com/LiveCodeBench/LiveCodeBench`
- Метрики: pass@k, строгая компиляция/исполнение, CodeBLEU/Exact Match (для некоторых задач), время инференса, контекст‑window hit rate

### Ключевые модели/линии (проверка ссылок)
- Code Llama (Meta, 2023) — страница/репозиторий: `https://ai.meta.com/research/publications/code-llama/`
- StarCoder2 (BigCode, 2024) — репозиторий: `https://huggingface.co/bigcode/starcoder2`
- DeepSeek‑Coder (2024) — репозиторий: `https://huggingface.co/deepseek-ai/`
- Qwen2.5‑Coder (2024/2025) — репозиторий: `https://huggingface.co/Qwen`
- Mistral Codestral (2024) — страница: `https://mistral.ai/news/codestral/`
- Open-source оценочные каркасы: `https://github.com/bigcode-project/bigcode-evaluation-harness`

### Представительные исследования (проверка ссылок)
- A Survey of LLMs for Code: Evolution, Benchmarking, and Future Trends. arXiv: `https://arxiv.org/abs/2311.10372`
- Parameter‑Efficient Fine‑Tuning for Large Code Models (SLR). arXiv: `https://arxiv.org/abs/2504.21569`
- Code Comparison Tuning (CCT). arXiv: `https://arxiv.org/abs/2403.19121`
- Hyperparameters for Code Generation. ResearchGate: `https://www.researchgate.net/publication/383266869`
- Assessing Code Reasoning Benchmarks. Preprints: `https://www.preprints.org/manuscript/202411.1147/v1`

### Синтез найденного
- Архитектура и размер модели коррелируют с базовой производительностью, но на кодовых задачах существенны: качество и «чистота» обучающих данных, специализация на коде, и настройки инференса (temperature/top_p/max_new_tokens/tool‑use)
- Бенчмарки различаются по сложности и риску контаминации; EvalPlus/LiveCodeBench уменьшают эффект утечек, SWE‑bench проверяет «инженерные» сценарии
- Заметна чувствительность к гиперпараметрам инференса; корректная отчётность настроек необходима для сопоставимости
- Параметр‑эффективные методы дообучения (LoRA/PEFT, instruction/code tuning) дают значимый прирост при умеренных ресурсах

### Факты vs Гипотезы
**Факты:**
- Существуют устоявшиеся бенчмарки (HumanEval/MBPP/CodeXGLUE) и новые более строгие (EvalPlus, LiveCodeBench, SWE‑bench)
- Параметры инференса существенно влияют на pass@k и стабильность результатов
- Параметр‑эффективные методы дообучения дают улучшения на кодовых задачах

**Гипотезы (для дальнейшей проверки в работе):**
- Влияние настроек инференса сопоставимо по вкладу с ростом размера модели для средних размеров
- Комбинированные признаки «параметры модели + инференс‑настройки + свойства датасета обучения» образуют устойчивые кластеры производительности

### Методологические обоснования
- Для сопоставимости необходимо фиксировать/протоколировать: версии датасетов/скриптов оценки, семена, температуры/top_p, max tokens, policy на повторные попытки
- Рекомендуется отчётность по «контаминации» и проверка на расширенных тестах (EvalPlus/LiveCodeBench)

### Практическая значимость
- Обзор формирует базу для выбора моделей/настроек и корректной методологии сравнения в нашем сравнительном исследовании

## Проверка качества
- [x] Все источники проверены на доступность
- [x] Факты отделены от гипотез
- [x] Цитирования оформлены корректно (идентификаторы/репозитории)
- [x] Структура и язык соответствуют академическим стандартам
- [x] Использован воспроизводимый протокол поиска

## Дата создания/обновления
2025-09-29 — Первичная версия систематического обзора с проверенными ссылками


