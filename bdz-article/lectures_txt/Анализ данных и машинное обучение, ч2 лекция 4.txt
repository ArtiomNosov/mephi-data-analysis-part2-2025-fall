
================================================================================
ФАЙЛ: Анализ данных и машинное обучение, ч2 лекция 4
ИСТОЧНИК: /Users/23108022/Documents/repositories/mephi-data-analysis-part2-2025-fall/bdz-article/lectures/Анализ данных и машинное обучение, ч2 лекция 4.pdf
КОНВЕРТИРОВАНО: pypdf
================================================================================

--- СТРАНИЦА 1 ---
Анализ данных и машинное 
обучение, ч. 2
Лекция 4. Обучение без учителя. Факторный анализ. Метод главных 
компонент. Диагностика и оценка результатов
Киреев В.С.,
к.т.н., доцент
Москва, 2025

--- СТРАНИЦА 2 ---
Обучение без учителя. Методы сокращения 
размерности. Извлечение признаков
Разложение 
матрицы
PCA SVD
Построение графа 
соседства
TSNE UMAP

--- СТРАНИЦА 3 ---
Метод сингулярного разложения (SVD)
Метод сингулярного разложения (SVD)— это подход к разложению матриц, 
который помогает сократить матрицу путем обобщения собственного 
разложения квадратной матрицы (с одинаковым количеством столбцов и 
строк) на любую матрицу. 
SVD широко используется как при вычислении других матричных операций, 
таких как обращение матрицы, так и в качестве метода сокращения данных в 
машинном обучении. SVD также можно использовать для линейной регрессии 
методом наименьших квадратов, сжатия изображений и шумоподавления 
данных.

--- СТРАНИЦА 4 ---
SVD. Пример на Python
import numpy as np
from scipy.linalg import svd
# Создадим случайную матрицу
np.random.seed(42)
A = np.random.randn(5, 3) * 10
print("Исходная матрица A:\n", A)
# Вычисляем SVD
U, s, Vt = svd(A)
print("\nЛевые сингулярные векторы U:\n", U)
print("\nСингулярные значения s:\n", s)
print("\nПравые сингулярные векторы V^T:\n", Vt)
# Проверка восстановления
Sigma = np.zeros((A.shape[0], A.shape[1]))
np.fill_diagonal(Sigma, s)
A_reconstructed = U @ Sigma @ Vt
print("\nВосстановленная матрица:\n", A_reconstructed)
print("\nОшибка восстановления:", np.linalg.norm(A - A_reconstructed))

--- СТРАНИЦА 5 ---

\1

\1Столбцы 

\1Столбцы - это нагрузки;

\1Столбцы - это главные оси (также известные как главные направления, также 
известные как собственные векторы).

--- СТРАНИЦА 15 ---
Двумерные диаграммы, биплоты (biplots). 
Отображение исходных переменных. Пример

--- СТРАНИЦА 16 ---
Биплот для набора Ирисы Фишера. Python
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# Данные Iris для примера
from sklearn.datasets import load_iris
data = load_iris()
X = data.data
y = data.target
# Нормализация
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
# Визуализация объектов
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=80)
# Визуализация переменных (стрелки)
for i, feature in enumerate(data.feature_names):
plt.arrow(0, 0, pca.components_[0, i]*3, pca.components_[1, i]*3,
color='red', width=0.02, head_width=0.1)
plt.text(pca.components_[0, i]*3.2, pca.components_[1, i]*3.2, feature,
color='red', fontsize=10)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Биплот PCA для набора Iris')
plt.colorbar(scatter, label='Класс')
plt.grid(True)
plt.show()

--- СТРАНИЦА 17 ---
Биплот для набора Ирисы Фишера

--- СТРАНИЦА 18 ---
SVD. Пример выбора разных рангов

--- СТРАНИЦА 19 ---
Применение SVD. Эффективная инверсия 
матриц
Разложение по сингулярным числам может значительно упростить 
инверсию матриц для некоторых конкретных матричных структур. 
Это полезно в алгоритмах, где требуется повторяющаяся инверсия 
с небольшими изменениями в матричной структуре.

--- СТРАНИЦА 20 ---
Применение SVD. Сжатие данных
Разложение по сингулярным числам может выявить наиболее значимые 
линейные компоненты (направления, подпространства и т. д.), содержащие 
наибольшее количество информации. Это имеет очевидное применение в 
сжатии данных. При использовании PCA в сжатии изображений, PCA 
использует SVD внутри для расчета компонентов с наибольшей дисперсией.

--- СТРАНИЦА 21 ---
Применение SVD. Шумоподавление данных
Подобно сжатию данных, для удаления шума из данных 
можно использовать разложение по сингулярным числам 
с удалением шума.

--- СТРАНИЦА 22 ---
Применение SVD. Обработка естественного 
языка
SVD применяется во многих процессах с большим 
количеством линейной алгебры, таких как искусственный 
интеллект, машинное обучение, LSA, совместная 
фильтрация и обработка естественного языка.

--- СТРАНИЦА 23 ---
Применение SVD. Обработка естественного 
языка
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
documents = [
"I love machine learning",
"Machine learning is fun",
"Python is great for data science",
"Data science and machine learning are related"
]
# TF-IDF
vectorizer = TfidfVectorizer(stop_words='english')
X_tfidf = vectorizer.fit_transform(documents)
# SVD (TruncatedSVD — для больших матриц)
svd = TruncatedSVD(n_components=2, random_state=42)
X_svd = svd.fit_transform(X_tfidf)
# Визуализация
plt.scatter(X_svd[:, 0], X_svd[:, 1])
for i, doc in enumerate(documents):
plt.text(X_svd[i, 0], X_svd[i, 1], f'Doc {i+1}', fontsize=9)
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.title('LSA: Documents in Latent Space')
plt.grid(True)
plt.show()

--- СТРАНИЦА 24 ---
Применение SVD. Обработка естественного 
языка

--- СТРАНИЦА 25 ---
Связь SVD и PCA
Аспект PCA SVD
Вход Центрированная матрица Любая матрица
Основа Ковариационная матрица Разложение матрицы
Главные компоненты Собственные векторы Столбцы U × \Sigma 
Нагрузки Собственные векторы Столбцы V 

--- СТРАНИЦА 26 ---
Спасибо за внимание!