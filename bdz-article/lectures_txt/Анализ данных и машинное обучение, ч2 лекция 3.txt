
================================================================================
ФАЙЛ: Анализ данных и машинное обучение, ч2 лекция 3
ИСТОЧНИК: /Users/23108022/Documents/repositories/mephi-data-analysis-part2-2025-fall/bdz-article/lectures/Анализ данных и машинное обучение, ч2 лекция 3.pdf
КОНВЕРТИРОВАНО: pypdf
================================================================================

--- СТРАНИЦА 1 ---

\1

\1отбор признаков (фич) – в этом случае применяются 
статистические тесты, чтобы сделать вывод о важности признаков и 
отобрать какое-либо их подмножество. Подразумевает потерю 
информации и меньшую стабильность.

\1извлечение признаков (фич) – в этом случае создаются новых 
независимые признаки в виде комбинации исходных признаков. 
Применяются линейные и нелинейные методологии.

--- СТРАНИЦА 7 ---
Обучение без учителя. Методы сокращения 
размерности. Извлечение признаков
Разложение 
матрицы
PCA SVD
Построение графа 
соседства
TSNE UMAP

--- СТРАНИЦА 8 ---
Задача сокращения размерности. 
Разложение матрицы

--- СТРАНИЦА 9 ---
Метод главных компонент (PCA)
Метод главных компонент (МГК) применяется для снижения размерности 
пространства наблюдаемых векторов, не приводя к существенной потере 
информативности. Предпосылкой МГК является нормальный закон 
распределения многомерных векторов. 
В МГК линейные комбинации случайных величин определяются 
характеристическими векторами ковариационной матрицы. Главные 
компоненты представляют собой ортогональную систему координат, в 
которой дисперсии компонент характеризуют их статистические свойства. 

--- СТРАНИЦА 10 ---
Метод главных компонент (PCA). 
Принципиальная схема работы

--- СТРАНИЦА 11 ---
Метод главных компонент (PCA). Схема 
матриц

--- СТРАНИЦА 12 ---
Метод главных компонент (PCA). Пример. 
Ирисы Фишера

--- СТРАНИЦА 13 ---
Метод главных компонент (PCA). Матрица 
ковариации

--- СТРАНИЦА 14 ---
Метод главных компонент (PCA). Пример. 
Ирисы Фишера. Расчет матрицы ковариации

--- СТРАНИЦА 15 ---
Метод главных компонент (PCA). Собственные 
вектора и собственные значения

--- СТРАНИЦА 16 ---
Метод главных компонент (PCA). Пример. 
Ирисы Фишера. Расчет собственных значений

--- СТРАНИЦА 17 ---
Матрица нагрузок(loadings matrix)
Матрица нагрузок P – это матрица перехода из исходного пространства 
переменных x1, … xJ (J-мерного) в пространство главных компонент (A-
мерное). Каждая строка матрицы P состоит из коэффициентов, связывающих 
переменные t и x.
Например, a-я строка – это проекция всех переменных x1, …xJ на a-ю ось 
главных компонент. Каждый столбец P – это проекция соответствующей 
переменной xj на новую систему координат. 

--- СТРАНИЦА 18 ---
Метод главных компонент (PCA). Пример. 
Ирисы Фишера. Расчет матрицы нагрузок

--- СТРАНИЦА 19 ---
Метод главных компонент (PCA). Пример. 
Ирисы Фишера. Визуализация нагрузок

--- СТРАНИЦА 20 ---
Матрица счетов(score matrix)
Матрица счетов T дает нам проекции исходных образцов (J –мерных 
векторов x1,…,xI) на подпространство главных компонент (A-мерное). 
Строки t1,…,tI матрицы T – это координаты образцов в новой системе 
координат. Столбцы t1,…,tA матрицы T – ортогональны и представляют 
проекции всех образцов на одну новую координатную ось.

--- СТРАНИЦА 21 ---
Метод главных компонент (PCA). Пример. 
Ирисы Фишера. Расчет матрицы счетов

--- СТРАНИЦА 22 ---
Отбор главных компонент. График «осыпи» 
(scree plot)
Можно показать, что дисперсия главной компоненты равна соответствующему 
собственному значению. Более того, поскольку след корреляционной 
матрицы равен сумме ее собственных значений и, очевидно, равен p - 
количеству переменных, а кроме того, главные компоненты 
некоррелированы, то сразу следует, что доля общей дисперсии которая 
объясняется j-той компонентой — это просто λj/p.
График «осыпи» представляет собой график, показывающий объясненную 
дисперсию для вновь определенной компоненты (главной компоненты). 
Мерой графика может быть процент или абсолютное значение объясненной 
дисперсии (собственные значения). 

--- СТРАНИЦА 23 ---
Метод главных компонент (PCA). Пример. 
Ирисы Фишера. График «осыпи»

--- СТРАНИЦА 24 ---
Отбор главных компонент. Критерий Кайзера
Критерий Кайзера-Гуттмана гласит, что компоненты, основанные на 
собственных значениях больше 1, должны быть отобраны. Это 
основано на представлении о том, что, поскольку сумма 
собственных значений равна p, собственное значение больше 1 
представляет компоненту «выше среднего».

--- СТРАНИЦА 25 ---
Метод главных компонент (PCA). Пример. 
Ирисы Фишера. Визуализация

--- СТРАНИЦА 26 ---
Особенности метода главных компонент

\1nel trick).

\1Второй недостаток метода главных компонент состоит в том, что направления, максимизирующие 
дисперсию, далеко не всегда максимизируют информативность. Например, переменная с 
максимальной дисперсией может не нести почти никакой информации, в то время как 
переменная с минимальной дисперсией позволяет полностью разделить классы. Метод главных 
компонент в данном случае отдаст предпочтение первой (менее информативной) переменной. 
Вся дополнительная информация, связанная с вектором (например, принадлежность образа к 
одному из классов), игнорируется.

--- СТРАНИЦА 28 ---
Вращение системы компонент (факторов)

\1ncome Доход: в тысячах евро в год
Beer Пиво: потребление в литрах в год
Wine Вино: потребление в литрах в год
Sex Пол: мужской: –1, или женский: +1
Strength Сила: индекс, основанный на 
проверке физических способностей
Region Регион: север : –1, или юг: +1
IQ Коэффициент интеллекта, 
измеряемый по стандартному тесту
Пример данных для МГК. Обозначения

--- СТРАНИЦА 35 ---

\1