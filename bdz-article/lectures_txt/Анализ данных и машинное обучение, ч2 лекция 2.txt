
================================================================================
ФАЙЛ: Анализ данных и машинное обучение, ч2 лекция 2
ИСТОЧНИК: /Users/23108022/Documents/repositories/mephi-data-analysis-part2-2025-fall/bdz-article/lectures/Анализ данных и машинное обучение, ч2 лекция 2.pdf
КОНВЕРТИРОВАНО: pypdf
================================================================================

--- СТРАНИЦА 1 ---
Анализ данных и машинное 
обучение, ч. 2
Лекция 2. Трансформация данных. Квантование. Кодирование.Анализ аномалий. 
Понятие и классификация аномалий. LOF, Isolation Forest. ZSCORE, IQR. HBOS. 
Восстановление пропущенных значений. Винсоризация
Киреев В.С.,
к.т.н., доцент
Москва, 2025

--- СТРАНИЦА 2 ---
Трансформация данных
Трансформация данных – комплекс методов и алгоритмов, 
направленных на оптимизацию представления и форматов данных 
с точки зрения решаемых задач и целей анализа. Трансформация 
данных не ставит целью изменить информационное содержание 
данных. Её задача представить эту информацию в таком виде, 
чтобы она могла быть использована наиболее эффективно.

--- СТРАНИЦА 3 ---

\1nks natural breaks 
optimization) делит данные на группы (кластеры) таким образом, 
чтобы минимизировать отклонение наблюдений от среднего 
каждого класса (дисперсию внутри классов) и максимизировать 
отклонение среднего каждого класса от среднего других классов 
(дисперсию между классами).

--- СТРАНИЦА 13 ---
Алгоритм Дженкса. Пример

--- СТРАНИЦА 14 ---
Особенности категориальных признаков

\1С категориальными признаками можно столкнуться с проблемами с 
редкими метками, категориями /группами, которые крайне редки в 
наборе данных. Эта проблема часто связана с функциями, имеющими 
высокую мощность — другими словами, с множеством различных 
категорий.

\1Наличие слишком большого количества категорий, и особенно редких 
категорий, приводит к зашумленному набору данных. Алгоритму ML 
может быть трудно пробиться сквозь этот шум и извлечь уроки из 
более значимых сигналов в данных.

\1Высокая кардинальность также может усугубить проклятие 
размерности, если необходимо однократно закодировать свои 
категориальные характеристики. 

--- СТРАНИЦА 15 ---
Кодирование категориальных признаков. 
One Hot Encoding
Г орячее кодирование (One Hot Encoding) используется для 
преобразования категориальных переменных в формат, который 
может быть легко использован алгоритмами машинного обучения .
Основная идея горячего кодирования заключается в создании 
новых переменных, которые принимают значения 0 и 1 для 
представления исходных категориальных значений.

--- СТРАНИЦА 16 ---
Кодирование категориальных признаков. 
One Hot Encoding. Пример

--- СТРАНИЦА 17 ---
Кодирование категориальных признаков. 
Label Encoding
Label Encoder (кодирование меткой) очень прост и включает 
преобразование каждого значения признака в число. Label-Encoder, 
стоит применять когда категориальный признак является 
порядковым (например, низкий, средний, высокий) , и количество 
категорий довольно велико.

--- СТРАНИЦА 18 ---
Кодирование категориальных признаков. 
Label Encoding. Пример

--- СТРАНИЦА 19 ---

\1

\1Точечные (point anomalies)

\1Контекстуальные (contextual)

\1Коллективные (collective)

--- СТРАНИЦА 23 ---

\1

\1Статистические методы — основаны на предположении о распределении данных (нормальное, экспоненциальное 
и т.д.)

\1Z-score, Grubbs’ test и др.

\1Методы, основанные на плотности и расстоянии — используют локальную структуру данных

\1LOF, kNN и др.

\1Методы, основанные на деревьях и ансамблях — строят модели для изоляции или классификации аномалий

\1Isolation Forest, HBOS и др.

\1Машинное обучение с учителем и без 

\1автоэнкодеры, SVM, GAN и др.

--- СТРАНИЦА 25 ---
Метод межквартильного размаха
Метод межквартильного размаха (Interquartile Range, IQR) 
используется для измерения изменчивости путем деления набора 
данных на квартили. 
IQR – это диапазон между первым и третьим квартилями, а именно 
Q1 и Q3: IQR = Q3 - Q1. Точки данных, которые находятся ниже Q1 – 
1,5 IQR или выше Q3 + 1,5 IQR, являются выбросами.

--- СТРАНИЦА 26 ---

\1

\1Вычислительно затратен — O(n2) для поиска всех kNN (можно ускорить через KD-Tree или 
Ball Tree — O(nlogn) в среднем).

\1Чувствителен к выбору k — слишком маленькое k → шум воспринимается как аномалия; 
слишком большое → “размывает” локальные особенности.

\1Трудно интерпретировать абсолютные значения — LOF не даёт вероятности, только 
относительную аномальность.

\1Не масштабируется на очень большие данные без приближённых методов/

--- СТРАНИЦА 33 ---
Аномальные значения. Обнаружение. 
Фактор локального выброса. Пример

--- СТРАНИЦА 34 ---
Isolation Forest

\1

\1n Forest. Характеристики

\1

\1Плохо работает при наличии категориальных признаков

\1Не учитывает корреляции

\1Требует настройки глубины и числа деревьев

--- СТРАНИЦА 36 ---
Аномальные значения. Обнаружение. 
Изолирующий лес. Пример

--- СТРАНИЦА 37 ---
Сравнительный анализ методов
Метод Скорость Масштабируемость Учет 
корреляций Интерпретируемость Чувствительность 
к шуму
Z-score Очень 
высокая Низкая (1D) Нет Высокая Высокая
HBOS Очень 
высокая Высокая Нет Средняя Средняя
LOF Низкая Низкая Да Низкая Низкая
Isolation Forest Высокая Высокая Нет Средняя Низкая

--- СТРАНИЦА 38 ---

\1

\1Комбинация iForest + LOF для улучшения качества

\1Ансамбли детекторов (например, Feature Bagging)

\1

\1Autoencoders — аномалии имеют высокую ошибку реконструкции

\1GANomaly, Deep SVDD — специализированные архитектуры

\1

\1Обнаружение аномалий во временных рядах (LSTM-AE, USAD)

\1Интерпретируемость (SHAP , LIME для аномалий)

--- СТРАНИЦА 39 ---
Аномальные значения. Обнаружение. 
Робастная ковариация
Для независимых от гаусса объектов можно использовать простые 
статистические методы для обнаружения аномалий в наборе 
данных. Для гауссовского/нормального распределения точки 
данных, лежащие в стороне от 3-го отклонения, можно 
рассматривать как аномалии. 
Для набора данных, имеющего все признаки гауссовой природы, 
статистический подход может быть обобщен путем определения 
эллиптической гиперсферы, которая охватывает большинство 
обычных точек данных, а точки данных, лежащие вдали от 
гиперсферы, можно рассматривать как аномалии.

--- СТРАНИЦА 40 ---
Аномальные значения. Обнаружение. 
Робастная ковариация. Пример

--- СТРАНИЦА 41 ---
Аномальные значения. Обнаружение. 
Метод SVM
Базовый алгоритм SVM пытается найти гиперплоскость, которая 
наилучшим образом разделяет два класса точек данных. Для SVM 
одного класса, где у нас есть один класс точек данных, и задача 
состоит в том, чтобы предсказать гиперсферу, которая отделяет 
кластер точек данных от аномалий.

--- СТРАНИЦА 42 ---
Аномальные значения. Обнаружение. 
Метод SVM. Пример

--- СТРАНИЦА 43 ---
Пропущенные значения 
Пропущенные значения могут возникать, когда не предоставляется 
информация по одному или нескольким элементам или по целому 
подразделению. Пропущенные значения - очень большая проблема в 
реальных сценариях.

\1В Pandas Пропущенные значения представлены двумя значениями: 
None - это одноэлементный объект Python, который часто 
используется для обозначения отсутствующих данных в коде Python. 

\1NaN (аббревиатура от "Не число") - это специальное значение с 
плавающей запятой, распознаваемое всеми системами, 
использующими стандартное представление IEEE с плавающей 
запятой

--- СТРАНИЦА 44 ---
Пропущенные значения. Обнаружение в 
Pandas 

--- СТРАНИЦА 45 ---
Пропущенные значения. Восстановление в 
Pandas 

--- СТРАНИЦА 46 ---
Спасибо за внимание!