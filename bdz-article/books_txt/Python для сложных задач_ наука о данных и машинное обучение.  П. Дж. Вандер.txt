
================================================================================
ФАЙЛ: Python для сложных задач_ наука о данных и машинное обучение.  П. Дж. Вандер
ИСТОЧНИК: /Users/23108022/Documents/repositories/mephi-data-analysis-part2-2025-fall/bdz-article/books/Python для сложных задач_ наука о данных и машинное обучение.  П. Дж. Вандер.pdf
КОНВЕРТИРОВАНО: pypdf
================================================================================

--- СТРАНИЦА 4 ---
ББК 32.973.2-018.1
УДК 004.43
П37
	 Плас	Дж.	Вандер
П37 Python для сложных задач: наука о данных и машинное обучение. — СПб.: Пи-
тер, 2018. — 576 с.: ил. — (Серия «Бестселлеры O’Reilly»).
 ISBN 978-5-496-03068-7
Книга «Python для сложных задач: наука о данных и машинное обучение» — это подробное ру -
ководство по самым разным вычислительным и статистическим мето дам, без которых немыслима 
любая интенсивная обработка данных, научные исследования и передовые разработки. Читатели, 
уже имеющие опыт программирования и желающие эффективно использовать Python в сфере Data 
Science, найдут в этой книге ответы на всевозможные вопросы, например: как считать этот формат 
данных в скрипт? как преобразовать, очистить эти данные и манипулировать ими? как визуализиро-
вать данные такого типа? как при помощи этих данных разобраться в ситуации, получить ответы на 
вопросы, построить статистические модели или реализовать машинное обучение?
16+ (В соответствии с Федеральным законом от 29 декабря 2010 г. № 436-ФЗ.)
 ББК 32.973.2-018.1
 УДК 004.43
Права на издание получены по соглашению с O’Reilly. Все права защищены. Никакая часть данной книги не 
может быть воспроизведена в какой бы то ни было форме без письменного разрешения владельцев авторских 
прав.
Информация, содержащаяся в данной книге, получена из источников, рассматриваемых издательством как на-
дежные. Тем не менее, имея в виду возможные человеческие или технические ошибки, издательство не может 
гарантировать абсолютную точность и полноту приводимых сведений и не несет ответственности за возможные 
ошибки, связанные с использованием книги.
ISBN 978-1491912058 англ. Authorized Russian translation of the English edition of Python Data Science 
 Handbook, ISBN 9781491912058 © 2017 Jake VanderPlas
ISBN 978-5-496-03068-7 © Перевод на русский язык ООО Издательство «Питер», 2018
 © Издание на русском языке, оформление ООО Издательство «Питер», 2018
 © Серия «Бестселлеры O’Reilly», 2018

--- СТРАНИЦА 5 ---
Оглавление
Предисловие ....................................................................................................... 16
Что такое наука о данных ......................................................................................16
Для кого предназначена эта книга ........................................................................17
Почему Python .......................................................................................................18
Общая структура книги.......................................................................................... 19
Использование примеров кода ..............................................................................19
Вопросы установки ................................................................................................20
Условные обозначения ..........................................................................................21
Глава 1. IPython: за пределами обычного Python .................................. 22
Командная строка или блокнот? ............................................................................23
Запуск командной оболочки IPython ................................................................23
Запуск блокнота Jupiter ...................................................................................23
Справка и документация в оболочке IPython .........................................................24
Доступ к документации с помощью символа ? .................................................25
Доступ к исходному коду с помощью символов ?? ...........................................27
Просмотр содержимого модулей с помощью 
Tab-автодополнения ........................................................................................28
Сочетания горячих клавиш в командной оболочке IPython ....................................30
Навигационные горячие клавиши ....................................................................31
Горячие клавиши ввода текста ........................................................................31
Горячие клавиши для истории команд .............................................................32
Прочие горячие клавиши ................................................................................33

--- СТРАНИЦА 6 ---

\1n .............................................................................33
Вставка блоков кода: %paste и %cpaste ..........................................................34
Выполнение внешнего кода: %run ..................................................................35
Длительность выполнения кода: %timeit .........................................................36
Справка по «магическим» функциям: ?, %magic и %lsmagic ...........................36
История ввода и вывода ........................................................................................37
Объекты In и Out оболочки IPython .................................................................37
Быстрый доступ к предыдущим выводам с помощью знака подчеркивания .....38
Подавление вывода .........................................................................................39
Соответствующие «магические» команды .......................................................39
Оболочка IPython и использование системного командного процессора ................40
Краткое введение в использование командного процессора ............................40
Инструкции командного процессора в оболочке IPython..................................42
Передача значений в командный процессор и из него.....................................42
«Магические» команды для командного процессора ..............................................43
Ошибки и отладка .................................................................................................44
Управление исключениями: %xmode ..............................................................44
Отладка: что делать, если чтения трассировок недостаточно .........................47
Профилирование и мониторинг скорости выполнения кода ...................................49
Оценка времени выполнения фрагментов кода: %timeit и %time ....................50
Профилирование сценариев целиком: %prun ..................................................52
Пошаговое профилирование с помощью %lprun ..............................................53
Профилирование использования памяти: %memit и %mprun ..........................54
Дополнительные источники информации об оболочке IPython ..............................56
Веб-ресурсы ....................................................................................................56
Книги ..............................................................................................................56
Глава 2. Введение в библиотеку NumPy .................................................... 58
Работа с типами данных в языке Python ................................................................59
Целое число в языке Python — больше, чем просто целое число .....................60
Список в языке Python — больше, чем просто список ......................................62
Массивы с фиксированным типом в языке Python ............................................63

--- СТРАНИЦА 7 ---
Оглавление 7
Создание массивов из списков языка Python ...................................................64
Создание массивов с нуля ...............................................................................65
Стандартные типы данных библиотеки NumPy ................................................66
Введение в массивы библиотеки NumPy ................................................................67
Атрибуты массивов библиотеки NumPy ...........................................................68
Индексация массива: доступ к отдельным элементам ......................................69
Срезы массивов: доступ к подмассивам ...........................................................70
Изменение формы массивов ............................................................................74
Слияние и разбиение массивов .......................................................................75
Выполнение вычислений над массивами библиотеки NumPy: 
универсальные функции ........................................................................................77
Медлительность циклов ..................................................................................77
Введение в универсальные функции ...............................................................79
Обзор универсальных функций библиотеки NumPy .........................................80
Продвинутые возможности универсальных функций .......................................84
Универсальные функции: дальнейшая информация ........................................86
Агрегирование: минимум, максимум и все, что посередине ...................................86
Суммирование значений из массива ................................................................87
Минимум и максимум ......................................................................................87
Пример: чему равен средний рост президентов США ......................................90
Операции над массивами. Транслирование ...........................................................91
Введение в транслирование ............................................................................92
Правила транслирования ................................................................................94
Транслирование на практике ..........................................................................97
Сравнения, маски и булева логика ........................................................................98
Пример: подсчет количества дождливых дней ................................................98
Операторы сравнения как универсальные функции .......................................100
Работа с булевыми массивами .......................................................................102
Булевы массивы как маски ............................................................................104
«Прихотливая» индексация .................................................................................108
Исследуем возможности «прихотливой» индексации .....................................108

--- СТРАНИЦА 8 ---

\1np.sort и np.argsort ........117
Частичные сортировки: секционирование .....................................................118
Пример: K ближайших соседей ......................................................................119
Структурированные данные: структурированные массивы 
библиотеки NumPy ..............................................................................................123
Создание структурированных массивов .........................................................125
Более продвинутые типы данных ..................................................................126
Массивы записей: структурированные массивы 
с дополнительными возможностями ..............................................................127
Вперед, к Pandas ...........................................................................................128
Глава 3. Манипуляции над данными 
с помощью пакета Pandas ............................................................................ 129
Установка и использование библиотеки Pandas ...................................................130
Знакомство с объектами библиотеки Pandas ........................................................131
Объект Series библиотеки Pandas ..................................................................131
Объект DataFrame библиотеки Pandas ...........................................................135
Объект Index библиотеки Pandas ...................................................................138
Индексация и выборка данных ............................................................................140
Выборка данных из объекта Series ................................................................140
Выборка данных из объекта DataFrame .........................................................144
Операции над данными в библиотеке Pandas ...................................................... 149
Универсальные функции: сохранение индекса ..............................................149
Универсальные функции: выравнивание индексов ........................................150
Универсальные функции: выполнение операции 
между объектами DataFrame и Series .............................................................153
Обработка отсутствующих данных .......................................................................154
Компромиссы при обозначении отсутствующих данных .................................155

--- СТРАНИЦА 9 ---
Оглавление 9
Отсутствующие данные в библиотеке Pandas ................................................155
Операции над пустыми значениями ...............................................................159
Иерархическая индексация ..................................................................................164
Мультииндексированный объект Series.......................................................... 164
Методы создания мультииндексов .................................................................168
Индексация и срезы по мультииндексу ..........................................................171
Перегруппировка мультииндексов .................................................................174
Агрегирование по мультииндексам ................................................................177
Объединение наборов данных: конкатенация и добавление в конец ...................178
Напоминание: конкатенация массивов NumPy ...............................................179
Простая конкатенация с помощью метода pd.concat ......................................180
Объединение наборов данных: слияние и соединение .........................................184
Реляционная алгебра ....................................................................................184
Виды соединений ..........................................................................................185
Задание ключа слияния .................................................................................187
Задание операций над множествами для соединений ....................................191
Пересекающиеся названия столбцов: ключевое слово suffixes ......................192
Пример: данные по штатам США ...................................................................193
Агрегирование и группировка ..............................................................................197
Данные о планетах ........................................................................................198
Простое агрегирование в библиотеке Pandas ................................................198
GroupBy: разбиение, применение, объединение ............................................200
Сводные таблицы ................................................................................................210
Данные для примеров работы со сводными таблицами .................................210
Сводные таблицы «вручную» ........................................................................211
Синтаксис сводных таблиц ............................................................................212
Пример: данные о рождаемости ....................................................................214
Векторизованные операции над строками ........................................................... 219
Знакомство со строковыми операциями библиотеки Pandas .......................... 219
Таблицы методов работы со строками библиотеки Pandas............................. 221
Пример: база данных рецептов .....................................................................226
Работа с временными рядами ..............................................................................230
Дата и время в языке Python .........................................................................231

--- СТРАНИЦА 10 ---

\1ndas: индексация по времени .......................235
Структуры данных для временных рядов библиотеки Pandas ........................ 235
Периодичность и смещения дат..................................................................... 238
Где найти дополнительную информацию ......................................................246
Пример: визуализация количества велосипедов в Сиэтле .............................246
Увеличение производительности библиотеки Pandas: eval() и query() ................. 252
Основания для использования функций query() и eval(): 
составные выражения ...................................................................................254
Использование функции pandas.eval() для эффективных операций ...............255
Использование метода DataFrame.eval() для выполнения операций 
по столбцам ..................................................................................................257
Метод DataFrame.query() ...............................................................................259
Производительность: когда следует использовать эти функции ....................259
Дополнительные источники информации ............................................................ 260
Глава 4. Визуализация с помощью библиотеки Matplotlib ................ 262
Общие советы по библиотеке Matplotlib ...............................................................263
Импорт matplotlib ..........................................................................................263
Настройка стилей ..........................................................................................263
Использовать show() или не использовать? 
Как отображать свои графики .......................................................................264
Сохранение рисунков в файл ........................................................................266
Два интерфейса по цене одного ..........................................................................267
Интерфейс в стиле MATLAB ...........................................................................267
Объектно-ориентированный интерфейс ........................................................268
Простые линейные графики ................................................................................ 269
Настройка графика: цвета и стили линий ......................................................271
Настройка графика: пределы осей координат ...............................................273
Метки на графиках ........................................................................................276
Простые диаграммы рассеяния ............................................................................278
Построение диаграмм рассеяния с помощью функции plt.plot ........................279
Построение диаграмм рассеяния с помощью функции plt.scatter ...................281

--- СТРАНИЦА 11 ---
Оглавление 1 1
Сравнение функций plot и scatter: примечание относительно 
производительности ......................................................................................283
Визуализация погрешностей ................................................................................283
Простые планки погрешностей ......................................................................283
Непрерывные погрешности ...........................................................................285
Графики плотности и контурные графики ............................................................286
Гистограммы, разбиения по интервалам и плотность .......................................... 290
Двумерные гистограммы и разбиения по интервалам ....................................292
Ядерная оценка плотности распределения ....................................................294
Пользовательские настройки легенд на графиках ............................................... 295
Выбор элементов для легенды ......................................................................297
Задание легенды для точек различного размера ...........................................298
Отображение нескольких легенд ...................................................................300
Пользовательские настройки шкал цветов ...........................................................301
Выбор карты цветов ......................................................................................302
Ограничения и расширенные возможности по использованию цветов ...........305
Дискретные шкалы цветов ............................................................................306
Пример: рукописные цифры ..........................................................................306
Множественные субграфики ................................................................................308
plt.axes: создание субграфиков вручную .......................................................309
plt.subplot: простые сетки субграфиков .........................................................310
Функция plt.subplots: создание всей сетки за один раз ..................................311
Функция plt.GridSpec: более сложные конфигурации .....................................313
Текст и поясняющие надписи ..............................................................................314
Пример: влияние выходных дней на рождение детей в США .........................315
Преобразования и координаты текста ...........................................................317
Стрелки и поясняющие надписи ....................................................................319
Пользовательские настройки делений на осях координат ....................................321
Основные и промежуточные деления осей координат ...................................322
Прячем деления и/или метки ........................................................................323
Уменьшение или увеличение количества делений .........................................324
Более экзотические форматы делений ..........................................................325

--- СТРАНИЦА 12 ---

\1n .................................................... 357
Seaborn по сравнению с Matplotlib .................................................................358
Анализируем графики библиотеки Seaborn ................................................... 360
Пример: время прохождения марафона ........................................................368
Дополнительные источники информации ............................................................ 377
Источники информации о библиотеке Matplotlib ............................................377
Другие графические библиотеки языка Python ..............................................377
Глава 5. Машинное обучение ...................................................................... 379
Что такое машинное обучение .............................................................................380
Категории машинного обучения ....................................................................380
Качественные примеры прикладных задач машинного обучения ...................381
Классификация: предсказание дискретных меток ..........................................381
Резюме ..........................................................................................................390
Знакомство с библиотекой Scikit-Learn .................................................................391
Представление данных в Scikit-Learn .............................................................391
API статистического оценивания библиотеки Scikit-Learn ..............................394

--- СТРАНИЦА 13 ---
Оглавление 1 3
Прикладная задача: анализ рукописных цифр ...............................................403
Резюме ..........................................................................................................408
Гиперпараметры и проверка модели ................................................................... 408
Соображения относительно проверки модели ...............................................409
Выбор оптимальной модели ..........................................................................413
Кривые обучения ..........................................................................................420
Проверка на практике: поиск по сетке ..........................................................425
Резюме ..........................................................................................................426
Проектирование признаков .................................................................................427
Категориальные признаки .............................................................................427
Текстовые признаки ......................................................................................429
Признаки для изображений ...........................................................................430
Производные признаки .................................................................................430
Внесение отсутствующих данных ..................................................................433
Конвейеры признаков ...................................................................................434
Заглянем глубже: наивная байесовская классификация ...................................... 435
Байесовская классификация ..........................................................................435
Гауссов наивный байесовский классификатор ...............................................436
Полиномиальный наивный байесовский классификатор ................................439
Когда имеет смысл использовать наивный байесовский классификатор ........442
Заглянем глубже: линейная регрессия ................................................................ 443
Простая линейная регрессия .........................................................................443
Регрессия по комбинации базисных функций ................................................446
Регуляризация............................................................................................... 450
Пример: предсказание велосипедного трафика .............................................453
Заглянем глубже: метод опорных векторов .........................................................459
Основания для использования метода опорных векторов ..............................459
Метод опорных векторов: максимизируем отступ ..........................................461
Пример: распознавание лиц ..........................................................................470
Резюме по методу опорных векторов ............................................................474
Заглянем глубже: деревья решений и случайные леса ........................................ 475
Движущая сила случайных лесов: деревья принятия решений ......................475

--- СТРАНИЦА 14 ---

\1nfaces ..............................................................................497
Резюме метода главных компонент ...............................................................500
Заглянем глубже: обучение на базе многообразий ..............................................500
Обучение на базе многообразий: HELLO ........................................................501
Многомерное масштабирование (MDS) ..........................................................502
MDS как обучение на базе многообразий ......................................................505
Нелинейные вложения: там, где MDS не работает ........................................507
Нелинейные многообразия: локально линейное вложение ............................508
Некоторые соображения относительно методов обучения 
на базе многообразий ...................................................................................510
Пример: использование Isomap для распознавания лиц ................................511
Пример: визуализация структуры цифр .........................................................515
Заглянем глубже: кластеризация методом k-средних ...........................................518
Знакомство с методом k-средних ...................................................................518
Алгоритм k-средних: максимизация математического ожидания ....................520
Примеры .......................................................................................................525
Заглянем глубже: смеси Гауссовых распределений.............................................. 532
Причины появления GMM: недостатки метода k-средних ...............................532
Обобщение EM-модели: смеси Гауссовых распределений ..............................535
GMM как метод оценки плотности распределения .........................................540
Пример: использование метода GMM для генерации новых данных .............. 544
Заглянем глубже: ядерная оценка плотности распределения ..............................547
Обоснование метода KDE: гистограммы ........................................................547
Ядерная оценка плотности распределения на практике ................................552
Пример: KDE на сфере ..................................................................................554
Пример: не столь наивный байес ..................................................................557

--- СТРАНИЦА 15 ---
Оглавление 1 5
Прикладная задача: конвейер распознавания лиц ...............................................562
Признаки в методе HOG ................................................................................563
Метод HOG в действии: простой детектор лиц ..............................................564
Предостережения и дальнейшие усовершенствования ..................................569
Дополнительные источники информации по машинному обучению .....................571
Машинное обучение в языке Python ..............................................................571
Машинное обучение в целом .........................................................................572
Об авторе ........................................................................................................... 573

--- СТРАНИЦА 16 ---
Предисловие
Что такое наука о данных
Эта книга посвящена исследованию данных с помощью языка программирования 
Python. Сразу же возникает вопрос: что же такое наука о данных (data science)? 

\1
 навыки специалиста по математической статистике, умеющего моделировать 
наборы данных и извлекать из них основное;
 навыки специалиста в области компьютерных наук, умеющего проектировать 
и использовать алгоритмы для эффективного хранения, обработки и визуали-
зации этих данных;
 экспертные знания предметной области, полученные в ходе традиционного 
изучения предмета, — умение как формулировать правильные вопросы, так 
и рассматривать ответы на них в соответствующем контексте.
С учетом этого я рекомендовал бы рассматривать науку о данных не как новую 
область знаний, которую нужно изучить, а как новый набор навыков, который 
вы можете использовать в рамках хорошо знакомой вам предметной области. Из-
вещаете ли вы о результатах выборов, прогнозируете ли прибыльность ценных 
бумаг, занимаетесь ли оптимизацией контекстной рекламы в Интернете или рас-
познаванием микроорганизмов на сделанных с помощью микроскопа фото, ищете 
ли новые классы астрономических объектов или же работаете с данными в любой 
другой сфере, цель этой книги — научить задавать новые вопросы о вашей пред-
метной области и отвечать на них.
Для кого предназначена эта книга
«Как именно следует изучать Python?» — один из наиболее часто задаваемых мне 
вопросов на различных технологических конференциях и встречах. Задают его за-
интересованные в технологиях студенты, разработчики или исследователи, часто 
уже со значительным опытом написания кода и использования вычислительного 
и цифрового инструментария. Большинству из них не нужен язык программирова-
ния Python в чистом виде, они хотели бы изучать его, чтобы применять в качестве 
инструмента для решения задач, требующих вычислений с обработкой больших 
объемов данных.
Эта книга не планировалась в качестве введения в язык Python или в программиро-
вание вообще. Я предполагаю, что читатель знаком с языком Python, включая опи-
сание функций, присваивание переменных, вызов методов объектов, управление 
потоком выполнения программы и решение других простейших задач. Она должна 
помочь пользователям языка Python научиться применять стек инструментов ис-
следования данных языка Python — такие библиотеки, как IPython, NumPy, Pandas, 

--- СТРАНИЦА 18 ---

\1n и соответствующие инструменты, — для эффективного 
хранения, манипуляции и понимания данных.
Почему Python
За последние несколько десятилетий язык программирования Python превратился 
в первоклассный инструмент для научных вычислений, включая анализ и визуализа-
цию больших наборов данных. Это может удивить давних поклонников Python: сам 
по себе этот язык не был создан в расчете на анализ данных или научные вычисления.
Язык программирования Python пригоден для науки о данных в основном благода-
ря большой и активно развивающейся экосистеме пакетов, созданных сторонними 
разработчиками:
 библиотеки NumPy — для работы с однородными данными в виде массивов;
 библиотеки Pandas — для работы с неоднородными и поименованными дан -
ными;
 SciPy — для общих научных вычислительных задач;
 библиотеки Matplotlib — для визуализаций типографского качества;
 оболочки IPython — для интерактивного выполнения и совместного исполь -
зования кода;
 библиотеки Scikit-Learn — для машинного обучения и множества других ин -
струментов, которые будут упомянуты в дальнейшем.
Если вы ищете руководство по самому языку программирования Python, рекомен-
дую обратить ваше внимание на проект «Краткая экскурсия по языку программи-
рования Python» ( https://github.com/jakevdp/WhirlwindTourOfPython). Он знакомит с важ -
нейшими возможностями языка Python и рассчитан на исследователей данных, уже 
знакомых с одним или несколькими языками программирования.
Языки программирования Python 2 и Python 3. В книге используется синтаксис 
Python 3, содержащий несовместимые с выпусками 2.x языка программирования 
Python расширения. Хотя Python 3.0 был впервые выпущен в 2008 году, он внедрялся 
довольно медленно, особенно в научном сообществе и сообществе веб-разработчиков. 
Главная причина заключалась в том, что многие важные сторонние пакеты и наборы 
программ только через некоторое время стали совместимы с новым языком.
В начале 2014 года стабильные выпуски важнейших инс трументов экосистемы на-
уки о данных были полностью совместимы как с языком Python 2, так и Python 3, 
поэтому данная книга использует более новый синтаксис языка Python 3. Однако 
абсолютное большинство фрагментов кода в этой книге будет также работать без 
всяких модификаций на языке Python 2. Случаи, когда применяется несовмести-
мый с Python 2 синтаксис, я буду указывать.

--- СТРАНИЦА 19 ---
Общая структура книги 1 9
Общая структура книги
Каждая глава книги посвящена конкретному пакету или инструменту, составля-
ющему существенную часть инструментария Python для исследования данных.
 IPython и Jupyter (глава 1) — предоставляют вычислительную среду, в которой 
работают многие использующие Python исследователи данных.
 NumPy (глава 2) — предоставляет объект ndarray для эффективного хранения 
и работы с плотными массивами данных в Python.
 Pandas (глава 3) — предоставляет объект DataFrame для эффективного хранения 
и работы с поименованными/столбчатыми данными в Python.
 Matplotlib (глава 4) — предоставляет возможности для разнообразной гибкой 
визуализации данных в Python.
 Scikit-Learn (глава 5) — предоставляет эффективные реализации на Python 
большинства важных и широко известных алгоритмов машинного обучения.
Мир PyData гораздо шире представленных пакетов, и он растет день ото дня. 
С учетом этого я использую каждую возможность в книге, чтобы сослаться на 
другие интересные работы, проекты и пакеты, расширяющие пределы того, 
что можно сделать на языке Python. Тем не менее сегодня эти пять пакетов 
являются основополагающими для многого из того, что можно сделать в об -
ласти применения языка программирования Python к исследованию данных. 
Я полагаю, что они будут сохранять свое значение и при росте окружающей их 
экосистемы.
Использование примеров кода
Дополнительные материалы (примеры кода, упражнения и т. п.) доступны для 
скачивания по адресу https://github.com/jakevdp/PythonDataScienceHandbook.
Задача этой книги — помочь вам делать вашу работу. Вы можете использовать лю-
бой пример кода из книги в ваших программах и документации. Обращаться к нам 
за разрешением нет необходимости, разве что вы копир уете значительную часть 
кода. Например, написание программы, использующей несколько фрагментов кода 
из этой книги, не требует отдельного разрешения. Однако для продажи или рас-
пространения компакт-диска с примерами из книг O’Reilly разрешение требуется. 
Ответ на вопрос путем цитирования этой книги и цитирования примеров кода не 
требует разрешения. Но включение значительного количества примеров кода из 
книги в документацию к вашему продукту потребует разрешения.
Мы ценим, хотя и не требуем ссылки на первоисточник. Она включает назва -
ние, автора, издательство и ISBN. Например, «Python для сложных задач. Наука 

--- СТРАНИЦА 20 ---

\1ns@oreilly.com.
Вопросы установки
Инсталляция Python и набора библиотек, обеспечивающих возможность научных 
вычислений, не представляет сложности. В данном разделе будут рассмотрены 
особенности, которые следует принимать во внимание при настройке.
Существует множество вариантов установки Python, но я предложил бы вос -
пользоваться дистрибутивом Anaconda, одинаково работающим в операционных 
системах Windows, Linux и Mac OS X. Дистрибутив Anaconda существует в двух 
вариантах.
 Miniconda ( http://conda.pydata.org/miniconda.html ) содержит сам интерпретатор 
языка программирования Python, а также утилиту командной строки conda, 
функционирующую в качестве межплатформенной системы управления 
пакетами, ориентированной на работу с пакетами Python и аналогичной по 
духу утилитам apt и yum, хорошо знакомым пользователям операционной 
системы Linux.
 Anaconda ( https://www.continuum.io/downloads ) включает интерпретатор Python 
и утилиту conda, а также набор предустановленных пакетов, ориентированных 
на научные вычисления. Приготовьтесь к тому, что установка займет несколько 
гигабайт дискового пространства.
Все включаемые в Anaconda пакеты можно также установить вручную поверх 
Miniconda, именно поэтому я рекомендую вам начать с Miniconda.
Для начала скачайте и установите пакет Miniconda (не забудьте выбрать версию 
с языком Python 3), после чего установите базовые пакеты, используемые в данной 
книге:
[~]$ conda install numpy pandas scikit-learn matplotlib seaborn ipython-notebook
На протяжении всей книги мы будем применять и другие, более специализирован-
ные утилиты из научной экосистемы Python, установка которых сводится к выпол-
нению команды conda install название_пакета. За дополнительной информацией 
об утилите conda, включая информацию о создании и использовании сред разра-
ботки conda (что я бы крайне рекомендовал), обратитесь к онлайн-документации 
утилиты conda ( http://conda.pydata.org/docs/).

--- СТРАНИЦА 21 ---
Условные обозначения 2 1
Условные обозначения
В этой книге используются следующие условные обозначения.
Курсив
Курсивом выделены новые термины.
Моноширинный шрифт
Используется для листингов программ, а также внутри а бзацев, чтобы обра -
титься к элементам программы вроде переменных, функций и типов данных. 
Им также выделены имена и расширения файлов.
Полужирный моноширинный шрифт
Показывает команды или другой текст, который пользователь должен ввести 
самостоятельно.
Курсивный моноширинный шрифт
Показывает текст, который должен быть заменен значениями, введенными 
пользователем, или значениями, определяемыми контекстом.

--- СТРАНИЦА 22 ---
1
IPython: за пределами 
обычного Python
Меня часто спрашивают, какой из множества вариантов среды разработки для 
Python я использую в своей работе. Мой ответ иногда удивляет спрашивающих: 
моя излюбленная среда представляет собой оболочку IPython ( http://ipython.org/) 
плюс текстовый редактор (в моем случае редактор Emacs или Atom, в зависимости 
от настроения). IPython (сокращение от «интерактивный Python» ) был основан 
в 2001 году Фернандо Пересом в качестве продвинутого интерпретатора Python 
и с тех пор вырос в проект, призванный обеспечить, по словам Переса, «утилиты 
для всего жизненного цикла исследовательских расчетов». Если язык Python — 
механизм решения нашей задачи в области науки о данных, то оболочку IPython 
можно рассматривать как интерактивную панель управления.
Оболочка IPython является полезным интерактивным ин терфейсом для языка 
Python и имеет несколько удобных синтаксических дополнений к нему. Боль -
шинство из них мы рассмотрим. Кроме этого, оболочка IPython тесно связана 
с проектом Jupiter ( http://jupyter.org/ ), предоставляющим своеобразный блокнот 
(текстовый редактор) для браузера, удобный для разработки, совместной работы 
и использования ресурсов, а также для публикации научных результатов. Блок -
нот оболочки IPython, по сути, частный случай более общей структуры блокнота 
Jupiter, включающего блокноты для Julia, R и других языков программирования. 
Не стоит далеко ходить за примером, показывающим удобства формата блокнота, 
им служит страница, которую вы сейчас читаете: вся рукопись данной книги была 
составлена из набора блокнотов IPython.
Оболочка IPython позволяет эффективно использовать язык Python для интер-
активных научных вычислений, требующих обработки большого количества 
данных. В этой главе мы рассмотрим возможности оболочки IPython, полезные 

--- СТРАНИЦА 23 ---
Командная строка или блокнот? 2 3
при исследовании данных. Сосредоточим свое внимание на тех предоставляемых 
IPython синтаксических возможностях, которые выходят за пределы стандарт -
ных возможностей языка Python. Немного углубимся в «магические» команды, 
позволяющие ускорить выполнение стандартных задач при создании и исполь -
зовании предназначенного для исследования данных кода. И наконец, затронем 
возможности блокнота, полезные для понимания смысла данных и совместного 
использования результатов.
Командная строка или блокнот?
Существует два основных способа использования оболочки IPython: командная 
строка IPython и блокнот IPython. Большая часть изложенного в этой главе мате-
риала относится к обоим, а в примерах будет применяться более удобный в кон-
кретном случае вариант. Я буду отмечать немногие разделы, относящиеся только 
к одному из них. Прежде чем начать, вкратце расскажу, как запускать командную 
оболочку IPython и блокнот IPython.
Запуск командной оболочки IPython
Данная глава, как и большая часть книги, не предназначена для пассивного чтения. 
Я рекомендую вам экспериментировать с описываемыми инструментами и синтак-
сисом: формируемая при этом мышечная память принесет намного больше пользы, 
чем простое чтение. Начнем с запуска интерпретатора оболочки IPython путем 
ввода команды IPython в командной строке. Если же вы установили один из таких 
дистрибутивов, как Anaconda или EPD, вероятно, у вас есть запускающий модуль 
для вашей операционной системы (мы обсудим это подробнее в разделе «Справка 
и документация в оболочке Python» данной главы).

\1
IPython 4.0.1 – An enhanced Interactive Python.
? -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
Help -> Python's own help system.
Object? -> Details about 'object', use 'object??' for extra details.
In [1]:
Далее вы можете активно следить за происходящим в книге.
Запуск блокнота Jupiter
Блокнот Jupiter — браузерный графический интерфейс для командной оболоч -
ки IPython и богатый набор основанных на нем возможностей динамической 

--- СТРАНИЦА 24 ---

\1n: за пределами обычного Python 
визуализации . Помимо выполнения операторов Python/IPython, блокнот позволя -
ет пользователю вставлять форматированный текст, статические и динамические 
визуализации, математические уравнения, виджеты JavaScript и многое другое. 
Более того, эти документы можно сохранять так, что другие люди смогут открывать 
и выполнять их в своих системах.
Хотя просмотр и редактирование блокнота IPython осуществляется через окно 
браузера, он должен подключаться к запущенному процессу Python для выпол -
нения кода. Для запуска этого процесса (называемого ядром (kernel)) выполните 
следующую команду в командной строке вашей операционной системы:
$ jupyter notebook

\1
$ jupyter notebook
[NotebookApp] Serving notebooks from local directory: /Users/jakevdp/…
[NotebookApp] 0 active kernels
[NotebookApp] The IPython Notebook is running at: http://localhost:8888/
[NotebookApp] Use Control-C to stop this server and shut down all kernels…
После выполнения этой команды ваш браузер по умолчанию должен автома -
тически запуститься и перейти по указанному локальному URL; точный адрес 
зависит от вашей системы. Если браузер не открывается автоматически, можете 
запустить его вручную и перейти по указанному адресу (в данном примере http://
localhost:8888/).
Справка и документация 
в оболочке IPython
Если вы не читали остальных разделов в данной главе, прочитайте хотя бы этот. 
Обсуждаемые здесь утилиты внесли наибольший (из IPython) вклад в мой еже -
дневный процесс разработки.
Когда человека с техническим складом ума просят помочь другу, родственнику или 
коллеге решить проблему с компьютером, чаще всего речь идет об умении быстро 
найти неизвестное решение. В науке о данных все точно так же: допускающие по-
иск веб-ресурсы, такие как онлайн-документация, дискуссии в почтовых рассылках 
и ответы на сайте Stack Overflow, содержат массу информации, даже (особенно?) 
если речь идет о теме, информацию по которой вы уже искали. Уметь эффективно 
исследовать данные означает скорее не запоминание утилит или команд, которые 
нужно использовать в каждой из возможных ситуаций, а знание того, как эффек-

--- СТРАНИЦА 25 ---
Справка и документация в оболочке IPython 2 5
тивно искать неизвестную пока информацию: посредством ли поиска в Интернете 
или с помощью других средств.
Одна из самых полезных возможностей IPython/Jupiter заключается в сокраще-
нии разрыва между пользователями и типом документации и поиска, что должно 
помочь им эффективнее выполнять свою работу. Хотя поиск в Интернете все еще 
играет важную роль в ответе на сложные вопросы, большое количество инфор -
мации можно найти, используя саму оболочку IPython. Вот несколько примеров 
вопросов, на которые IPython может помочь ответить буквально с помощью не -
скольких нажатий клавиш.
 Как вызвать эту функцию? Какие аргументы и параметры есть у нее?
 Как выглядит исходный код этого объекта Python?
 Что имеется в импортированном мной пакете? Какие ат рибуты или методы 
есть у этого объекта?
Мы обсудим инструменты IPython для быстрого доступа к этой информации, 
а именно символ ? для просмотра документации, символы ?? для просмотра ис -
ходного кода и клавишу Tab для автодополнения.
Доступ к документации с помощью символа ?
Язык программирования Python и его экосистема для исследования данных яв -
ляются клиентоориентированными, и в значительной степени это проявляется 
в доступе к документации. Каждый объект Python содержит ссылку на ст року, 
именуемую docstring (сокращение от documentation string — «строка документа -
ции»), которая в большинстве случаев будет содержать краткое описание объекта 
и способ его использования. В языке Python имеется встроенная функция help(), 
позволяющая обращаться к этой информации и выводить результат. Например, 
чтобы посмотреть документацию по встроенной функции len, можно сделать 
следующее:
In [1]: help(len)
Help on built-in function len in module builtins:
len(…)
 len(object) -> integer
 Return the number of items of a sequence or mapping1.
В зависимости от интерпретатора информация будет отображена в виде встраива-
емого текста или в отдельном всплывающем окне.

\1n: за пределами обычного Python 
Поскольку поиск справочной информации по объекту — очень распространенное 
и удобное действие, оболочка IPython предоставляет символ ? для быстрого до -
ступа к документации и другой соответствующей информации:
In [2]: len?
Type: builtin_function_or_method
String form: <built-in function len>
Namespace: Python builtin
Docstring:
len(object) -> integer
Return the number of items of a sequence or mapping1.

\1
In [3]: L = [1, 2, 3]
In [4]: L.insert?
Type: builtin_function_or_method
String form: <built-in method insert of list object at 0x1024b8ea8>
Docstring: L.insert(index, object) – insert object before index2
или даже сами объекты с документацией по их типу:
In [5]: L?
Type: list
String form: [1, 2, 3]
Length: 3
Docstring:
list() -> new empty list3
list(iterable) -> new list initialized from iterable's items4
Это будет работать даже для созданных пользователем функций и других объектов! 
В следующем фрагменте кода мы опишем маленькую функцию с docstring:
In [6]: def square(a):
 ….: """Return the square of a."""
 ….: return a ** 2
 ….:
Обратите внимание, что при создании docstring для нашей функции мы про -
сто вставили в первую строку строковый литерал. Поскольку docstring обычно 
занимает несколько строк, в соответствии с условными соглашениями мы ис -
пользовали для многострочных docstring нотацию языка Python с тройными 
кавычками.

\1ndex.

\1n 2 7
Теперь воспользуемся знаком ? для нахождения этой docstring:
In [7]: square?
Type: function
String form: <function square at 0x103713cb0>
Definition: square(a)
Docstring: Return the square a.
Быстрый доступ к документации через элементы docstring — одна из причин, по 
которым желательно приучить себя добавлять подобную встроенную документа-
цию в создаваемый код!
Доступ к исходному коду с помощью символов ??
Поскольку текст на языке Python читается очень легко, чтение исходного кода 
интересующего вас объекта может обеспечить более глубокое его понимание. 
Оболочка IPython предоставляет сокращенную форму обращения к исходному 
коду — двойной знак вопроса ( ??):
In [8]: square??
Type: function
String form: <function square at 0x103713cb0>
Definition: square(a)
Source:
def square(a):
 "Return the square of a"
 return a ** 2
Для подобных простых функций двойной знак вопроса позволяет быстро проник-
нуть в особенности внутренней реализации.
Поэкспериментировав с ним немного, вы можете обратить внимание, что иногда 
добавление в конце ?? не приводит к отображению никакого исходного кода: 
обычно это происходит потому, что объект, о котором идет речь, реализован не 
на языке Python, а на C или каком-либо другом транслируемом языке расши -
рений. В подобном случае добавление ?? приводит к такому же результату, что 
и добавление ?. Вы столкнетесь с этим в отношении многих встроенных объектов 
и типов Python, например для упомянутой выше функции len:
In [9]: len??
Type: builtin_function_or_method
String form: <built-in function len>
Namespace: Python builtin
Docstring:
len(object) -> integer
Return the number of items of a sequence or mapping1.

\1n: за пределами обычного Python 
Использование ? и/или ?? — простой способ для быстрого поиска информации 
о работе любой функции или модуля языка Python.
Просмотр содержимого модулей с помощью 
Tab-автодополнения
Другой удобный интерфейс оболочки IPython — использование клавиши Tab для 
автодополнения и просмотра содержимого объектов, модулей и пространств имен. 
В следующих примерах мы будем применять обозначение <TAB> там, где необхо-
димо нажать клавишу Tab.
Автодополнение названий содержимого объектов 
с помощью клавиши Tab
С каждым объектом Python связано множество различных атрибутов и методов. 
Аналогично обсуждавшейся выше функции help в языке Python есть встроенная 
функция dir, возвращающая их список, но на практике использовать интерфейс 
Tab-автодополнения гораздо удобнее. Чтобы просмотреть список всех доступных 
атрибутов объекта, необходимо набрать имя объекта с последующим символом 
точки ( .) и нажать клавишу Tab:
In [10]: L.<TAB>
L.append L.copy L.extend L.insert L.remove L.sort
L.clear L.count L.index L.pop L.reverse

\1
In [10]: L.c<TAB>
L.clear L.copy L.count
In [10]: L.co<TAB>
L.copy L.count
Если имеется только один вариант, нажатие клавиши Tab приведет к автодополне-
нию строки. Например, следующее будет немедленно заменено на L.count:
In [10]: L.cou<TAB>
Хотя в языке Python отсутствует четкое разграничение между открытыми/внеш-
ними и закрытыми/внутренними атрибутами, по соглашениям, для обозначения 
подобных методов принято использовать знак подчерки вания. Для ясности эти 
закрытые методы, а также специальные методы по умолчанию исключаются из 
списка, но можно вывести их список, набрав знак подчеркивания:

--- СТРАНИЦА 29 ---
Справка и документация в оболочке IPython 2 9
In [10]: L._<TAB>
L.__add__ L.__gt__ L.__reduce__
L.__class__ L.__hash__ L.__reduce_ex__
Для краткости я показал только несколько первых строк вывода. Большинство 
этих методов — методы специального назначения языка Python, отмечен -
ные двойным подчеркиванием в названии (на сленге называемые dunder 1 -
методами).

\1
In [10]: from itertools import co<TAB>
combinations compress
combinations_with_replacement count
Аналогично можно использовать Tab-автодополнение для просмотра доступных 
в системе вариантов импорта (полученное вами может отличаться от нижепри -
веденного листинга в зависимости от того, какие сторонние сценарии и модули 
являются видимыми данному сеансу Python):
In [10]: import <TAB>
Display all 399 possibilities? (y or n)
Crypto dis py_compile
Cython distutils pyclbr
… … …
difflib pwd zmq
In [10]: import h<TAB>
hashlib hmac http
heapq html husl
Отмечу, что для краткости я не привожу здесь все 399 пакетов и модулей, доступ-
ных в моей системе для импорта.
Помимо автодополнения табуляцией, подбор 
по джокерному символу
Автодополнение табуляцией удобно в тех случаях, когда вам известны первые не-
сколько символов искомого объекта или атрибута. Однако оно малопригодно, когда 
необходимо найти соответствие по символам, находящимся в середине или конце 

\1nderscore — «двойное подчеркивание» 
и dunderhead — «тупица», «болван».

--- СТРАНИЦА 30 ---

\1n: за пределами обычного Python 
слова. На этот случай оболочка IPython позволяет искать соответствие названий 
по джокерному символу *.
Например, можно использовать следующий код для вывода списка всех объектов 
в пространстве имен, заканчивающихся словом Warning:
In [10]: *Warning?
BytesWarning RuntimeWarning
DeprecationWarning SyntaxWarning
FutureWarning UnicodeWarning
ImportWarning UserWarning
PendingDeprecationWarning Warning
ResourceWarning
Обратите внимание, что символ * соответствует любой строке, включая пустую.
Аналогично предположим, что мы ищем строковый метод, содержащий где-то в на-
звании слово find. Отыскать его можно следующим образом:
In [10]: str.*find*?
Str.find
str.rfind
Я обнаружил, что подобный гибкий поиск с помощью джокерных символов очень 
удобен для поиска нужной команды при знакомстве с новым пакетом или обраще-
нии после перерыва к уже знакомому.
Сочетания горячих клавиш в командной 
оболочке IPython
Вероятно, все, кто проводит время за компьютером, используют в своей работе со -
четания горячих клавиш. Наиболее известные — Cmd+ C и Cmd+ V (или Ctrl+ C и Ctrl+ V), 
применяемые для копирования и вставки в различных программах и системах. 
Опытные пользователи выбирают популярные текстовые редакторы, такие как 
Emacs, Vim и другие, позволяющие выполнять множество операций посредством 
замысловатых сочетаний клавиш.
В командной оболочке IPython также имеются сочетания горячих клавиш для 
быстрой навигации при наборе команд. Эти сочетания горячих клавиш предо -
ставляются не самой оболочкой IPython, а через ее зависимости от библиотеки 
GNU Readline: таким образом определенные сочетания горячих клавиш могут раз -
личаться в зависимости от конфигурации вашей системы. Хотя некоторые из них 
работают в блокноте для браузера, данный раздел в основном касается сочетаний 
горячих клавиш именно в командной оболочке IPython.

--- СТРАНИЦА 31 ---
Сочетания горячих клавиш в командной оболочке IPython 3 1
После того как вы привыкнете к сочетаниям горячих клавиш, вы сможете их ис-
пользовать для быстрого выполнения команд без изменения исходного положения 
рук на клавиатуре. Если вы пользователь Emacs или имеете опыт работы с Linux-
подобными командными оболочками, некоторые сочетания горячих клавиш пока-
жутся вам знакомыми. Мы сгруппируем их в несколько категорий: навигационные 
горячие клавиши , горячие клавиши ввода текста , горячие клавиши для истории 
команд и прочие горячие клавиши.
Навигационные горячие клавиши
Использовать стрелки «влево» ( ←) и «вправо» ( →) для перемещения назад и впе -
ред по строке вполне естественно, но есть и другие возможности, не требующие 
изменения исходного положения рук на клавиатуре (табл. 1.1).
Таблица 1.1. Горячие клавиши для навигации
Комбинация клавиш Действие
Ctrl+A Перемещает курсор в начало строки
Ctrl+E Перемещает курсор в конец строки
Ctrl+B (или стрелка «влево») Перемещает курсор назад на один символ
Ctrl+F (или стрелка «вправо») Перемещает курсор вперед на один символ
Горячие клавиши ввода текста
Для удаления предыдущего символа привычно использовать клавишу Backspace, 
несмотря на то что требуется небольшая гимнастика для пальцев, чтобы до нее дотя-
нуться. Эта клавиша удаляет только один символ за раз. В оболочке IPython имеется 
несколько сочетаний горячих клавиш для удаления различных частей набираемого 
текста. Наиболее полезные из них — команды для удаления сразу целых строк текста 
(табл. 1.2). Вы поймете, что привыкли к ним, когда поймаете себя на использовании со-
четания Ctrl+ B и Ctrl+ D вместо клавиши Backspace для удаления предыдущего символа!
Таблица 1.2. Горячие клавиши для ввода текста
Комбинация клавиш Действие
Backspace Удаляет предыдущий символ в строке
Ctrl+D Удаляет следующий символ в строке
Ctrl+K Вырезает текст, начиная от курсора и до 
конца строки
Ctrl+U Вырезает текст с начала строки до курсора
Ctrl+Y Вставляет предварительно вырезанный текст
Ctrl+T Меняет местами предыдущие два символа

--- СТРАНИЦА 32 ---

\1n: за пределами обычного Python 
Горячие клавиши для истории команд
Вероятно, наиболее важные из обсуждаемых здесь сочетаний горячих кла -
виш — сочетания для навигации по истории команд. Данная история команд 
распространяется за пределы текущего сеанса оболочки IPython: полная исто -
рия команд хранится в базе данных SQLite в каталоге с профилем IPython. 
Простейший способ получить к ним доступ — с помощью стрелок «вверх» ( ↑) 
и «вниз» ( ↓) для пошагового перемещения по истории, но есть и другие вари -
анты (табл. 1.3).
Таблица 1.3. Горячие клавиши для истории команд
Комбинация клавиш Действие
Ctrl+P (или стрелка «вверх») Доступ к предыдущей команде в истории
Ctrl+N (или стрелка «вниз») Доступ к следующей команде в истории
Ctrl+R Поиск в обратном направлении по истории 
команд
Особенно полезным может оказаться поиск в обратном направлении. Как вы 
помните, в предыдущем разделе мы описали функцию square. Выполним поиск 
в обратном направлении по нашей истории команд Python в новом окне оболочки 
IPython и найдем это описание снова. После нажатия Ctrl+ R в терминале IPython 
вы должны увидеть следующее приглашение командной строки:
In [1]:
(reverse-i-search)`':
Если начать вводить символы в этом приглашении, IPython автоматически будет 
дополнять их, подставляя недавние команды, соответствующие этим символам, 
если такие существуют:
In [1]:
(reverse-i-search)`sqa': square??

\1
In [1]:
(reverse-i-search)`sqa': def square(a):
 """Return the square of a"""
 return a ** 2
Найдя искомую команду, нажмите Enter и поиск завершится. После этого можно 
использовать найденную команду и продолжить работу в нашем сеансе:

--- СТРАНИЦА 33 ---
«Магические» команды IPython 3 3
In [1]: def square(a):
 """Return the square of a"""
 return a ** 2
In [2]: square(2)
Out[2]: 4
Обратите внимание, что также можно использовать сочетания клавиш Ctrl+ P/
Ctrl+ N или стрелки вверх/вниз для поиска по истории команд, но только по совпада-
ющим символам в начале строки. Если вы введете def и нажмете Ctrl+ P, будет най -
дена при наличии таковой последняя команда в истории, начинающаяся с символов 
def.
Прочие горячие клавиши
Имеется еще несколько сочетаний клавиш, не относящихся ни к одной из преды-
дущих категорий, но заслуживающих упоминания (табл. 1.4).
Таблица 1.4. Дополнительные горячие клавиши
Комбинация клавиш Действие
Ctrl+L Очистить экран терминала
Ctrl+C (или стрелка «вниз») Прервать выполнение текущей команды Python
Ctrl+D Выйти из сеанса IPython 1
Сочетание горячих клавиш Ctrl+ C особенно удобно при случайном запуске очень 
долго работающего задания.
Хотя использование некоторых сочетаний горячих клавиш может показаться уто-
мительным, вскоре у вас появится соответствующая мышечная память и вы будете 
жалеть, что эти команды недоступны в других программах.
«Магические» команды IPython
Предыдущие два раздела продемонстрировали возможнос ти эффективного ис -
пользования и изучения языка Python с помощью оболочки IPython. Здесь же мы 
начнем обсуждать некоторые предоставляемые IPython расширения обычного 
синтаксиса языка Python. В IPython они известны как «магические» команды 
(magic commands) и отличаются указанием перед названием команды символа %. 
Эти «магические» команды предназначены для быстрого решения различных рас-
пространенных задач стандартного анализа данных.

\1n: за пределами обычного Python 
Существует два вида «магических» команд: строчные «магические» команды, 
обозначаемые одним знаком % и работающие с одной строкой ввода, и блочные 
«магические» команды, обозначаемые удвоенным префиксом ( %%) и работающие 
с несколькими строками ввода. Мы обсудим несколько коротких примеров и вер-
немся к более подробному обсуждению некоторых «магических» команд далее 
в данной главе.
Вставка блоков кода: %paste и %cpaste
При работе с интерпретатором IPython одна из распространенных проблем за -
ключается в том, что вставка многострочных блоков кода может привести к неожи-
данным ошибкам, особенно при использовании отступов и меток интерпретатора. 

\1
>>> def donothing(x):
… return x
Код отформатирован так, как он будет отображаться в интерпретаторе языка 
Python, и если вы скопируете и вставите его непосредственно в оболочку IPython, 
то вам будет возвращена ошибка:
In [2]: >>> def donothing(x):
 …: … return x
 …:
 File "<ipython-input-20-5a66c8964687>", line 2
 … return x
 ^
SyntaxError: invalid syntax
При такой непосредственной вставке интерпретатор сбивают с толку лишние сим-
волы приглашения к вводу. Но не волнуйтесь: «магиче ская» функция оболочки 
IPython %paste разработана специально для корректной обработки данного типа 
многострочного ввода:
In [3]: %paste
>>> def donothing(x):
… return x
## -- Конец вставленного текста –

\1
In [4]: donothing(10)
Out[4]: 10

--- СТРАНИЦА 35 ---
«Магические» команды IPython 3 5

\1
In [5]: %cpaste
Pasting code; enter '–-' alone on the line to stop or use Ctrl-D.
:>>> def donothing(x):
:… return x
:--
Эти «магические» команды, подобно другим, предоставляют функциональность, 
которая была бы сложна или вообще невозможна при использовании обычного 
интерпретатора языка Python.
Выполнение внешнего кода: %run
Начав писать более обширный код, вы, вероятно, будете работать как в оболочке 
IPython для интерактивного исследования, так и в текстовом редакторе для сохра -
нения кода, который вам хотелось бы использовать неоднократно. Будет удобно 
выполнять этот код не в отдельном окне, а непосредственно в сеансе оболочки 
IPython. Для этого можно применять «магическую» функцию %run.

\1
 """square a number"""
 return x ** 2
for N in range(1, 4):
 print(N, "squared is", square(N))
Выполнить это в сеансе IPython можно следующим образом:
In [6]: %run myscript.py
1 squared is 1
2 squared is 4
3 squared is 9
Обратите внимание, что после выполнения этого сценария все описанные в нем 
функции становятся доступными для использования в данном сеансе оболочки 
IPython:
In [7]: square(5)
Out[7]: 25

--- СТРАНИЦА 36 ---

\1n: за пределами обычного Python 
Существует несколько параметров для точной настройки метода выполнения кода. 
Посмотреть относящуюся к этому документацию можно обычным способом, набрав 
команду %run? в интерпретаторе IPython.
Длительность выполнения кода: %timeit
Еще один пример полезной «магической» функции — %timeit, автоматически опре-
деляющая время выполнения следующего за ней однострочного оператора языка 
Python. Например, если нам нужно определить производительность спискового 
включения:
In [8]: %timeit L = [n ** 2 for n in range(1000)]
1000 loops, best of 3: 325 µs per loop

\1
In [9]: %%timeit
 …: L = []
 …: for n in range(1000):
 …: L.append(n ** 2)
 …:
1000 loops, best of 3: 373 µs per loop
Сразу же видно, что в данном случае списковое включение происходит примерно 
на 10 % быстрее, чем эквивалентная конструкция для цикла for. Мы рассмотрим 
%timeit и другие подходы к мониторингу скорости выполнения и профилированию 
кода в разделе «Профилирование и мониторинг скорости выполнения кода» этой 
главы.
Справка по «магическим» функциям: ?, %magic 
и %lsmagic
Подобно обычным функциям языка программирования Python, у «магических» 
функций оболочки IPython есть свои инструкции (docstring), и к этой докумен -
тации можно обратиться обычным способом. Например, чтобы просмотреть до -
кументацию по «магической» функции %timeit, просто введите следующее:
In [10]: %timeit?

\1

--- СТРАНИЦА 37 ---
История ввода и вывода 3 7
In [11]: %magic

\1
In [12]: %lsmagic
При желании можно описать свою собственную «магическую» функцию. Если вас 
это интересует, загляните в приведенные в разделе «Дополнительные источники 
информации об оболочке IPython» данной главы.
История ввода и вывода
Командная оболочка IPython позволяет получать доступ к предыдущим командам 
с помощью стрелок «вверх» ( ↑) и «вниз» ( ↓) или сочетаний клавиш Ctrl+ P/ Ctrl+ N. 
Дополнительно как в командной оболочке, так и в блокноте IPython дает возмож -
ность получать вывод предыдущих команд, а также строковые версии самих этих 
команд.
Объекты In и Out оболочки IPython
Полагаю, вы уже хорошо знакомы с приглашениями In[1]:/ Out[1]:, используемы -
ми оболочкой IPython. Они представляют собой не просто изящные украшения, 
а подсказывают вам способ обратиться к предыдущим вводам и выводам в текущем 
сеансе. Допустим, вы начали сеанс, который выглядит следующим образом:
In [1]: import math
In [2]: math.sin(2)
Out[2]: 0.9092974268256817
In [3]: math.cos(2)
Out[3]: -0.4161468365471424
Мы импортировали встроенный пакет math, после чего вычислили значение синуса 
и косинуса числа 2. Вводы и выводы отображаются в командной оболочке с мет-
ками In/ Out, но за этим кроется нечто большее: оболочка IPython на самом деле 
создает переменные языка Python с именами In и Out, автоматически обновляемые 
так, что они отражают историю:
In [4]: print(In)
['', 'import math', 'math.sin(2)', 'math.cos(2)', 'print(In)']
In [5]: Out
Out[5]: {2: 0.9092974268256817, 3: -0.4161468365471424}

--- СТРАНИЦА 38 ---

\1n: за пределами обычного Python 
Объект In представляет собой список, отслеживающий очередность команд 
(первый элемент этого списка — «заглушка», чтобы In[1] ссылался на первую 
команду):
In [6]: print(In[1])
import math
Объект Out — не список, а словарь, связывающий ввод с выводом (если таковой 
есть).
Обратите внимание, что не все операции генерируют вывод: например, операторы 
import и print на вывод не влияют. Последнее обстоятельство может показаться 
странным, но смысл его становится понятен, если знать, что функция print возвра-
щает None; для краткости, возвращающие None команды не вносят вклада в объект 
Out.
Это может оказаться полезным при необходимости применять ранее полученные 
результаты. Например, вычислим сумму sin(2) ** 2 и cos(2) ** 2, используя най -
денные ранее значения:
In [8]: Out[2] ** 2 + Out[3] ** 2
Out[8]: 1.0
Результат равен 1.0, как и можно было ожидать из хорошо известного тригономе-
трического тождества. В данном случае использовать ранее полученные результа-
ты, вероятно, не было необходимости, но эта возможность может оказаться очень 
полезной, когда после выполнения ресурсоемких вычислений следует применить 
их результат повторно!
Быстрый доступ к предыдущим выводам с помощью 
знака подчеркивания
В обычной командной оболочке Python имеется лишь одна функция быстрого 
доступа к предыдущему выводу: значение переменной _ (одиночный символ 
подчеркивания) соответствует предыдущему выводу; это работает и в оболочке 
IPython:
In [9]: print(_)
1.0
Но IPython несколько более продвинут в этом смысле: можно использовать 
двойной символ подчеркивания для доступа к выводу на шаг ранее и тройной — 
для предшествовавшего ему (не считая команд, не генерировавших никакого 
вывода):
In [10]: print(__)
-0.4161468365471424 

--- СТРАНИЦА 39 ---
История ввода и вывода 3 9
In [11]: print(___)
0.9092974268256817
На этом оболочка IPython останавливается: более чем три подчеркивания уже не -
много сложно отсчитывать и на этом этапе проще ссылаться на вывод по номеру 
строки.

\1
In [12]: Out[2]
Out[12]: 0.9092974268256817
In [13]: _2
Out[13]: 0.9092974268256817

\1
In [14]: math.sin(2) + math.cos(2);

\1
In [15]: 14 in Out
Out[15]: False

\1
In [16]: %history –n 1-4
 1: import math
 2: math.sin(2)
 3: math.cos(2)
 4: print(In)
Как обычно, вы можете набрать команду %history? для получения дополнитель-
ной информации об этой команде и описания доступных параметров. Другие 
 аналогичные «магические» команды — %rerun (выполняющая повторно какую-либо 

--- СТРАНИЦА 40 ---

\1n: за пределами обычного Python 
часть истории команд) и %save (сохраняющая какую-либо часть истории команд 
в файле). Для получения дополнительной информации рекомендую изучить их 
с помощью справочной функциональности ?, обсуждавшейся в разделе «Справки 
и документация в Python» данной главы.
Оболочка IPython и использование системного 
командного процессора
При интерактивной работе со стандартным интерпретатором языка Python вы 
столкнетесь с досадным неудобством в виде необходимости переключаться между 
несколькими окнами для обращения к инструментам Python и системным утили-
там командной строки. Оболочка IPython исправляет э ту ситуацию, предоставляя 
пользователям синтаксис для выполнения инструкций системного командного 
процессора непосредственно из терминала IPython. Для этого используется вос-
клицательный знак: все, что находится после ! в строке, будет выполняться не 
ядром языка Python, а системной командной строкой.
Далее в этом разделе предполагается, что вы работаете в Unix-подобной операци-
онной системе, например Linux или Mac OS X. Некоторые из следующих примеров 
не будут работать в операционной системе Windows, использующей по умолчанию 
другой тип командного процессора (хотя после анонса в 2016 году нативн ой ко -
мандной оболочки Bash на Windows в ближайшем будущем это может перестать 
быть проблемой!). Если инструкции системного командного процессора вам не 
знакомы, рекомендую просмотреть руководство по нему ( http://swcarpentry.github.io/
shell-novice/), составленное фондом Software Carpentry.
Краткое введение в использование командного 
процессора
Полный вводный курс использования командного процессора/терминала/команд -
ной строки выходит за пределы данной главы, но непосвященных мы кратко позна-
комим с ним. Командный процессор — способ текстового взаимодействия с ком -
пьютером. Начиная с середины 1980-х годов, когда корпорации Microsoft и Apple 
представили первые версии их графических операционных систем, большинство 
пользователей компьютеров взаимодействуют со своей операционной системой 
посредством привычных щелчков кнопкой мыши на меню и движений «перета -
скивания». Но операционные системы существовали задолго до этих графических 
интерфейсов пользователя и управлялись в основном посредством последователь-
ностей текстового ввода: в приглашении командной строки пользователь вводил 
команду, а компьютер выполнял указанное пользователем действие. Эти первые 
системы командной строки были предшественниками командных процессоров 

--- СТРАНИЦА 41 ---
Оболочка IPython и использование системного командного процессора 4 1
и терминалов, используемых до сих пор наиболее деятельными специалистами по 
науке о данных.
Человек, не знакомый с командными процессорами, мог бы задать вопрос: зачем 
вообще тратить на это время, если можно многого добиться с помощью простых 
нажатий на пиктограммы и меню? Пользователь командного процессора мог 
бы ответить на этот вопрос другим вопросом: зачем гоняться за пиктограммами 
и щелкать по меню, если можно добиться того же гораздо проще, с помощью 
ввода команд? Хотя это может показаться типичным вопросом предпочтений, при 
выходе за пределы простых задач быстро становится понятно, что командный 
процессор предоставляет неизмеримо больше возможностей управления для 
сложных задач.
В качестве примера приведу фрагмент сеанса командного процессора операцион-
ной системы Linux/OS X, в котором пользователь просматривает, создает и меняет 
каталоги и файлы в своей системе ( osx:~ $ представляет собой приглашение ко 
вводу, а все после знака $ — набираемая команда; текст, перед которым указан 
символ #, представляет собой просто описание, а не действительно вводимый вами 
текст):
osx:~ $ echo "hello world" # echo аналогично функции print
 # языка Python
hello world
osx:~ $ pwd # pwd = вывести рабочий каталог
/home/jake # это «путь», где мы находимся
osx:~ $ ls # ls = вывести содержимое 
 # рабочего каталога
notebooks projects
osx:~ $ cd projects/ # cd = сменить каталог
osx:projects $ pwd
/home/jake/projects
osx:projects $ ls
datasci_book mpld3 myproject.txt
osx:projects $ mkdir myproject # mkdir = создать новый каталог
osx:projects $ cd myproject/
osx:myproject $ mv ../myproject.txt ./ # mv = переместить файл. 
 # В данном случае мы перемещаем
 # файл myproject.txt, находящийся
 # в каталоге на уровень выше (../), 
 # в текущий каталог (./)
osx:myproject $ ls

--- СТРАНИЦА 42 ---

\1n: за пределами обычного Python 
myproject.txt
Обратите внимание, что все это всего лишь краткий способ выполнения привыч -
ных операций (навигации по дереву каталогов, создания каталога, перемещения 
файла и т. д.) путем набора команд вместо щелчков по пиктограммам и меню. 
Кроме того, с помощью всего нескольких команд ( pwd, ls, cd, mkdir и cp) можно 
выполнить большинство распространенных операций с файлами. А уж когда вы 
выходите за эти простейшие операции, подход с использованием командного про-
цессора открывает по-настоящему широкие возможности.
Инструкции командного процессора 
в оболочке IPython
Вы можете использовать любую работающую в командной строке команду в обо-
лочке IPython, просто поставив перед ней символ !. Например, команды ls, pwd 
и echo можно выполнить следующим образом:
In [1]: !ls
myproject.txt
In [2]: !pwd
/home/jake/projects/myproject
In [3]: !echo "printing from the shell"
printing from the shell
Передача значений 
в командный процессор и из него
Инструкции командного процессора можно не только вы полнять из оболочки 
IPython, они могут также взаимодействовать с пространством имен IPython. На-
пример, можно сохранить вывод любой инструкции командного процессора с по-
мощью оператора присваивания:
In [4]: contents = !ls
In [5]: print(contents)
['myproject.txt']
In [6]: directory = !pwd
In [7]: print(directory)
['/Users/jakevdp/notebooks/tmp/myproject']

--- СТРАНИЦА 43 ---
«Магические» команды для командного процессора 4 3
Обратите внимание, что эти результаты возвращаются не в виде списков, а как спе-
циальный определенный в IPython для командного процессора тип возвращаемого 
значения:
In [8]: type(directory)
IPython.utils.text.Slist
Этот тип выглядит и работает во многом подобно спискам языка Python, но у него 
есть и дополнительная функциональность, в частности методы grep и fields, а так -
же свойства s, n и p, позволяющие выполнять поиск, фильтрацию и отображение 
результатов удобным способом. Чтобы узнать об этом больше, воспользуйтесь 
встроенными справочными возможностями оболочки IPython.
Отправка информации в обратную сторону — передача переменных Python в ко-
мандный процессор — возможна посредством синтаксиса {varname}:
In [9]: message = "Hello from Python"
In [10]: !echo {message}
Hello from Python

\1
In [11]: !pwd
/home/jake/projects/myproject
In [12]: !cd ..
In [13]: !pwd
/home/jake/projects/myproject
Причина заключается в том, что инструкции командног о процессора в блокноте 
оболочки IPython выполняются во временном командном подпроцессоре. Если вам 
нужно поменять рабочий каталог на постоянной основе, можно воспользоваться 
«магической» командой %cd:
In [14]: %cd ..
/home/jake/projects

--- СТРАНИЦА 44 ---

\1n: за пределами обычного Python 

\1
In [15]: cd myproject
/home/jake/projects/myproject
Такое поведение носит название «автомагических» (automagic) функций, его мож -
но настраивать с помощью «магической» функции %automagic.
Кроме %cd, доступны и другие «автомагические» функции: %cat, %cp, %env, %ls, 
%man, %mkdir, %more, %mv, %pwd, %rm и %rmdir, каждую из которых можно применять 
без знака %, если активизировано поведение automagic. При этом командную стро-
ку оболочки IPython можно использовать практически как обычный командный 
процессор:
In [16]: mkdir tmp
In [17]: ls
myproject.txt tmp/
In [18]: cp myproject.txt tmp/
In [19]: ls tmp
myproject.txt
In [20]: rm –r tmp
Доступ к командному процессору из того же окна терминала, в котором происхо-
дит сеанс работы с языком Python, означает резкое снижение числа необходимых 
переключений между интерпретатором и командным процессором при написании 
кода на языке Python.
Ошибки и отладка
Разработка кода и анализ данных всегда требуют некоторого количества проб 
и ошибок, и в оболочке IPython есть инструменты для упрощения этого про -
цесса. В данном разделе будут вкратце рассмотрены некоторые возможности 
по управлению оповещением об ошибках Python, а также утилиты для отладки 
ошибок в коде.
Управление исключениями: %xmode
Почти всегда при сбое сценария на языке Python генерируется исключение. В слу-
чае столкновения интерпретатора с одним из этих исключений, информацию о его 
причине можно найти в трассировке (traceback), к которой можно обратиться из 
Python. С помощью «магической» функции %xmode оболочка IPython дает вам 

--- СТРАНИЦА 45 ---

\1
In[1]: def func1(a, b):
 return a / b 
 def func2(x):
 a = x
 b = x – 1
 return func1(a, b)
In[2]: func2(1)
---------------------------------------------------------------------------
ZeroDivisionError Traceback (most recent call last)
<ipython-input-2-b2e110f6fc8f^gt; in <module>()
----> 1 func2(1)
<ipython-input-1-d849e34d61fb> in func2(x)
 5 a = x
 6 b = x – 1
----> 7 return func1(a, b)
<ipython-input-1-d849e34d61fb> in func1(a, b)
 1 def func1(a, b):
----> 2 return a / b
 3
 4 def func2(x):
 5 a = x 
ZeroDivisionError: division by zero
Вызов функции func2 приводит к ошибке, и чтение выведенной трассы позволяет 
нам в точности понять, что произошло. По умолчанию эта трасса включает не -
сколько строк, описывающих контекст каждого из приведших к ошибке шагов. 
С помощью «магической» функции %xmode (сокращение от exception mode — режим 
отображения исключений) мы можем управлять тем, какая информация будет вы -
ведена.
Функция %xmode принимает на входе один аргумент, режим, для которого есть три 
значения: Plain (Простой), Context (По контексту) и Verbose (Расширенный). 
Режим по умолчанию — Context, вывод при котором показан выше. Режим Plain 
дает более сжатый вывод и меньше информации:
In[3]: %xmode Plain
Exception reporting mode: Plain
In[4]: func2(1)

--- СТРАНИЦА 46 ---

\1n: за пределами обычного Python 
---------------------------------------------------------------------------
Traceback (most recent call last):
 File "<ipython-input-4-b2e110f6fc8f>", line 1, in <module>
 func2(1)
 File "<ipython-input-1-d849e34d61fb>", line 7, in func2
 return func1(a, b)
 File "<ipython-input-1-d849e34d61fb>", line 2, in func1
 return a / b 
ZeroDivisionError: division by zero

\1
In[5]: %xmode Verbose
Exception reporting mode: Verbose
In[6]: func2(1)
---------------------------------------------------------------------------
ZeroDivisionError Traceback (most recent call last)
<ipython-input-6-b2e110f6fc8f> in <module>()
----> 1 func2(1)
 global func2 = <function func2 at 0x103729320>
<ipython-input-1-d849e34d61fb> in func2(x=1)
 5 a = x
 6 b = x – 1
----> 7 return func1(a, b)
 global func1 = <function func1 at 0x1037294d0>
 a = 1
 b = 0
<ipython-input-1-d849e34d61fb> in func1(a=1, b=0)
 1 def func1(a, b):
----> 2 return a / b
 a = 1
 b = 0
 3
 4 def func2(x):
 5 a = x 
ZeroDivisionError: division by zero

--- СТРАНИЦА 47 ---
Ошибки и отладка 4 7
Эта дополнительная информация может помочь сузить круг возможных причин 
генерации исключения. Почему бы тогда не использовать режим Verbose всегда? 
Дело в том, что при усложнении кода такой вид трассировки может стать чрез -
вычайно длинным. В зависимости от контекста иногда проще работать с более 
кратким выводом режима по умолчанию.
Отладка: что делать, если чтения трассировок 
недостаточно
Стандартная утилита языка Python для интерактивной отладки называется pdb 
(сокращение от Python Debugger — «отладчик Python»). Этот отладчик предо -
ставляет пользователю возможность выполнять код строка за строкой, чтобы вы-
яснить, что могло стать причиной какой-либо замысловатой ошибки. Расширенная 
версия этого отладчика оболочки IPython называется ipdb (сокращение от IPython 
Debugger — «отладчик IPython»).
Существует множество способов запуска и использования этих отладчиков; мы 
не станем описывать их все. Для более полной информации по данным утилитам 
обратитесь к онлайн-документации.
Вероятно, наиболее удобный интерфейс для отладки в IPython — «магическая» 
команда %debug. Если ее вызвать после столкновения с исключением, она автомати-
чески откроет интерактивную командную строку откладки в точке возникновения 
исключения. Командная строка ipdb позволяет изучать текущее состояние стека, 
доступные переменные и даже выполнять команды Python!

\1
In[7]: %debug
> <ipython-input-1-d849e34d61fb>(2)func1()
 1 def func1(a, b):
----> 2 return a / b
 3
ipdb> print(a)
1
ipdb> print(b)
0
ipdb> quit

\1

--- СТРАНИЦА 48 ---

\1n: за пределами обычного Python 
In[8]: %debug
> <ipython-input-1-d849e34d61fb>(2)func1()
 1 def func1(a, b):
----> 2 return a / b
 3
ipdb> up
> <ipython-input-1-d849e34d61fb>(7)func2()
 5 a = x
 6 b = x – 1
----> 7 return func1(a, b)
ipdb> print(x)
1
ipdb> up
> <ipython-input-6-b2e110f6fc8f>(1)<module>()
----> 1 func2(1)
ipdb> down
> <ipython-input-1-d849e34d61fb>(7)func2()
 5 a = x
 6 b = x – 1
----> 7 return func1(a, b)
ipdb> quit

\1
In[9]: %xmode Plain
 %pdb on
 func2(1)
Exception reporting mode: Plain
Automatic pdb calling has been turned ON
Traceback (most recent call last):
 File "<ipython-input-9-569a67d2d312>", line 3, in <module>
 func2(1)
 File "<ipython-input-1-d849e34d61fb>", line 7, in func2
 return func1(a, b)

--- СТРАНИЦА 49 ---
Профилирование и мониторинг скорости выполнения кода 4 9
 File "<ipython-input-1-d849e34d61fb>", line 2, in func1
 return a / b 
ZeroDivisionError: division by zero
> <Ipython-input-1-d849e34d61fb>(2)func1()
 1 def func1(a, b):
----> 2 return a / b
 3
ipdb> print(b)
0
ipdb> quit
Наконец, если у вас есть сценарий, который вы хотели бы запускать в начале ра-
боты в интерактивном режиме, можно запустить его с помощью команды %run -d 
и использовать команду next для пошагового интерактивного перемещения по 
строкам кода.
Неполный список команд отладки. Для интерактивной отладки доступно намного 
больше команд, чем мы перечислили (табл. 1.5).
Таблица 1.5. Наиболее часто используемые команды
Команда Описание
list Отображает текущее место в файле
h(elp) Отображает список команд или справку по конкретной команде
q(uit) Выход из отладчика и программы
c(ontinue) Выход из отладчика, продолжение выполнения программы
n(ext) Переход к следующему шагу программы
<enter> Повтор предыдущей команды
p(rint) Вывод значений переменных
s(tep) Вход в подпрограмму
r(eturn) Возврат из подпрограммы
Для дальнейшей информации воспользуйтесь командой help в отладчике или за-
гляните в онлайн-документацию по ipdb ( https://github.com/gotcha/ipdb).
Профилирование и мониторинг скорости 
выполнения кода
В процессе разработки кода и создания конвейеров обработки данных всегда при-
сутствуют компромиссы между различными реализациями. В начале создания ал-
горитма забота о подобных вещах может оказаться контрпродуктивной. Согласно 

--- СТРАНИЦА 50 ---

\1n: за пределами обычного Python 
знаменитому афоризму Дональда Кнута: «Лучше не держать в голове подобные 
“малые” вопросы производительности, скажем, 97 % времени: преждевременная 
оптимизация — корень всех зол».
Однако, как только ваш код начинает работать, будет полезно немного заняться 
его производительностью. Иногда бывает удобно проверить время выполнения 
заданной команды или набора команд, а иногда — поко паться в состоящем из 
множества строк процессе и выяснить, где находится узкое место каког о-либо 
сложного набора операций. Оболочка IPython предоставляет широкий выбор 
функциональности для выполнения подобного мониторинга скорости выпол -
нения кода и его профилирования. Здесь мы обсудим следующие «магические» 
команды оболочки IPython:
 %time — длительность выполнения отдельного оператора;
 %timeit — длительность выполнения отдельного оператора при неоднократном 
повторе, для большей точности;
 %prun — выполнение кода с использованием профилировщика;
 %lprun — пошаговое выполнение кода с применением профилировщика;
 %memit — оценка использования оперативной памяти для отдельного опера -
тора;
 %mprun — пошаговое выполнение кода с применением профилировщика па -
мяти.
Последние четыре команды не включены в пакет IPython — для их использования 
необходимо установить расширения line_profiler и memory_profiler, которые мы 
обсудим в следующих разделах.
Оценка времени выполнения фрагментов кода: %timeit 
и %time
Мы уже встречались со строчной «магической» командой %timeit и блочной «маги-
ческой» командой %%timeit во введении в «магические» функции в разделе «“Ма-
гические” команды IPython» данной главы; команду %%timeit можно использовать 
для оценки времени многократного выполнения фрагментов кода:
In[1]: %timeit sum(range(100))
1 00000 loops, best of 3: 1.54 µs per loop

\1

--- СТРАНИЦА 51 ---
Профилирование и мониторинг скорости выполнения кода 5 1
In[2]: %%timeit
 total = 0
 for i in range(1000):
 for j in range(1000):
 total += i * (-1) ** j 
1 loops, best of 3: 407 ms per loop

\1
In[3]: import random
 L = [random.random() for i in range(100000)]
 %timeit L.sort()
100 loops, best of 3: 1.9 ms per loop

\1
In[4]: import random
 L = [random.random() for i in range(100000)]
 print("sorting an unsorted list:")
 %time L.sort()
sorting an unsorted list:
CPU times: user 40.6 ms, sys: 896 µs, total: 41.5 ms
Wall time: 41.5 ms
In[5]: print("sorting an already sorted list:")
 %time L.sort()
sorting an already sorted list:
CPU times: user 8.18 ms, sys: 10 µs, total: 8.19 ms
Wall time: 8.24 ms
Обратите внимание на то, насколько быстрее сортируется предварительно отсорти-
рованный список, а также насколько больше времени занимает оценка с помощью 
функции %time по сравнению с функцией %timeit, даже в случае предварительно 
отсортированного списка! Это происходит в результате того, что «магическая» 
функция %timeit незаметно для нас осуществляет некоторые хитрые трюки, чтобы 
системные вызовы не мешали оценке времени. Например, она не допускает удале-
ния неиспользуемых объектов Python (известного как сбор мусора), которое могло 
бы повлиять на оценку времени. Именно поэтому выдаваемое функцией %timeit 
время обычно заметно короче выдаваемого функцией %time.

--- СТРАНИЦА 52 ---

\1n: за пределами обычного Python 

\1
In[6]: %%time
 total = 0
 for i in range(1000):
 for j in range(1000):
 total += i * (-1) ** j 
CPU times: user 504 ms, sys: 979 µs, total: 505 ms
Wall time: 505 ms
Для получения дальнейшей информации по «магическим» функциям %time 
и %timeit, а также их параметрам воспользуйтесь справочными функциями обо-
лочки IPython (то есть наберите %time? в командной строке IPython).
Профилирование сценариев целиком: %prun
Программы состоят из множества отдельных операторов, и иногда оценка времени 
их выполнения в контексте важнее, чем по отдельности. В языке Python имеется 
встроенный профилировщик кода (о котором можно прочитать в документации 
языка Python), но оболочка IPython предоставляет намного более удобный способ 
его использования в виде «магической» функции %prun.

\1
In[7]: def sum_of_lists(N):
 total = 0
 for i in range(5):
 L = [j ^ (j >> i) for j in range(N)]
 total += sum(L)
 return total
Теперь мы можем обратиться к «магической» функции %prun с указанием вызова 
функции, чтобы увидеть результаты профилирования:
In[8]: %prun sum_of_lists(10 00000)

\1
14 function calls in 0.714 seconds
 Ordered by: internal time
 ncalls tottime percall cumtime percall filename:lineno(function)

\1n-input-
 19>:4(<listcomp>)
 5 0.064 0.013 0.064 0.013 {built-in method sum}
 1 0.036 0.036 0.699 0.699 <ipython-input-
 19>:1(sum_of_lists)
 1 0.014 0.014 0.714 0.714 <string>:1(<module>)
 1 0.000 0.000 0.714 0.714 {built-in method exec}
Результат представляет собой таблицу, указывающую в порядке общего затра -
ченного на каждый вызов функции времени, в каких местах выполнение занимает 
больше всего времени. В данном случае большую часть времени занимает списоч-
ное включение внутри функции sum_of_lists. Зная это, можно начинать обду -
мывать возможные изменения в алгоритме для улучшения производительности.
Для получения дополнительной информации по «магической» функции %prun, 
а также доступным для нее параметрам воспользуйтесь справочной функциональ-
ностью оболочки IPython (то есть наберите %prun? В командной строке IPython).
Пошаговое профилирование с помощью %lprun
Профилирование по функциям с помощью функции %prun довольно удобно, но 
иногда больше пользы может принести построчный отчет профилировщика. Такая 
функциональность не встроена в язык Python или оболочку IPython, но можно 
установить пакет line_profiler, обладающий такой возможностью. Начнем с ис-
пользования утилиты языка Python для работы с пакетами, pip, для установки 
пакета line_profiler:
$ pip install line_profiler
Далее можно воспользоваться IPython для загрузки расширения line_profiler 
оболочки IPython, предоставляемой в виде части указанного пакета:
In[9]: %load_ext line_profiler
Теперь команда %lprun может выполнить построчное профилирование любой 
функции. В нашем случае необходимо указать ей явным образом, какие функции 
мы хотели быть профилировать:
In[10]: %lprun –f sum_of_lists sum_of_lists(5000)

\1
Timer unit: 1e-06 s 
Total time: 0.009382 s File: <ipython-input-19-fa2be176cc3e>
Function: sum_of_lists at line 1
Line # Hits Time Per Hit % Time Line Contents
==============================================================
 1 def sum_of_lists(N):
 2 1 2 2.0 0.0 total = 0

--- СТРАНИЦА 54 ---

\1n: за пределами обычного Python 
 3 6 8 1.3 0.1 for i in range(5):
 4 5 9001 1800.2 95.9 L = [j ^ (j >> i)...
 5 5 371 74.2 4.0 total += sum(L)
 6 1 0 0.0 0.0 return total
Информация в заголовке дает нам ключ к чтению результатов: время указывается 
в микросекундах, и мы можем увидеть, в каком месте выполнение программы за-
нимает наибольшее количество времени. На этой стадии мы получаем возможность 
применить эту информацию для модификации кода и улучшения его производи-
тельности для желаемого сценария использования.
Для получения дополнительной информации о «магической» функции %lprun, 
а также о доступных для нее параметрах воспользуйтесь справочной функци -
ональностью оболочки IPython (то есть наберите %lprun? в командной строке 
IPython).
Профилирование использования памяти: %memit 
и %mprun
Другой аспект профилирования — количество используемой операциями памяти. 
Это количество можно оценить с помощью еще одного расширения оболочки 
IPython — memory_profiler. Как и в случае с утилитой line_profiler, мы начнем 
с установки расширения с помощью утилиты pip:
$ pip install memory_profiler
Затем можно воспользоваться оболочкой IPython для загрузки эт ого расширения:
In[12]: %load_ext memory_profiler
Расширение профилировщика памяти содержит две удобные «магические» функ-
ции: %memit (аналог %timeit для измерения количества памяти) и %mprun (аналог 
%lprun для измерения количества памяти). Применять функцию %memit несложно:
In[13]: %memit sum_of_lists(1000000)
peak memory: 100.08 MiB, increment: 61.36 MiB
Мы видим, что данная функция использует около 100 Мбайт памяти.
Для построчного описания применения памяти можно использовать «магическую» 
функцию %mprun. К сожалению, она работает только для функций, описанных в от-
дельных модулях, а не в самом блокноте, так что начнем с применения «магиче -
ской» функции %%file для создания простого модуля под названием mprun_demo.py, 
содержащего нашу функцию sum_of_lists, с одним дополнением, которое немного 
прояснит нам результаты профилирования памяти:

--- СТРАНИЦА 55 ---
Профилирование и мониторинг скорости выполнения кода 5 5
In[14]: %%file mprun_demo.py
 def sum_of_lists(N):
 total = 0
 for i in range(5):
 L = [j ^ (j >> i) for j in range(N)]
 total += sum(L)
 del L # Удалить ссылку на L
 return total
Overwriting mprun_demo.py

\1
In[15]: from mprun_demo import sum_of_lists
 %mprun –f sum_of_lists sum_of_lists(1000000)

\1
Filename: ./mprun_demo.py
Line # Mem usage Increment Line Contents
==============================================================
 4 71.9 MiB 0.0 MiB L = [j ^ (j >> i) for j in range(N)]
Filename: ./mprun_demo.py
Line # Mem usage Increment Line Contents
==============================================================
 1 39.0 MiB 0.0 MiB def sum_of_lists(N):
 2 39.0 MiB 0.0 MiB total = 0
 3 46.5 MiB 7.5 MiB for i in range(5):
 4 71.9 MiB 25.4 MiB L = [j ^ (j >> i)
 for j in range(N)]
 5 71.9 MiB 0.0 MiB total += sum(L)
 6 46.5 MiB -25.4 MiB del L # Удалить ссылку на L
 7 39.1 MiB -7.4 MiB return total
Здесь столбец Increment сообщает нам, насколько каждая строка отражается в об-
щем объеме памяти. Обратите внимание, что при создании и удалении списка L, 
помимо фонового использования памяти самим интерпретатором языка Python, 
использование памяти увеличивается примерно на 25 Мбайт.
Для получения дополнительной информации о «магических» функциях %memit 
и %mprun, а также о доступных для них параметрах воспользуйтесь справочной 
функциональностью оболочки IPython (то есть наберите %memit? в командной 
строке IPython).

--- СТРАНИЦА 56 ---

\1n: за пределами обычного Python 
Дополнительные источники информации 
об оболочке IPython
В данной главе мы захватили лишь верхушку айсберга по использованию языка 
Python для задач науки о данных. Гораздо больше информации доступно как в пе-
чатном виде, так и в Интернете. Здесь мы приведем некоторые дополнительные 
ресурсы, которые могут вам пригодиться.
Веб-ресурсы
 Сайт IPython ( http://ipython.org/). Сайт IPython содержит ссылки на документа-
цию, примеры, руководства и множество других ресурсов.
 Сайт nbviewer ( http://nbviewer.ipython.org/). Этот сайт демонстрирует статические 
визуализации всех доступных в Интернете блокнотов оболочки IPython. Глав -
ная его страница показывает несколько примеров блокнотов, которые можно 
пролистать, чтобы увидеть, для чего другие разработчики используют язык 
Python!
 Галерея интересных блокнотов оболочки IPython ( http://github.com/ipython/ipython/
wiki/A-gallery-of-interesting-ipython-Notebooks/ ) . Этот непрерывно растущий список 
блокнотов, поддерживаемый nbviewer, демонстрирует глубину и размах чис -
ленного анализа, возможного с помощью оболочки IPython. Он включает все, 
начиная от коротких примеров и руководств и заканчивая полноразмерными 
курсами и книгами, составленными в формате блокнотов!
 Видеоруководства. В Интернете вы найдете немало видеоруководств по 
оболочке IPython. Особенно рекомендую руководства Фернандо Переса 
и Брайана Грейнджера — двух основных разработчиков, создавших и под -
держивающих оболочку IPython и проект Jupiter, — с конференций PyCon, 
SciPy and PyData.
Книги
 Python for Data Analysis 1 ( http://bit.ly/python-for-data-analysis). Эта книга Уэса Мак-
кинли включает главу, описывающую использование оболочки IPython с точки 
зрения исследователя данных. Хотя многое из ее материала пересекается с тем, 
что мы тут обсуждали, вторая точка зрения никогда не помешает.
 Learning IPython for Interactive Computing and Data Visualization ( «Изучаем обо-
лочку IPython для целей интерактивных вычислений и визуализации данных», 
http://bit.ly/2eLCBB7). Эта книга Цириллы Россан предлагает неплохое введение 
в использование оболочки IPython для анализа данных.

\1n и анализ данных. — М.: ДМК-Пресс, 2015.

--- СТРАНИЦА 57 ---
Дополнительные источники информации об оболочке IPython 5 7
 IPython Interactive Computing and Visualization Cookbook ( «Справочник по ин -
терактивным вычислениям и визуализации с помощью языка IP ython», http://
bit.ly/2fCEtNE). Данная книга также написана Цириллой Россан и представляет 
собой более длинное и продвинутое руководство по использованию оболочки 
IPython для науки о данных. Несмотря на название, она посвящена не только 
оболочке IPython, в книге рассмотрены некоторые детали широкого спектра 
проблем науки о данных.
Вы можете и сами найти справочные материалы: основанная на символе ? спра -
вочная функциональность оболочки IPython (обсуждавшаяся в разделе «Справка 
и документация в оболочке Python» этой главы) может оказаться чрезвычайно 
полезной, если ее использовать правильно. При работе с примерами из данной 
книги и другими применяйте ее для знакомства со всеми утилитами, которые 
предоставляет IPython.

--- СТРАНИЦА 58 ---

\1n, находящимися в оперативной памяти. Тематика 
очень широка, ведь наборы данных могут поступать из разных источников и быть 
различных форматов (наборы документов, изображений, аудиоклипов, наборы 
численных измерений и др.). Несмотря на столь очевидную разнородность, все эти 
данные удобно рассматривать как массивы числовых значений.
Изображения (особенно цифровые) можно представить в виде простых двумерных 
массивов чисел, отражающих яркость пикселов соответствующей области; аудио-
клипы — как одномерные массивы интенсивности звука в соответствующие мо -
менты времени. Текст можно преобразовать в числовое представление различными 
путями, например с двоичными числами, представляющими частоту определенных 
слов или пар слов. Вне зависимости от характера данных первым шагом к их анали-
зу является преобразование в числовые массивы (мы обсудим некоторые примеры 
этого процесса в разделе «Проектирование признаков» главы 5).
Эффективное хранение и работа с числовыми массивами очень важны для про -
цесса исследования данных. Мы изучим специализированные инструменты языка 
Python, предназначенные для обработки подобных массивов, — пакет NumPy 
и пакет Pandas (см. главу 3).
Библиотека NumPy (сокращение от Numerical Python — «числовой Python») обе-
спечивает эффективный интерфейс для хранения и работы с плотными буферами 
данных. Массивы библиотеки NumPy похожи на встроенный тип данных языка 
Python list, но обеспечивают гораздо более эффективное хранение и операции 
с данными при росте размера массивов. Массивы библиотеки NumPy формиру -
ют ядро практически всей экосистемы утилит для исследования данных Python. 
Время, проведенное за изучением способов эффективного использования пакета 

--- СТРАНИЦА 59 ---
Работа с типами данных в языке Python 5 9
NumPy, не будет потрачено впустую вне зависимости от интересующего вас аспек -
та науки о данных.
Если вы последовали приведенному в предисловии совету и установили стек 
утилит Anaconda, то пакет NumPy уже готов к использованию. Если же вы от -
носитесь к числу тех, кто любит все делать своими руками, то перейдите на сайт 
NumPy и следуйте имеющимся там указаниям. После этого импортируйте NumPy 
и тщательно проверьте его версию:
In[1]: import numpy
 numpy.__version__
Out[1]: '1.11.1'
Для используемых здесь частей этого пакета я рекомендовал бы применять NumPy 
версии 1.8 или более поздней. По традиции, большинство людей в мире SciPy/
PyData импортируют пакет NumPy, используя в качестве его псевдонима np:
In[2]: import numpy as np
На протяжении книги мы будем именно так импортировать и применять NumPy.
Напоминание о встроенной документации
Читая данную главу, не забывайте, что оболочка IPython позволяет быстро про -
сматривать содержимое пакетов (посредством автодополнения при нажатии кла -
виши Tab), а также документацию по различным функциям (используя символ ?). 
Загляните в раздел «Справка и документация в оболочке Python» главы 1, если вам 
нужно освежить в памяти эти возможности.
Например, для отображения всего содержимого пространства имен numpy можете 
ввести команду:
In [3]: np.<TAB>

\1
In [4]: np?
Более подробную документацию, а также руководства и другие источники информа-
ции можно найти на сайте http://www.numpy.org.
Работа с типами данных в языке Python
Чтобы достичь эффективности научных вычислений, ориентированных на работу 
с данными, следует знать, как хранятся и обрабатываются данные. В этом разделе 
описываются и сравниваются способы обработки массивов данных в языке Python, 
а также вносимые в этот процесс библиотекой NumPy усовершенствования. Зна -
ние различий очень важно для понимания большей части материала в книге.

--- СТРАНИЦА 60 ---

\1n зачастую привлекает его простота, немаловажной 
частью которой является динамическая типизация. В то время как в языках со 
статической типизацией, таких как C и Java, необходимо явным о бразом объяв -
лять все переменные, языки с динамической типизацией, например Python, этого 
не требуют. Например, в языке C можно описать операцию следующим образом:
/* Код на языке C */
int result = 0;
for(int i=0; i<100; i++){
 result += i;
}
На языке Python соответствующую операцию можно описать так:
# Код на языке Python
result = 0
for i in range(100):
 result += i
Обратите внимание на главное отличие: в языке C типы данных каждой перемен-
ной объявлены явным образом, а в Python они определяются динамически. Это 
значит, что мы можем присвоить любой переменной данные любого типа:
# Код на языке Python
x = 4
x = "four"

\1
/* Код на языке C */
int x = 4;
x = "four"; // СБОЙ
Подобная гибкость делает Python и другие языки с динамической типизацией 
удобными и простыми в использовании. Понимать, как это работает, очень важно, 
чтобы научиться эффективно анализировать данные с помощью языка Python. Од-
нако такая гибкость при работе с типами указывает на то, что переменные Python 
представляют собой нечто большее, чем просто значение, они содержат также 
дополнительную информацию о типе значения. Мы рассмотрим это подробнее 
в следующих разделах.
Целое число в языке Python — больше, чем просто 
целое число
Стандартная реализация языка Python написана на языке C. Это значит, что 
каждый объект Python — просто искусно замаскированная структура языка C, со -

--- СТРАНИЦА 61 ---
Работа с типами данных в языке Python 6 1
держащая не только значение, но и другую информацию. Например, при описании 
целочисленной переменной на языке Python, такой как x = 10000, x представляет 
собой не просто «чистое» целое число. На самом деле это указатель на составную 
структуру языка C, содержащую несколько значений. Посмотрев в исходный код 
Python 3.4, можно узнать, что описание целочисленного типа (типа long) факти-
чески выглядит следующим образом (после разворачивания макросов языка C):
struct _longobject {
 long ob_refcnt;
 PyTypeObject *ob_type;
 size_t ob_size;
 long ob_digit[1];
};
Отдельное целое число в языке Python 3.4 фактически состоит из четырех частей:
 ob_refcnt — счетчика ссылок, с помощью которого Python незаметно выполняет 
выделение и освобождение памяти;
 ob_type — кодирующей тип переменной;
 ob_size — задающей размер следующих элементов данных;
 ob_digit — содержащей фактическое целочисленное значение, к оторое пред -
ставляет переменная языка Python.
Это значит, что существует некоторая избыточность при хранении целого числа 
в языке Python по сравнению с целым числом в компилируемых языках, таких 
как C (рис. 2.1).
Рис. 2.1. Разница между целыми числами в языках C и Python
PyObject_HEAD на этом рисунке — часть структуры, содержащая счетчик ссылок, 
код типа и другие упомянутые выше элементы.
Еще раз акцентируем внимание на основном отличии: целое число в языке C пред -
ставляет собой ярлык для места в памяти, байты в котором кодируют целочислен-
ное значение. Целое число в Python — указатель на место в памяти, где хранится 
вся информация об объекте языка Python, включая байты, содержащие целочис-
ленное значение. Именно эта дополнительная информация в структуре Python для 

--- СТРАНИЦА 62 ---

\1n с использованием динамической типизации. Однако эта дополнительная 
информация в типах Python влечет и накладные расходы, что становится особенно 
заметно в структурах, объединяющих значительное количество таких объектов.
Список в языке Python — больше, чем просто список
Теперь рассмотрим, что происходит при использовании структуры языка Python, 
содержащей много объектов. Стандартным изменяемым многоэлементным контей-
нером в Python является список. Создать список целочисленных значений можно 
следующим образом:
In[1]: L = list(range(10))
 L 
Out[1]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
In[2]: type(L[0])
Out[2]: int

\1
In[3]: L2 = [strI for c in L]
 L2
Out[3]: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
In[4]: type(L2[0])
Out[4]: str
В силу динамической типизации языка Python можно создавать даже неоднород-
ные списки:
In[5]: L3 = [True, "2", 3.0, 4]
 [type(item) for item in L3]
Out[5]: [bool, str, float, int] Однако подобная гибкость имеет свою цену: для ис -
пользования гибких типов данных каждый элемент списка должен содержать ин-
формацию о типе, счетчик ссылок и другую информацию, то есть каждый элемент 
представляет собой целый объект языка Python. В частном случае совпадения типа 
всех переменных большая часть этой информации избыточна: намного рациональ-
нее хранить данные в массиве с фиксированным типом значений. Различие между 
списком с динамическим типом значений и списком с фиксированным типом 
(в стиле библиотеки NumPy) проиллюстрировано на рис. 2.2.

--- СТРАНИЦА 63 ---
Работа с типами данных в языке Python 6 3
PyObject_HEAD 1
2
3
4
5
6
7
8
PyObject_HEAD
0x310718
0x310748
0x310730
0x310760
0x310700
0x3106b8
0x3106d0
0x3106e8
Данные
Измерения
Шаги по индексу
Длина
Элементы
Массив
из библиотеки
NumPy Список языка
Python
Рис. 2.2. Различие между списками в языках C и Python
На уровне реализации массив фактически содержит один указатель на непрерыв-
ный блок данных. Список в языке Python же содержит указатель на блок указате-
лей, каждый из которых, в свою очередь, указывает на целый объект языка Python, 
например, целое число. Преимущество такого списка состоит в его гибкости: раз 
каждый элемент списка — полномасштабная структура, содержащая как данные, 
так и информацию о типе, список можно заполнить данными любого требуемого 
типа. Массивам с фиксированным типом из библиотеки NumPy недостает этой 
гибкости, но они гораздо эффективнее хранят данные и работают с ними.
Массивы с фиксированным типом в языке Python
Язык Python предоставляет несколько возможностей для хранения данных в эф-
фективно работающих буферах с фиксированным типом значений. Встроенный 
модуль array (доступен начиная с версии 3.3 языка Python) можно использовать 
для создания плотных массивов данных одного типа:
In[6]: import array
 L = list(range(10))
 A = array.array('i', L)
 A 
Out[6]: array('i', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

--- СТРАНИЦА 64 ---

\1ndarray из библиотеки NumPy. В то время как объект 
array языка Python обеспечивает эффективное хранение данных в формате мас-
сива, библиотека NumPy добавляет еще и возможность выполнения эффективных 
операций над этими данными. Мы рассмотрим такие операции в следующих раз-
делах, а здесь продемонстрируем несколько способов создания NumPy-массива.
Начнем с обычного импорта пакета NumPy под псевдонимом np:
In[7]: import numpy as np
Создание массивов из списков языка Python
Для создания массивов из списков языка Python можно воспользоваться функ -
цией np.array:
In[8]: # массив целочисленных значений:
 np.array([1, 4, 2, 5, 3])
Out[8]: array([1, 4, 2, 5, 3])
В отличие от списков языка Python библиотека NumPy ограничивается массивами, 
содержащими элементы одного типа. Если типы элементов не совпадают, NumPy 
попытается выполнить повышающее приведение типов (в данном случае цело -
численные значения приводятся к числам с плавающей точкой):
In[9]: np.array([3.14, 4, 2, 3])
Out[9]: array([ 3.14, 4. , 2. , 3. ])

\1
In[10]: np.array([1, 2, 3, 4], dtype='float32')
Out[10]: array([ 1., 2., 3., 4.], dtype=float32)
Наконец, в отличие от списков в языке Python массивы библиотеки NumPy могут 
явным образом описываться как многомерные. Вот один из способов задания зна-
чений многомерного массива с помощью списка списков:
In[11]: # Вложенные списки преобразуются в многомерный массив
 np.array([range(i, i + 3) for i in [2, 4, 6]])
Out[11]: array([[2, 3, 4],
 [4, 5, 6],
 [6, 7, 8]])
Вложенные списки рассматриваются как строки в итоговом двумерном массиве.

--- СТРАНИЦА 65 ---
Работа с типами данных в языке Python 6 5

\1
In[12]: # Создаем массив целых чисел длины 10, заполненный нулями
 np.zeros(10, dtype=int)
Out[12]: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
In[13]: # Создаем массив размером 3 x 5 значений с плавающей точкой, 
 # заполненный единицами
 np.ones((3, 5), dtype=float)
Out[13]: array([[ 1., 1., 1., 1., 1.],
 [ 1., 1., 1., 1., 1.],
 [ 1., 1., 1., 1., 1.]])
In[14]: # Создаем массив размером 3 x 5, заполненный значением 3.14
 np.full((3, 5), 3.14)
Out[14]: array([[ 3.14, 3.14, 3.14, 3.14, 3.14],
 [ 3.14, 3.14, 3.14, 3.14, 3.14],
 [ 3.14, 3.14, 3.14, 3.14, 3.14]])
In[15]: # Создаем массив, заполненный линейной последовательностью,
 # начинающейся с 0 и заканчивающейся 20, с шагом 2
 # (аналогично встроенной функции range())
 np.arange(0, 20, 2)
Out[15]: array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18])
In[16]: # Создаем массив из пяти значений, 
 # равномерно располагающихся между 0 и 1
 np.linspace(0, 1, 5)
Out[16]: array([ 0. , 0.25, 0.5 , 0.75, 1. ])
In[17]: # Создаем массив размером 3 x 3 равномерно распределенных
 # случайных значения от 0 до 1
 np.random.random((3, 3))
Out[17]: array([[ 0.99844933, 0.52183819, 0.22421193],
 [ 0.08007488, 0.45429293, 0.20941444],
 [ 0.14360941, 0.96910973, 0.946117 ]])
In[18]: # Создаем массив размером 3 x 3 нормально распределенных
 # случайных значения с медианой 0 и стандартным отклонением 1
 np.random.normal(0, 1, (3, 3))
Out[18]: array([[ 1.51772646, 0.39614948, -0.10634696],

--- СТРАНИЦА 66 ---

\1n[19]: # Создаем массив размером 3 x 3 случайных целых числа 
 # в промежутке [0, 10)
 np.random.randint(0, 10, (3, 3))
Out[19]: array([[2, 3, 4],
 [5, 7, 8],
 [0, 5, 0]])
In[20]: # Создаем единичную матрицу размером 3 x 3
 np.eye(3)
Out[20]: array([[ 1., 0., 0.],
 [ 0., 1., 0.],
 [ 0., 0., 1.]])
In[21]: # Создаем неинициализированный массив из трех целочисленных 
 # значений. Значениями будут произвольные, случайно оказавшиеся 
 # в соответствующих ячейках памяти данные
 np.empty(3)
Out[21]: array([ 1., 1., 1.])
Стандартные типы данных библиотеки NumPy
Массивы библиотеки NumPy содержат значения простых типов, поэтому важно знать 
все подробности об этих типах и их ограничениях. Поскольку библиотека NumPy на -
писана1 на языке C, эти типы будут знакомы пользователям языков C, Fortran и др.

\1
np.zeros(10, dtype='int16')
или соответствующего объекта из библиотеки NumPy:
Np.zeros(10, dtype=np.int16)
Таблица 2.1. Стандартные типы данных библиотеки NumPy
Тип данных Описание
bool_ Булев тип (True или False), хранящийся в виде 1 байта
int_
Тип целочисленного значения по умолчанию (аналогичен типу long языка C; 
обычно int64 или int32)
intc Идентичен типу int языка C (обычно int32 или int64)

\1ntp
Целочисленное значение, используемое для индексов (аналогично типу ssize_t 
языка C; обычно int32 или int64)
int8 Байтовый тип (от –128 до 127)
int16 Целое число (от –32 768 до 32 767)
int32 Целое число (от –2 147 483 648 до 2 147 483 647)
int64 Целое число (от –9 223 372 036 854 775 808 до 9 223 372 036 854 775 807)
uint8 Беззнаковое целое число (от 0 до 255)
uint16 Беззнаковое целое число (от 0 до 65 535)
uint32 Беззнаковое целое число (от 0 до 4 294 967 295)
uint64 Беззнаковое целое число (от 0 до 18 446 744 073 709 551 615)
float_ Сокращение для названия типа float64
float16
Число с плавающей точкой с половинной точностью: 1 бит знак, 5 бит порядок, 
10 бит мантисса
float32
Число с плавающей точкой с одинарной точностью: 1 бит знак, 8 бит порядок, 
23 бита мантисса
float64
Число с плавающей точкой с удвоенной точностью: 1 бит знак, 11 бит порядок, 
52 бита мантисса
complex_ Сокращение для названия типа complex128
complex64 Комплексное число, представленное двумя 32-битными числами
complex128 Комплексное число, представленное двумя 64-битными числами
Возможно задание и более сложных типов, например задание чисел с прямым 
или обратным порядком байтов. Для получения более подробной информации за-
гляните в документацию по пакету NumPy ( http://numpy.org/). Библиотека NumPy 
также поддерживает составные типы данных, которые мы рассмотрим в разделе 
«Структурированные данные: структурированные массивы би блиотеки NumPy» 
данной главы.
Введение в массивы библиотеки NumPy
Работа с данными на языке Python — практически синоним работы с массивами 
библиотеки NumPy: даже более новые утилиты, например библиотека Pandas 
(см. главу 3), основаны на массивах NumPy. В этом разделе мы рассмотрим не -
сколько примеров использования массивов библиотеки NumPy для доступа к дан -
ным и подмассивам, а также срезы, изменения формы и объединения массивов. 
Хотя демонстрируемые типы операций могут показаться несколько скучными 
и педантичными, они являются своеобразными «кирпичиками» для множества 
других примеров из этой книги. Изучите их хорошенько!

--- СТРАНИЦА 68 ---

\1n[1]: import numpy as np
 np.random.seed(0) # начальное значение для целей воспроизводимости
 x1 = np.random.randint(10, size=6) # одномерный массив
 x2 = np.random.randint(10, size=(3, 4)) # двумерный массив
 x3 = np.random.randint(10, size=(3, 4, 5)) # трехмерный массив
У каждого из массивов есть атрибуты ndim (размерность), shape (размер каждого 
измерения) и size (общий размер массива):
In[2]: print("x3 ndim: ", x3.ndim)
 print("x3 shape:", x3.shape)
 print("x3 size: ", x3.size)
x3 ndim: 3
x3 shape: (3, 4, 5)
x3 size: 60
Еще один полезный атрибут — dtype, тип данных массива (который мы уже ранее 
обсуждали в разделе «Работа с типами данных в языке Python» этой главы):
In[3]: print("dtype:", x3.dtype)
dtype: int64
Другие атрибуты включают itemsize, выводящий размер (в байтах) каждого эле-
мента массива, и nbytes, выводящий полный размер массива (в байтах):
In[4]: print("itemsize:", x3.itemsize, "bytes")

--- СТРАНИЦА 69 ---
Введение в массивы библиотеки NumPy 6 9
 print("nbytes:", x3.nbytes, "bytes")
itemsize: 8 bytes
nbytes: 480 bytes
В общем значение атрибута nbytes должно быть равно значению атрибута itemsize, 
умноженному на size.
Индексация массива: доступ к отдельным элементам
Если вы знакомы с индексацией стандартных списков языка Python, то индексация 
библиотеки NumPy будет для вас привычной. В одномерном массиве обратиться 
к i -му (считая с 0) значению можно, указав требуемый индекс в квадратных скоб -
ках, точно так же, как при работе со списками языка Python:
In[5]: x1
Out[5]: array([5, 0, 3, 3, 7, 9])
In[6]: x1[0]
Out[6]: 5
In[7]: x1[4]
Out[7]: 7

\1
In[8]: x1[-1]
Out[8]: 9
In[9]: x1[-2]
Out[9]: 7

\1
In[10]: x2
Out[10]: array([[3, 5, 2, 4],
 [7, 6, 8, 8],
 [1, 6, 7, 7]])
In[11]: x2[0, 0]
Out[11]: 3

--- СТРАНИЦА 70 ---

\1n[12]: x2[2, 0]
Out[12]: 1
In[13]: x2[2, -1]
Out[13]: 7
Вы также можете изменить значения, используя любую из перечисленных выше 
индексных нотаций.
In[14]: x2[0, 0] = 12
 x2
Out[14]: array([[12, 5, 2, 4],
 [ 7, 6, 8, 8],
 [ 1, 6, 7, 7]])
Не забывайте, что, в отличие от списков языка Python, у массивов NumPy фикси-
рованный тип данных. При попытке вставить в массив целых чисел значение с пла-
вающей точкой это значение будет незаметно усечено. Не попадитесь в ловушку!
In[15]: x1[0] = 3.14159 # это значение будет усечено!
 x1
Out[15]: array([3, 0, 3, 3, 7, 9])
Срезы массивов: доступ к подмассивам
Аналогично доступу к отдельным элементам массива можно использовать квадрат-
ные скобки для доступа к подмассивам с помощью срезов (slicing), обозначаемых 
знаком двоеточия ( :). Синтаксис срезов библиотеки NumPy соответствует анало-
гичному синтаксису для стандартных списков языка Python. Для доступа к срезу 
массива x используйте синтаксис:
x[начало:конец:шаг]
Если какие-либо из этих значений не указаны, значения применяются по умолча-
нию: начало = 0, конец = размер соответствующего измерения , шаг = 1. Мы рас-
смотрим доступ к массивам в одном и нескольких измерениях.
Одномерные подмассивы
In[16]: x = np.arange(10)
 x 
Out[16]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

--- СТРАНИЦА 71 ---
Введение в массивы библиотеки NumPy 7 1
In[17]: x[:5] # первые пять элементов
Out[17]: array([0, 1, 2, 3, 4])
In[18]: x[5:] # элементы после индекса = 5
Out[18]: array([5, 6, 7, 8, 9])
In[19]: x[4:7] # подмассив из середины
Out[19]: array([4, 5, 6])
In[20]: x[::2] # каждый второй элемент
Out[20]: array([0, 2, 4, 6, 8])
In[21]: x[1::2] # каждый второй элемент, начиная с индекса 1
Out[21]: array([1, 3, 5, 7, 9])

\1
In[22]: x[::-1] # все элементы в обратном порядке
Out[22]: array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])
In[23]: x[5::-2] # каждый второй элемент в обратном порядке, 
 # начиная с индекса 5
Out[23]: array([5, 3, 1])

\1
In[24]: x2
Out[24]: array([[12, 5, 2, 4],
 [ 7, 6, 8, 8],
 [ 1, 6, 7, 7]])
In[25]: x2[:2, :3] # две строки, три столбца
Out[25]: array([[12, 5, 2],
 [ 7, 6, 8]])
In[26]: x2[:3, ::2] # все строки, каждый второй столбец

--- СТРАНИЦА 72 ---

\1n[27]: x2[::-1, ::-1]
Out[27]: array([[ 7, 7, 6, 1],
 [ 8, 8, 6, 7],
 [ 4, 2, 5, 12]])

\1
In[28]: print(x2[:, 0]) # первый столбец массива x2
[12 7 1]
In[29]: print(x2[0, :]) # первая строка массива x2
[12 5 2 4]

\1
In[30]: print(x2[0]) # эквивалентно x2[0, :]
[12 5 2 4]
Подмассивы как предназначенные только для чтения 
представления
Срезы массивов возвращают представления (views), а не копии (copies) данных 
массива. Этим срезы массивов библиотеки NumPy отличаются от срезов списков 
языка Python (в списках срезы являются копиями). Рассмотрим уже знакомый 
нам двумерный массив:
In[31]: print(x2)
[[12 5 2 4]
 [ 7 6 8 8]
 [ 1 6 7 7]]

--- СТРАНИЦА 73 ---

\1
In[32]: x2_sub = x2[:2, :2]
 print(x2_sub)
[[12 5]
 [ 7 6]]

\1
In[33]: x2_sub[0, 0] = 99
 print(x2_sub)
[[99 5]
 [ 7 6]]
In[34]: print(x2)
[[99 5 2 4]
 [ 7 6 8 8]
 [ 1 6 7 7]]

\1
In[35]: x2_sub_copy = x2[:2, :2].copy()
 print(x2_sub_copy)
[[99 5]
 [ 7 6]]

\1
In[36]: x2_sub_copy[0, 0] = 42
 print(x2_sub_copy)
[[42 5]
 [ 7 6]]
In[37]: print(x2)
[[99 5 2 4]
 [ 7 6 8 8]
 [ 1 6 7 7]]

--- СТРАНИЦА 74 ---

\1n[38]: grid = np.arange(1, 10).reshape((3, 3))
 print(grid)
[[1 2 3]
 [4 5 6]
 [7 8 9]]
Обратите внимание, что размер исходного массива должен соответствовать раз -
меру измененного. По возможности метод reshape будет использовать предназна-
ченные только для чтения представления, но непрерывные буферы памяти найти 
удается не всегда.
Другой часто используемый паттерн изменения формы — преобразование одно -
мерного массива в двумерную матрицу-строку или матрицу-столбец. Для этого 
можно применить метод reshape, но лучше воспользоваться ключевым словом 
newaxis при выполнении операции среза:
In[39]: x = np.array([1, 2, 3])
 # Преобразование в вектор-строку с помощью reshape
 x.reshape((1, 3))
Out[39]: array([[1, 2, 3]])
In[40]: # Преобразование в вектор-строку посредством newaxis
 x[np.newaxis, :]
Out[40]: array([[1, 2, 3]])
In[41]: # Преобразование в вектор-столбец с помощью reshape
 x.reshape((3, 1))
Out[41]: array([[1],
 [2],
 [3]])
In[42]: # Преобразование в вектор-столбец посредством newaxis
 x[:, np.newaxis]
Out[42]: array([[1],
 [2],
 [3]])
В нашей книге мы будем часто встречать подобное преобразование.

--- СТРАНИЦА 75 ---
Введение в массивы библиотеки NumPy 7 5
Слияние и разбиение массивов
Все предыдущие операции работали с одним массивом. Но можно объединить 
несколько массивов в один и, наоборот, разбить единый массив на несколько под-
массивов. Эти операции мы рассмотрим далее.
Слияние массивов
Слияние, или объединение, двух массивов в библиотеке NumPy выполняется 
в основном с помощью методов np.concatenate , np.vstack и np.hstack. Метод 
np.concatenate принимает на входе кортеж или список массивов в качестве перво-
го аргумента:
In[43]: x = np.array([1, 2, 3])
 y = np.array([3, 2, 1])
 np.concatenate([x, y])
Out[43]: array([1, 2, 3, 3, 2, 1])

\1
In[44]: z = [99, 99, 99]
 print(np.concatenate([x, y, z]))
[ 1 2 3 3 2 1999999]
Для объединения двумерных массивов можно также использовать np.concatenate:
In[45]: grid = np.array([[1, 2, 3],
 [4, 5, 6]])
In[46]: # слияние по первой оси координат
 np.concatenate([grid, grid])
Out[46]: array([[1, 2, 3],
 [4, 5, 6],
 [1, 2, 3],
 [4, 5, 6]])
In[47]: # слияние по второй оси координат (с индексом 0)
 np.concatenate([grid, grid], axis=1)
Out[47]: array([[1, 2, 3, 1, 2, 3],
 [4, 5, 6, 4, 5, 6]])
Для работы с массивами с различающимися измерениями удобнее и понятнее 
использовать функции np.vstack (вертикальное объединение) и np.hstack (гори-
зонтальное объединение):

--- СТРАНИЦА 76 ---

\1n[48]: x = np.array([1, 2, 3])
 grid = np.array([[9, 8, 7],
 [6, 5, 4]])
 # Объединяет массивы по вертикали
 np.vstack([x, grid])
Out[48]: array([[1, 2, 3],
 [9, 8, 7],
 [6, 5, 4]])
In[49]: # Объединяет массивы по горизонтали
 y = np.array([[99],
 [99]])
 np.hstack([grid, y])
Out[49]: array([[ 9, 8, 7, 99],
 [ 6, 5, 4, 99]])
Функция np.dstack аналогично объединяет массивы по третьей оси.
Разбиение массивов
Противоположностью слияния является разбиение, выполняемое с помощью 
функций np.split, np.hsplit и np.vsplit. Каждой из них необходимо передавать 
список индексов, задающих точки раздела:
In[50]: x = [1, 2, 3, 99, 99, 3, 2, 1]
 x1, x2, x3 = np.split(x, [3, 5])
 print(x1, x2, x3)
[1 2 3] [99 99] [3 2 1]
Обратите внимание, что N точек раздела означают N + 1 подмассив. Соответству-
ющие функции np.hsplit и np.vsplit действуют аналогично:
In[51]: grid = np.arange(16).reshape((4, 4))
 grid
Out[51]: array([[ 0, 1, 2, 3],
 [ 4, 5, 6, 7],
 [ 8, 9, 10, 11],
 [12, 13, 14, 15]])
In[52]: upper, lower = np.vsplit(grid, [2])
 print(upper)
 print(lower)
[[0 1 2 3]
 [4 5 6 7]]
[[ 8 9 10 11]
 [12 13 14 15]]

--- СТРАНИЦА 77 ---
Выполнение вычислений над массивами библиотеки NumPy 7 7
In[53]: left, right = np.hsplit(grid, [2])
 print(left)
 print(right)
[[ 0 1]
 [ 4 5]
 [ 8 9]
 [12 13]]
[[ 2 3]
 [ 6 7]
 [10 11]
 [14 15]]
Функция np.dsplit аналогично разделяет массивы по третьей оси.
Выполнение вычислений над массивами 
библиотеки NumPy: универсальные функции
Библиотека NumPy предоставляет удобный и гибкий интерфейс для оптимизиро -
ванных вычислений над массивами данных.
Выполнение вычислений над массивами библиотеки NumPy может быть очень 
быстрым и очень медленным. Ключ к их ускорению — использование вектори -
зованных операций, обычно реализуемых посредством универсальных функций 
(universal functions, ufuncs) языка Python. В данном разделе будет обосновано, 
почему универсальные функции библиотеки NumPy необходимы и почему они 
могут намного ускорить выполнение повторяющихся вычислений над элементами 
массивов, а также познакомим вас с множеством наиболее распространенных и по-
лезных универсальных арифметических функций из библиотеки NumPy.
Медлительность циклов
Реализация языка Python по умолчанию (известная под названием CPython) 
выполняет некоторые операции очень медленно. Частично это происходит из-за 
динамической, интерпретируемой природы языка. Гибкость типов означает, что 
последовательности операций нельзя скомпилировать в столь же производитель-
ный машинный код, как в случае языков C и Fortran. В последнее время было 
предпринято несколько попыток справиться с этой проблемой:
 проект PyPy ( http://pypy.org), реализация языка Python с динамической компи-
ляцией;
 проект Cython ( http://cython.org), преобразующий код на языке Python в компи-
лируемый код на языке C;

--- СТРАНИЦА 78 ---

\1numba.pydata.org), преобразующий фрагменты кода на языке 
Python в быстрый LLVM-байткод.
У каждого проекта есть свои сильные и слабые стороны, но ни один из них пока не 
обошел стандартный механизм CPython по популярности.
Относительная медлительность Python обычно обнаруживается при повторении 
множества мелких операций, например при выполнении обработки всех элементов 
массива в цикле. Пусть у нас имеется массив значений и необходимо вычислить 
обратную величину каждого из них. Очевидное решение могло бы выглядеть сле-
дующим образом:
In[1]: import numpy as np
 np.random.seed(0)
 def compute_reciprocals(values):
 output = np.empty(len(values))
 for i in range(len(values)):
 output[i] = 1.0 / values[i]
 return output
 values = np.random.randint(1, 10, size=5)
 compute_reciprocals(values)
Out[1]: array([ 0.16666667, 1. , 0.25 , 0.25 , 0.125 ])
Такая реализация, вероятно, кажется вполне естественной разработчикам с опытом 
работы на языках программирования C или Java. Однако, оценив время выполне-
ния этого кода для большого объема данных, мы обнар ужим, что данная опера -
ция выполняется крайне медленно. Оценим это время с помощью «магической» 
функции %timeit оболочки IPython (обсуждавшейся в разделе «Профилирование 
и мониторинг скорости выполнения кода» главы 1):
In[2]: big_array = np.random.randint(1, 100, size=1000000)
 %timeit compute_reciprocals(big_array)
1 loop, best of 3: 2.91 s per loop
Выполнение миллионов операций и сохранение результата заняло несколько 
секунд! В наши дни, когда даже у смартфонов быстродействие измеряется в ги -
гафлопсах (то есть миллиардах операций с плавающей точкой в секунду), это 
представляется медленным практически до абсурда. Оказывается, проблема не 
в самих операциях, а в проверке типов и диспетчеризации функций, выполняемых 
CPython при каждом проходе цикла. Всякий раз, когда вычисляется обратная 
величина, Python сначала проверяет тип объекта и выполняет динамический по-
иск подходящей для этого типа функции. Если бы мы работали с компилируемым 
кодом, сведения о типе были бы известны до выполнения кода, а значит, результат 
вычислялся бы намного эффективнее.

--- СТРАНИЦА 79 ---

\1
In[3]: print(compute_reciprocals(values))
 print(1.0 / values)
[ 0.16666667 1. 0.25 0.25 0.125 ]
[ 0.16666667 1. 0.25 0.25 0.125 ]
Можем отметить, что векторизованная операция выполняется на несколько по -
рядков быстрее, чем стандартный цикл Python:
In[4]: %timeit (1.0 / big_array)
100 loops, best of 3: 4.6 ms per loop
Векторизованные операции в библиотеке NumPy реализованы посредством уни-
версальных функций (ufuncs), главная задача которых состоит в быстром выпол -
нении повторяющихся операций над значениями из массивов библиотеки NumPy . 

\1
In[5]: np.arange(5) / np.arange(1, 6)
Out[5]: array([ 0. , 0.5 , 0.66666667, 0.75 , 0.8 ])

\1
In[6]: x = np.arange(9).reshape((3, 3))
 2 ** x 
Out[6]: array([[ 1, 2, 4],
 [ 8, 16, 32],
 [ 64, 128, 256]])
Вычисления с применением векторизации посредством универсальных функ -
ций практически всегда более эффективны, чем их эквиваленты, реализованные 
с помощью циклов Python, особенно при росте размера массивов. Столкнувшись 
с подобным циклом в сценарии на языке Python, следует обдумать, не стоит ли 
заменить его векторизованным выражением.

--- СТРАНИЦА 80 ---

\1n. Можно 
выполнять обычные сложение, вычитание, умножение и деление:
In[7]: x = np.arange(4)
 print("x =", x)
 print("x + 5 =", x + 5)
 print("x - 5 =", x - 5)
 print("x * 2 =", x * 2)
 print("x / 2 =", x / 2)
 print("x // 2 =", x // 2) # деление с округлением в меньшую сторону
x = [0 1 2 3]
x + 5 = [5 6 7 8]
x - 5 = [-5 -4 -3 -2]
x * 2 = [0 2 4 6]
x / 2 = [ 0. 0.5 1. 1.5]
x // 2 = [0 0 1 1]

\1
In[8]: print("-x = ", -x)
 print("x ** 2 = ", x ** 2)
 print("x % 2 = ", x % 2)
-x = [ 0 -1 -2 -3]
x ** 2 = [0 1 4 9]
x % 2 = [0 1 0 1]

\1
In[9]: -(0.5*x + 1) ** 2
Out[9]: array([-1. , -2.25, -4. , -6.25])

\1
In[10]: np.add(x, 2)
Out[10]: array([2, 3, 4, 5])

--- СТРАНИЦА 81 ---
Выполнение вычислений над массивами библиотеки NumPy 8 1
В табл. 2.2 перечислены реализованные в библиотеке NumPy арифметические 
операторы.
Таблица 2.2. Реализованные в библиотеке NumPy арифметические операторы
Оператор Эквивалентная универсальная 
функция
Описание
+ np.add Сложение (например, 1 + 1 = 2)
– np.subtract Вычитание (например, 3 – 2 = 1)
– np.negative Унарная операция изменения знака (напри-
мер, –2)
* np.multiply Умножение (например, 2 * 3 = 6)
/ np.divide Деление (например, 3 / 2 = 1.5)
// np.floor_divide Деление с округлением в меньшую сторону 
(например, 3 // 2 = 1)
** np.power Возведение в степень (например, 2 ** 3 = 8)
% np.mod Модуль/остаток (например, 9 % 4 = 1)
Помимо этого, существуют еще логические/побитовые операции, которые мы рас -
смотрим в разделе «Сравнения, маски и булева логика» данной главы.
Абсолютное значение
Аналогично тому, что библиотека NumPy понимает встроенные арифметические 
операторы, она также понимает встроенную функцию абсолютного значения языка 
Python:
In[11]: x = np.array([-2, -1, 0, 1, 2])
 abs(x)
Out[11]: array([2, 1, 0, 1, 2])
Соответствующая универсальная функция библиотеки NumPy — np.absolute, до -
ступная также под псевдонимом np.abs:
In[12]: np.absolute(x)
Out[12]: array([2, 1, 0, 1, 2])
In[13]: np.abs(x)
Out[13]: array([2, 1, 0, 1, 2])

\1
In[14]: x = np.array([3 - 4j, 4 - 3j, 2 + 0j, 0 + 1j])

--- СТРАНИЦА 82 ---

\1np.abs(x)
Out[14]: array([ 5., 5., 2., 1.])

\1
In[15]: theta = np.linspace(0, np.pi, 3)

\1
In[16]: print("theta = ", theta)
 print("sin(theta) = ", np.sin(theta))
 print("cos(theta) = ", np.cos(theta))
 print("tan(theta) = ", np.tan(theta))
theta = [ 0. 1.57079633 3.14159265]
sin(theta) = [ 0.00000000e+00 1.00000000e+00 1.22464680e-16]
cos(theta) = [ 1.00000000e+00 6.12323400e-17 -1.00000000e+00]
tan(theta) = [ 0.00000000e+00 1.63312394e+16 -1.22464680e-16]

\1
In[17]: x = [-1, 0, 1]
 print("x = ", x)
 print("arcsin(x) = ", np.arcsin(x))
 print("arccos(x) = ", np.arccos(x))
 print("arctan(x) = ", np.arctan(x))
x = [-1, 0, 1]
arcsin(x) = [-1.57079633 0. 1.57079633]
arccos(x) = [ 3.14159265 1.57079633 0. ]
arctan(x) = [-0.78539816 0. 0.78539816]

\1
In[18]: x = [1, 2, 3]
 print("x =", x)
 print("e^x =", np.exp(x))
 print("2^x =", np.exp2(x))
 print("3^x =", np.power(3, x))

--- СТРАНИЦА 83 ---
Выполнение вычислений над массивами библиотеки NumPy 8 3
x = [1, 2, 3]
e^x = [ 2.71828183 7.3890561 20.08553692]
2^x = [ 2. 4. 8.]
3^x = [ 3 9 27]
Функции, обратные к показательным, и логарифмы также имеются в библиотеке. 
Простейшая функция np.log возвращает натуральный логарифм числа. Если вам 
требуется логарифм по основанию 2 или 10, они также доступны:
In[19]: x = [1, 2, 4, 10]
 print("x =", x)
 print("ln(x) =", np.log(x))
 print("log2(x) =", np.log2(x))
 print("log10(x) =", np.log10(x))
x = [1, 2, 4, 10]
ln(x) = [ 0. 0.69314718 1.38629436 2.30258509]
log2(x) = [ 0. 1. 2. 3.32192809]
log10(x) = [ 0. 0.30103 0.60205999 1. ]

\1
In[20]: x = [0, 0.001, 0.01, 0.1]
 print("exp(x) - 1 =", np.expm1(x))
 print("log(1 + x) =", np.log1p(x))
exp(x) - 1 = [ 0. 0.0010005 0.01005017 0.10517092]
log(1 + x) = [ 0. 0.0009995 0.00995033 0.09531018]
При очень малых значениях элементов вектора x данные функции возвращают 
намного более точные результаты, чем обычные функции np.log и np.exp.

\1

--- СТРАНИЦА 84 ---

\1n[21]: from scipy import special
In[22]: # Гамма-функции (обобщенные факториалы) и тому подобные функции
 x = [1, 5, 10]
 print("gamma(x) =", special.gamma(x))
 print("ln|gamma(x)| =", special.gammaln(x))
 print("beta(x, 2) =", special.beta(x, 2))
gamma(x) = [ 1.00000000e+00 2.40000000e+01 3.62880000e+05]
ln|gamma(x)| = [ 0. 3.17805383 12.80182748]
beta(x, 2) = [ 0.5 0.03333333 0.00909091]
In[23]: # Функция ошибок (интеграл от Гауссовой функции),
 # дополнительная и обратная к ней функции
 x = np.array([0, 0.3, 0.7, 1.0])
 print("erf(x) =", special.erf(x))
 print("erfc(x) =", special.erfc(x))
 print("erfinv(x) =", special.erfinv(x))
erf(x) = [ 0. 0.32862676 0.67780119 0.84270079]
erfc(x) = [ 1. 0.67137324 0.32219881 0.15729921]
erfinv(x) = [ 0. 0.27246271 0.73286908 inf]

\1
In[24]: x = np.arange(5)
 y = np.empty(5)
 np.multiply(x, 10, out=y)
 print(y)
[ 0. 10. 20. 30. 40.]

\1

--- СТРАНИЦА 85 ---
Выполнение вычислений над массивами библиотеки NumPy 8 5
In[25]: y = np.zeros(10)
 np.power(2, x, out=y[::2])
 print(y)
[ 1. 0. 2. 0. 4. 0. 8. 0. 16. 0.]

\1
In[26]: x = np.arange(1, 6)
 np.add.reduce(x)
Out[26]: 15

\1
In[27]: np.multiply.reduce(x)
Out[27]: 120

\1
In[28]: np.add.accumulate(x)
Out[28]: array([ 1, 3, 6, 10, 15])
In[29]: np.multiply.accumulate(x)
Out[29]: array([ 1, 2, 6, 24, 120])
Обратите внимание, что в данных конкретных случаях для вычисления этих зна-
чений существуют и специализированные функции библиотеки NumPy ( np.sum, 

--- СТРАНИЦА 86 ---

\1np.prod, np.cumsum, np.cumprod), которые мы рассмотрим в разделе «Агрегирование: 
минимум, максимум и все, что посередине» данной главы.

\1
In[30]: x = np.arange(1, 6)
 np.multiply.outer(x, x)
Out[30]: array([[ 1, 2, 3, 4, 5],
 [ 2, 4, 6, 8, 10],
 [ 3, 6, 9, 12, 15],
 [ 4, 8, 12, 16, 20],
 [ 5, 10, 15, 20, 25]])
Очень удобны методы ufunc.at и ufunc.reduceat, которые мы рассмотрим в разделе 
«“Прихотливая” индексация» данной главы.
Универсальные функции дают возможность работать с массивами различных 
размеров и форм, используя набор операций под названием транслирование 
(broadcasting). Эта тема достаточно важна, так что ей будет посвящен целый раздел 
(см. «Операции над массивами. Транслирование» данной главы).
Универсальные функции: дальнейшая информация
На сайтах документации библиотек NumPy и SciPy можно найти дополнительную 
информацию об универсальных функциях (включая полный список имеющихся 
функций).
Получить доступ к этой информации можно непосредственно из оболочки IPython 
путем импорта этих пакетов и использования автодополнения табуляц ией (кла -
виша Tab) и справочной функциональности ( ?), как описано в разделе «Справка 
и документация в оболочке Python» главы 1.
Агрегирование: минимум, максимум 
и все, что посередине
Очень часто при работе с большими объемами данных первый шаг заключается 
в вычислении сводных статистических показателей по этим данным. Среднее 
значение и стандартное отклонение, позволяющие выявить «типичные» значения 
в наборе данных, — наиболее распространенные сводные статистические показате-

--- СТРАНИЦА 87 ---
Агрегирование: минимум, максимум и все, что посередине 8 7
ли, но и другие сводные показатели также полезны (сумма, произведение, медиана, 
минимум и максимум, квантили и т. д.).
В библиотеке NumPy имеются быстрые функции агрегирования для работы с мас -
сивами. Продемонстрирую некоторые из них.
Суммирование значений из массива
В качестве примера рассмотрим вычисление суммы значений массива. В «чистом» 
языке Python это можно сделать с помощью встроенной функции sum:
In[1]: import numpy as np
In[2]: L = np.random.random(100)
 sum(L)
Out[2]: 55.61209116604941

\1
In[3]: np.sum(L)
Out[3]: 55.612091166049424

\1
In[4]: big_array = np.random.rand(1000000)
 %timeit sum(big_array)
 %timeit np.sum(big_array)
10 loops, best of 3: 104 ms per loop
1000 loops, best of 3: 442 µs per loop
Будьте осторожны: функции sum и np.sum не идентичны. Например, смысл их 
необязательных аргументов различен и функция np.sum умеет работать с много -
мерными массивами.
Минимум и максимум
В «чистом» языке Python имеются встроенные функции min и max, используемые 
для вычисления минимального и максимального значений любого заданного мас-
сива:
In[5]: min(big_array), max(big_array)
Out[5]: (1.1717128136634614e-06, 0.9999976784968716)

--- СТРАНИЦА 88 ---

\1n[6]: np.min(big_array), np.max(big_array)
Out[6]: (1.1717128136634614e-06, 0.9999976784968716)
In[7]: %timeit min(big_array)
 %timeit np.min(big_array)
10 loops, best of 3: 82.3 ms per loop
1000 loops, best of 3: 497 µs per loop
Для min, max, sum и еще нескольких функций вычисления сводных показателей 
библиотеки NumPy существует сокращенная запись операции путем применения 
методов самого объекта массива:
In[8]: print(big_array.min(), big_array.max(), big_array.sum())
1.17171281366e-060.999997678497499911.628197

\1
In[9]: M = np.random.random((3, 4))
 print(M)
[[ 0.8967576 0.03783739 0.75952519 0.06682827]
 [ 0.8354065 0.99196818 0.19544769 0.43447084]
 [ 0.66859307 0.15038721 0.37911423 0.6687194 ]]

\1
In[10]: M.sum()
Out[10]: 6.0850555667307118

\1
In[11]: M.min(axis=0)
Out[11]: array([ 0.66859307, 0.03783739, 0.19544769, 0.06682827])

--- СТРАНИЦА 89 ---

\1
In[12]: M.max(axis=1)
Out[12]: array([ 0.8967576 , 0.99196818, 0.6687194 ])
Способ задания оси в этих примерах может вызвать затруднения у пользователей, 
работавших ранее с другими языками программирования. Ключевое слово axis 
задает измерение массива, которое будет «схлопнуто» , а не возвращаемое изме -
рение. Так что указание axis=0 означает, что первая ось будет «схлопнута»: для 
двумерных массивов значения в каждом из столбцов будут агрегированы.
Другие функции агрегирования
Библиотека NumPy предоставляет много других агрегирующих функций. У боль -
шинства есть NaN-безопасный эквивалент, вычисляющий результат с игнориро -
ванием отсутствующих значений, помеченных специально определенным орга -
низацией IEEE значением с плавающей точкой NaN (см. в разделе «Обработка 
отсутствующих данных» главы 3 более подробное обсуждение этого вопроса). 
Некоторые из NaN-безопасных функций были добавлены только в версии 1.8 би-
блиотеки NumPy, поэтому они недоступны в ранних версиях.
В табл. 2.3 приведен список полезных агрегирующих функций, доступных в би -
блиотеке NumPy.
Таблица 2.3. Доступные в библиотеке NumPy функции агрегирования
Имя функции NaN-безопасная 
версия
Описание
np.sum np.nansum Вычисляет сумму элементов
np.prod np.nanprod Вычисляет произведение элементов
np.mean np.nanmean Вычисляет среднее значение элементов
np.std np.nanstd Вычисляет стандартное отклонение
np.var np.nanvar Вычисляет дисперсию
np.min np.nanmin Вычисляет минимальное значение
np.max np.nanmax Вычисляет максимальное значение
np.argmin np.nanargmin Возвращает индекс минимального значения
np.argmax np.nanargmax Возвращает индекс максимального значения
np.median np.nanmedian Вычисляет медиану элементов
np.percentile np.nanpercentile Вычисляет квантили элементов
np.any N/A Проверяет, существуют ли элементы со значением true
np.all N/A Проверяет, все ли элементы имеют значение true
Мы часто будем встречаться с этими агрегирующими функциями в дальнейшем.

--- СТРАНИЦА 90 ---

\1nt_heights.csv , 
представляющем собой простой разделенный запятыми список меток и значений:
In[13]: !head -4 data/president_heights.csv
order,name,height(cm)
1,George Washington,189
2,John Adams,170
3,Thomas Jefferson,189
Мы воспользуемся пакетом Pandas, который изучим более детально в главе 3, для 
чтения файла и извлечения данной информации (обратите внимание, что рост 
указан в сантиметрах):
In[14]: import pandas as pd
 data = pd.read_csv('data/president_heights.csv')
 heights = np.array(data['height(cm)'])
 print(heights)
[189 170 189 163 183 171 185 168 173 183 173 173 175 178 183 193 178 173
 174 183 183 168 170 178 182 180 183 178 182 188 175 179 183 193 182 183
 177 185 188 188 182 185]

\1
In[15]: print("Mean height: ", heights.mean())
 print("Standard deviation:", heights.std())
 print("Minimum height: ", heights.min())
 print("Maximum height: ", heights.max())
Mean height: 179.738095238
Standard deviation: 6.93184344275
Minimum height: 163
Maximum height: 193

\1
In[16]: print("25th percentile: ", np.percentile(heights, 25))
 print("Median: ", np.median(heights))
 print("75th percentile: ", np.percentile(heights, 75))
25th percentile: 174.25
Median: 182.0
75th percentile: 183.0

--- СТРАНИЦА 91 ---

\1
In[17]: %matplotlib inline
 import matplotlib.pyplot as plt
 import seaborn; seaborn.set() # задает стиль графика
In[18]: plt.hist(heights)
 plt.title('Height Distribution of US Presidents') # Распределение роста
 # президентов США
 plt.xlabel('height (cm)') # Рост, см
 plt.ylabel('number'); # Количество
Рис. 2.3. Гистограмма роста президентов
Эти сводные показатели — базовые элементы так называемого разведочного ана-
лиза данных (exploratory data analysis), который мы рассмотрим подробнее в сле-
дующих главах книги.
Операции над массивами. Транслирование
В предыдущем разделе мы рассмотрели, каким образом можно использовать уни-
версальные функции библиотеки NumPy для векторизации операций, а следова -
тельно, устранения медленных стандартных циклов языка Python. Еще один спо -
соб применения операций векторизации — использовать имеющиеся в библиотеке 

--- СТРАНИЦА 92 ---

\1ng). Транслирование представ -
ляет собой набор правил по применению бинарных универсальных функций 
(сложение, вычитание, умножение и т. д.) к массивам различного размера.

\1
In[1]: import numpy as np
In[2]: a = np.array([0, 1, 2])
 b = np.array([5, 5, 5])
 a + b 
Out[2]: array([5, 6, 7])

\1
In[3]: a + 5
Out[3]: array([5, 6, 7])

\1
In[4]: M = np.ones((3, 3))
 M 
Out[4]: array([[ 1., 1., 1.],
 [ 1., 1., 1.],
 [ 1., 1., 1.]])
In[5]: M + a 
Out[5]: array([[ 1., 2., 3.],
 [ 1., 2., 3.],
 [ 1., 2., 3.]])

\1
In[6]: a = np.arange(3)
 b = np.arange(3)[:, np.newaxis]
 print(a)
 print(b)
[0 1 2]
[[0]
 [1]
 [2]]
In[7]: a + b
Out[7]: array([[0, 1, 2],
 [1, 2, 3],
 [2, 3, 4]])
Аналогично тому, как мы раньше растягивали (транслировали) один массив, чтобы 
он соответствовал форме другого, здесь мы растягиваем оба массива a и b, чтобы 
привести их к общей форме. В результате мы получаем двумерный массив! Гео -
метрия этих примеров наглядно показана на рис. 2.4 1 .
0 1 2 5 6 75
np.arange(3)+5
11 1
11 1
11 1
1
2
3
1
1
2
2
3
3
1 20
np.ones((3,3))+np.arrange(3)
1
1 2
3 4
3
2
20
1
0
2
1 20
np.ones((3,1))+np.arrange(3)
Рис. 2.4. Визуализация транслирования массивов библиотекой NumPy

\1n[8]: M = np.ones((2, 3))
 a = np.arange(3)

\1
In[9]: M + a 
Out[9]: array([[ 1., 2., 3.],
 [ 1., 2., 3.]])

--- СТРАНИЦА 95 ---

\1
In[10]: a = np.arange(3).reshape((3, 1))
 b = np.arange(3)

\1
In[11]: a + b 
Out[11]: array([[0, 1, 2],
 [1, 2, 3],
 [2, 3, 4]])

\1
In[12]: M = np.ones((3, 2))
 a = np.arange(3)

\1

--- СТРАНИЦА 96 ---

\1n[13]: M + a 
---------------------------------------------------------------------------
ValueError Traceback (most recent call last)
<ipython-input-13-9e16e9f98da6> in <module>()
----> 1 M + a 
ValueError: operands could not be broadcast together with shapes (3,2) (3,)
Обратите внимание на имеющийся потенциальный источник ошибки: можно 
было бы сделать массивы a и M совместимыми, скажем путем дополнения формы a 
единицами справа, а не слева. Но правила транслирования работают не так! 
Если вам хочется применить правостороннее дополнени е, можете сделать это 
явным образом, поменяв форму массива (мы воспользуемся ключевым словом 
np.newaxis , описанным в разделе «Введение в массивы библиотеки NumPy» 
данной главы):
In[14]: a[:, np.newaxis].shape
Out[14]: (3, 1)
In[15]: M + a[:, np.newaxis]
Out[15]: array([[ 1., 1.],
 [ 2., 2.],
 [ 3., 3.]])

\1
In[16]: np.logaddexp(M, a[:, np.newaxis])
Out[16]: array([[ 1.31326169, 1.31326169],
 [ 1.69314718, 1.69314718],
 [ 2.31326169, 2.31326169]])
Для получения дальнейшей информации по множеству доступных универсальных 
функций см. раздел «Выполнение вычислений над массивами библиотеки NumPy: 
универсальные функции» данной главы.

--- СТРАНИЦА 97 ---
Операции над массивами. Транслирование 9 7
Транслирование на практике
Операции транслирования — основное ядро множества примеров, приводимых 
в дальнейшем в нашей книге. Рассмотрим несколько простых примеров сфер их 
возможного применения.
Центрирование массива
Благодаря универсальным функциям пользователи библиотеки NumPy избавля -
ются от необходимости писать явным образом медленно работающие циклы языка 
Python. Транслирование расширяет эти возможности. Один из часто встречающихся 
примеров — центрирование массива. Пускай у вас есть массив из десяти наблюдений, 
каждое состоит из трех значений. Используя стандартные соглашения (см. «Пред-
ставление данных в Scikit-Learn» главы 5), сохраним эти данные в массиве 10 × 3:
In[17]: X = np.random.random((10, 3))
Вычислить среднее значение каждого признака можно, примененив функцию 
агрегирования mean по первому измерению:
In[18]: Xmean = X.mean(0)
 Xmean
Out[18]: array([ 0.53514715, 0.66567217, 0.44385899])

\1
In[19]: X_centered = X – Xmean

\1
In[20]: X_centered.mean(0)
Out[20]: array([ 2.22044605e-17, -7.77156117e-17, -1.66533454e-17])

\1
In[21]: # Задаем для x и y 50 шагов от 0 до 5
 x = np.linspace(0, 5, 50)
 y = np.linspace(0, 5, 50)[:, np.newaxis]
 z = np.sin(x) ** 10 + np.cos(10 + y * x) * np.cos(x)

--- СТРАНИЦА 98 ---

\1n[22]: %matplotlib inline1
 import matplotlib.pyplot as plt
In[23]: plt.imshow(z, origin='lower', extent=[0, 5, 0, 5], cmap='viridis')
 plt.colorbar();2
Результат, показанный на рис. 2.5, представляет собой великолепную визуализа-
цию двумерной функции.
Рис. 2.5. Визуализация двумерного массива
Сравнения, маски и булева логика
В этом разделе показано использование булевых масок для просмотра и изменения 
значений в NumPy-массивах. Маскирование удобно для извлечения, модифика -
ции, подсчета или других манипуляций со значениями в массиве по какому-либо 
критерию. Например, вам может понадобиться подсчитать все значения, превы -
шающие определенное число, или, возможно, удалить в се аномальные значения, 
превышающие какую-либо пороговую величину. В библиотеке NumPy булевы 
маски зачастую самый эффективный способ решения подобных задач.
Пример: подсчет количества дождливых дней
Пускай у вас есть последовательности данных, отража ющие количество осадков 
в каждый день года для конкретного города. Например, с помощью библиотеки 

\1n, а только в блокноте.

\1ndas (рассматриваемой подробнее в главе 3) мы загрузили еж едневную стати -
стику по осадкам для Сиэтла за 2014 год:
In[1]: import numpy as np
 import pandas as pd
 # Используем Pandas для извлечения количества осадков в дюймах 
 # в виде NumPy-массива
 rainfall = pd.read_csv('data/Seattle2014.csv')['PRCP'].values
 inches = rainfall / 254 # 1/10mm -> inches
 inches.shape
Out[1]: (365,)

\1
In[2]: %matplotlib inline
 import matplotlib.pyplot as plt
 import seaborn; seaborn.set() # задаем стили
In[3]: plt.hist(inches, 40);
Эта гистограмма дает общее представление о том, что такое наши данные: несмотря 
на репутацию, измеренное количество осадков в Сиэтле в абсолютное большинство 
дней в 2014 году близко к нулю. Но она плохо отражает нужную информацию: на -
пример, сколько было дождливых дней? Каково было среднее количество осадков 
в эти дождливые дни? Сколько было дней с более чем половиной дюйма осадков?
Рис. 2.6. Гистограмма осадков в 2014 году в Сиэтле

--- СТРАНИЦА 100 ---

\1ng) для быстрого ответа на подобные вопросы.

\1
In[4]: x = np.array([1, 2, 3, 4, 5])
In[5]: x < 3 # меньше
Out[5]: array([ True, True, False, False, False], dtype=bool)
In[6]: x > 3 # больше
Out[6]: array([False, False, False, True, True], dtype=bool)
In[7]: x <= 3 # меньше или равно
Out[7]: array([ True, True, True, False, False], dtype=bool)
In[8]: x >= 3 # больше или равно
Out[8]: array([False, False, True, True, True], dtype=bool)
In[9]: x != 3 # не равно

--- СТРАНИЦА 101 ---
Сравнения, маски и булева логика 101
Out[9]: array([ True, True, False, True, True], dtype=bool)
In[10]: x == 3 # равно
Out[10]: array([False, False, True, False, False], dtype=bool)

\1
In[11]: (2 * x) == (x ** 2)
Out[11]: array([False, True, False, False, False], dtype=bool)
Как и в случае арифметических операторов, операторы сравнения реализованы 
в библиотеке NumPy как универсальные функции (табл. 2.4). Например, когда 
вы пишете x < 3, библиотека NumPy на самом деле использует внутри функцию 
np.less(x, 3).
Таблица 2.4. Краткий список операторов сравнения и эквивалентных им универсальных функций
Оператор Эквивалентная универсальная функция
== np.equal
!= np.not_equal
< np.less
<= np.less_equal
> np.greater
>= np.greater_equal

\1
In[12]: rng = np.random.RandomState(0)
 x = rng.randint(10, size=(3, 4))
 x 
Out[12]: array([[5, 0, 3, 3],
 [7, 9, 3, 5],
 [2, 4, 7, 6]])
In[13]: x < 6
Out[13]: array([[ True, True, True, True],
 [False, False, True, True],
 [ True, True, False, False]], dtype=bool)
Во всех случаях результат представляет собой булев массив, и в библиотеке NumPy 
имеется набор простых паттернов для работы с этими булевыми результатами.

--- СТРАНИЦА 102 ---

\1np.count_nonzero:
In[15]: # Сколько значений массива меньше 6?
 np.count_nonzero(x < 6)
Out[15]: 8
Можно увидеть, что в массиве есть восемь элементов, чье значение меньше 6. Дру -
гой способ получить эту информацию — воспользоваться функцией np.sum. В этом 
случае False интерпретируется как 0, а True — как 1:
In[16]: np.sum(x < 6)
Out[16]: 8
Преимущество функции np.sum заключается в том, что, подобно другим функциям 
агрегирования библиотеки NumPy, это суммирование можно выполнять также по 
столбцам или строкам:
In[17]: # Сколько значений меньше 6 содержится в каждой строке?
 np.sum(x < 6, axis=1)
Out[17]: array([4, 2, 2])
Этот оператор подсчитывает количество значений меньше 6 в каждой строке ма-
трицы.
Если вам необходимо быстро проверить, существует ли хоть одно истинное зна -
чение, или все ли значения истинны, можно воспользоваться (как вы наверняка 
догадались) функциями np.any() и np.all():
In[18]: # Имеются ли в массиве какие-либо значения, превышающие 8?
 np.any(x > 8)
Out[18]: True
In[19]: # Имеются ли в массиве какие-либо значения меньше 0?
 np.any(x < 0)
Out[19]: False

--- СТРАНИЦА 103 ---
Сравнения, маски и булева логика 103
In[20]: # Все ли значения меньше 10?
 np.all(x < 10)
Out[20]: True
In[21]: # Все ли значения равны 6?
 np.all(x == 6)
Out[21]: False
Функции np.any() и np.all() также можно использовать по конкретным осям. 

\1
In[22]: # Все ли значения в каждой строке меньше 8?
 np.all(x < 8, axis=1)
Out[22]: array([ True, False, True], dtype=bool)
В первой и третьей строках имеются значения меньше 8, а во второй — нет.
Наконец, небольшое предупреждение: как упоминалось в разделе «Агрегирование: 
минимум, максимум и все, что посередине» данной главы, в языке Python имеются 
встроенные функции sum(), any() и all(). Их синтаксис отличается от аналогичных 
функций библиотеки NumPy. В частности, они будут вы давать ошибку или неожи-
данные результаты при использовании для работы с многомерными массивами. 
Убедитесь, что вы применяете для данных примеров функции np.sum(), np.any() 
и np.all().
Булевы операторы
Вы уже знаете, как можно подсчитать все дни с осадками менее четырех дюймов 
или все дни с осадками более двух дюймов. Но что, если нужна информация обо 
всех днях с толщиной слоя осадков менее четырех дюймов и более одного дюйма? 
Это можно сделать с помощью побитовых логических операторов (bitwise logic 
operators) языка Python: &, |, ^ и ~. Аналогично обычным арифметическим операто-
рам библиотека NumPy перегружает их как универсальные функции, поэлементно 
работающие с (обычно булевыми) массивами.

\1
In[23]: np.sum((inches > 0.5) & (inches < 1))
Out[23]: 29
Видим, что в 2014 году в Сиэтле было 29 дней с толщиной слоя осадков от 0.5 до 
1 дюйма.

--- СТРАНИЦА 104 ---

\1nches > (0.5 & inches) < 1

\1
In[24]: np.sum(~( (inches <= 0.5) | (inches >= 1) ))
Out[24]: 29
Объединив операторы сравнения и булевы операторы при работе с массивами, 
можно получить целый диапазон эффективных логических операций (табл. 2.5).
Таблица 2.5. Побитовые булевы операторы и эквивалентные им универсальные функции
Оператор Эквивалентная универсальная функция
& np.bitwise_and
| np.bitwise_or
^ np.bitwise_xor
~ np.bitwise_not

\1
In[25]: print("Number days without rain: ", np.sum(inches == 0))
 print("Number days with rain: ", np.sum(inches != 0))
 print("Days with more than 0.5 inches:", np.sum(inches > 0.5))
 print("Rainy days with < 0.1 inches :", np.sum((inches > 0) &
 (inches < 0.2)))
Number days without rain: 215
Number days with rain: 150
Days with more than 0.5 inches: 37
Rainy days with < 0.1 inches : 75

\1

--- СТРАНИЦА 105 ---
Сравнения, маски и булева логика 105
In[26]: x 
Out[26]: array([[5, 0, 3, 3],
 [7, 9, 3, 5],
 [2, 4, 7, 6]])

\1
In[27]: x < 5
Out[27]: array([[False, True, True, True],
 [False, False, True, False],
 [ True, True, False, False]], dtype=bool)

\1
In[28]: x[x < 5]
Out[28]: array([0, 3, 3, 3, 2, 4])

\1
In[29]:
# создаем маску для всех дождливых дней
rainy = (inches > 0)
# создаем маску для всех летних дней (21 июня1 – 172-й день)
summer = (np.arange(365) - 172 < 90) & (np.arange(365) - 172 > 0)
print("Median precip on rainy days in 2014 (inches): ",
 np.median(inches[rainy]))
print("Median precip on summer days in 2014 (inches): ",
 np.median(inches[summer]))
print("Maximum precip on summer days in 2014 (inches): ",
 np.max(inches[summer]))
print("Median precip on non-summer rainy days (inches):",
 np.median(inches[rainy & ~summer]))
Median precip on rainy days in 2014 (inches): 0.194881889764
Median precip on summer days in 2014 (inches): 0.0
Maximum precip on summer days in 2014 (inches): 0.850393700787
Median precip on non-summer rainy days (inches): 0.200787401575

\1nd/or по сравнению с использованием 
операторов &\|
Разница между ключевыми словами AND и OR и операторами & и | — распространен-
ный источник путаницы. Какие из них и когда следует использовать?
Различие заключается в следующем: ключевые слова AND и OR определяют истин-
ность или ложность всего объекта, операторы & и | оперируют отдельными битами
 
внутри каждого из объектов.
Использование ключевых слов and и or приводит к тому, что язык Python будет рас-
сматривать объект как одну булеву сущность. В Python все ненулевые целые числа 
будут рассматриваться как True. Таким образом:
In[30]: bool(42), bool(0)
Out[30]: (True, False)
In[31]: bool(42 and 0)
Out[31]: False
In[32]: bool(42 or 0)
Out[32]: True
При использовании операторов & и | для работы с целыми числами выражение 
оперирует разрядами элемента, фактически применяя операции and и or к состав-
ляющим число отдельным битам:
In[33]: bin(42)
Out[33]: '0b101010'
In[34]: bin(59)
Out[34]: '0b111011'
In[35]: bin(42 & 59)
Out[35]: '0b101010'
In[36]: bin(42 | 59)
Out[36]: '0b111011'

\1
In[37]: A = np.array([1, 0, 1, 0, 1, 0], dtype=bool)
 B = np.array([1, 1, 1, 0, 1, 1], dtype=bool)
 A | B
Out[37]: array([ True, True, True, False, True, True], 
 dtype=bool)

\1
In[38]: A or B
------------------------------------------------------------------------
ValueError Traceback (most recent call last)
<ipython-input-38-5d8e4f2e21c0> in <module>()
----> 1 A or B 
ValueError: The truth value of an array with more than one element is...
При создании булева выражения с заданным массивом следует использовать опера-
торы & и |, а не операции and или or:
In[39]: x = np.arange(10)
 (x > 4) & (x < 8)
Out[39]: array([False, False, ..., True, True, False, False], 
 dtype=bool)

\1
In[40]: (x > 4) and (x < 8)
------------------------------------------------------------------------
ValueError Traceback (most recent call last)
<ipython-input-40-3d24f1ffd63d> in <module>()
----> 1 (x > 4) and (x < 8)
ValueError: The truth value of an array with more than one element is...
Итак, запомните: операции and и or вычисляют единое булево значение для всего 
объекта, в то время как операторы & и | вычисляют много булевых значений для со-
держимого (отдельных битов или байтов) объекта. Второй из этих вариантов прак-
тически всегда будет именно той операцией, которая будет вам нужна при работе 
с булевыми массивами библиотеки NumPy.

--- СТРАНИЦА 108 ---

\1ncy indexing). 
«Прихотливая» индексация похожа на уже рассмотренную нами простую индекса-
цию, но вместо скалярных значений передаются массивы индексов. Это дает воз -
можность очень быстрого доступа и модификации сложных подмножеств значений 
массива.

\1
In[1]: import numpy as np
 rand = np.random.RandomState(42)
 x = rand.randint(100, size=10)
 print(x)
[51 92 14 71 60 20 82 86 74 74]

\1
In[2]: [x[3], x[7], x[2]]
Out[2]: [71, 86, 14]

\1
In[3]: ind = [3, 7, 4]
 x[ind]
Out[3]: array([71, 86, 60])
В случае «прихотливой» индексации форма результата отражает форму массивов 
индексов (index arrays), а не форму индексируемого массива:
In[4]: ind = np.array([[3, 7],
 [4, 5]])
 x[ind]

--- СТРАНИЦА 109 ---
«Прихотливая» индексация 109
Out[4]: array([[71, 86],
 [60, 20]])
«Прихотливая» индексация работает и в случае многомерных массивов. Рассмо-
трим следующий массив:
In[5]: X = np.arange(12).reshape((3, 4))
 X 
Out[5]: array([[ 0, 1, 2, 3],
 [ 4, 5, 6, 7],
 [ 8, 9, 10, 11]])

\1
In[6]: row = np.array([0, 1, 2])
 col = np.array([2, 1, 3])
 X[row, col]
Out[6]: array([ 2, 5, 11])

\1
In[7]: X[row[:, np.newaxis], col]
Out[7]: array([[ 2, 1, 3],
 [ 6, 5, 7],
 [10, 9, 11]])

\1
In[8]: row[:, np.newaxis] * col
Out[8]: array([[0, 0, 0],
 [2, 1, 3],
 [4, 2, 6]])

\1

--- СТРАНИЦА 110 ---

\1n[9]: print(X)
[[ 0 1 2 3]
 [ 4 5 6 7]
 [ 8 9 10 11]]

\1
In[10]: X[2, [2, 0, 1]]
Out[10]: array([10, 8, 9])

\1
In[11]: X[1:, [2, 0, 1]]
Out[11]: array([[ 6, 4, 5],
 [10, 8, 9]])

\1
In[12]: mask = np.array([1, 0, 1, 0], dtype=bool)
 X[row[:, np.newaxis], mask]
Out[12]: array([[ 0, 2],
 [ 4, 6],
 [ 8, 10]])

\1
In[13]: mean = [0, 0]
 cov = [[1, 2],
 [2, 5]]
 X = rand.multivariate_normal(mean, cov, 100)
 X.shape
Out[13]: (100, 2)

\1

--- СТРАНИЦА 111 ---
«Прихотливая» индексация 111
In[14]: %matplotlib inline
 import matplotlib.pyplot as plt
 import seaborn; seaborn.set() # for plot styling
 plt.scatter(X[:, 0], X[:, 1]);

\1
In[15]: indices = np.random.choice(X.shape[0], 20, replace=False)
 indices
Out[15]: array([93, 45, 73, 81, 50, 10, 98, 94, 4, 64, 65, 89, 47, 84, 82,
 80, 25, 90, 63, 20])
In[16]: selection = X[indices] # Тут используется «прихотливая» индексация
 selection.shape
Out[16]: (20, 2)
Чтобы посмотреть, какие точки были выбраны, нарисуем поверх первой диаграм -
мы большие круги в местах расположения выбранных точек (рис. 2.8).
In[17]: plt.scatter(X[:, 0], X[:, 1], alpha=0.3)
 plt.scatter(selection[:, 0], selection[:, 1],
 facecolor='none', s=200);

--- СТРАНИЦА 112 ---

\1n[18]: x = np.arange(10)
 i = np.array([2, 1, 8, 4])
 x[i] = 99
 print(x)
[ 0 99 99 3 99 5 6 7 99 9]

\1
In[19]: x[i] -= 10
 print(x)
[ 0 89 89 3 89 5 6 7 89 9]

--- СТРАНИЦА 113 ---
«Прихотливая» индексация 113

\1
In[20]: x = np.zeros(10)
 x[[0, 0]] = [4, 6]
 print(x)
[ 6. 0. 0. 0. 0. 0. 0. 0. 0. 0.]

\1
In[21]: i = [2, 3, 3, 4, 4, 4]
 x[i] += 1
 x 
Out[21]: array([ 6., 0., 1., 1., 1., 0., 0., 0., 0., 0.])

\1
In[22]: x = np.zeros(10)
 np.add.at(x, i, 1)
 print(x)
[ 0. 0. 1. 2. 3. 0. 0. 0. 0. 0.]
Метод at() применяет соответствующий оператор к элементам с заданными индек-
сами (в данном случае i) с использованием заданного значения (в данном случае 1). 
Аналогичный по духу метод универсальных функций reduceat(), о котором можно 
прочитать в документации библиотеки NumPy.
Пример: разбиение данных на интервалы
Можно использовать эти идеи для эффективного разбиения данных с целью по -
строения гистограммы вручную. Например, пусть у нас есть 1000 значений и нам 

--- СТРАНИЦА 114 ---

\1n[23]: np.random.seed(42)
 x = np.random.randn(100)
 # Рассчитываем гистограмму вручную
 bins = np.linspace(-5, 5, 20)
 counts = np.zeros_like(bins)
 # Ищем подходящий интервал для каждого x
 i = np.searchsorted(bins, x)
 # Добавляем 1 к каждому из интервалов
 np.add.at(counts, i, 1)

\1
In[24]: # Визуализируем результаты
 plt.plot(bins, counts, linestyle='steps');

\1
plt.hist(x, bins, histtype='step');

--- СТРАНИЦА 115 ---
«Прихотливая» индексация 115
Эта функция создаст практически точно такую же диаграмму, как на рис. 2.9. Для 
расчета разбиения по интервалам библиотека Matplotlib использует функцию 
np.histogram , выполняющую вычисления, очень похожие на сделанные нами. 

\1
In[25]: print("NumPy routine:")
 %timeit counts, edges = np.histogram(x, bins)
 print("Custom routine:")
 %timeit np.add.at(counts, np.searchsorted(bins, x), 1)
NumPy routine:
10000 loops, best of 3: 97.6 µs per loop
Custom routine:
10000 loops, best of 3: 19.5 µs per loop
Наш собственный однострочный алгоритм работает в несколько раз быстрее, чем 
оптимизированный алгоритм из библиотеки NumPy! Как это возможно? Если мы 
заглянем в исходный код процедуры np.histogram (в оболочке IPython это можно 
сделать, введя команду np.histogram??), то увидим, что она гораздо сложнее про -
стого поиска-и-подсчета, выполненного нами. Дело в том, что алгоритм из библи -
отеки NumPy более гибок, потому что разработан с ориентацией на более высокую 
производительность при значительном увеличении количества точек данных:
In[26]: x = np.random.randn(1000000)
 print("NumPy routine:")
 %timeit counts, edges = np.histogram(x, bins)
 print("Custom routine:")
 %timeit np.add.at(counts, np.searchsorted(bins, x), 1)
NumPy routine:
10 loops, best of 3: 68.7 ms per loop
Custom routine:
10 loops, best of 3: 135 ms per loop
Это сравнение демонстрирует нам, что эффективность алгоритма почти всегда не-
простой вопрос. Эффективный для больших наборов данных алгоритм не всегда 
окажется оптимальным вариантом для маленьких, и наоборот (см. врезку «Нота -
ция “О-большого”» далее). Но преимущество самостоятельного программирования 
этого алгоритма заключается в том, что, получив понимание работы подобных 
простых методов, вы сможете «строить» из этих «кирпичиков» очень интересные 
варианты пользовательского поведения. Ключ к эффективному использованию 
языка Python в приложениях, требующих обработки больших о бъемов данных, 
заключается в том, чтобы знать о существовании удобных процедур, таких как 
np.histogram, и сферах их использования. Кроме того, нужно знать, как применять 
низкоуровневую функциональность при необходимости в узконаправленном по-
ведении.

--- СТРАНИЦА 116 ---

\1nsertion sort) многократно находит 
минимальное значение из списка и выполняет перестановки до тех пор, пока 
список не будет отсортирован. Это можно запрограммировать с помощью всего 
нескольких строк кода на языке Python:
In[1]: import numpy as np
 def selection_sort(x):
 for i in range(len(x)):
 swap = i + np.argmin(x[i:])
 (x[i], x[swap]) = (x[swap], x[i])
 return x 
In[2]: x = np.array([2, 1, 4, 3, 5])
 selection_sort(x)
Out[2]: array([1, 2, 3, 4, 5])

\1
In[3]: def bogosort(x):
 while np.any(x[:-1] > x[1:]):
 np.random.shuffle(x)
 return x 

--- СТРАНИЦА 117 ---
Сортировка массивов 117
In[4]: x = np.array([2, 1, 4, 3, 5])
 bogosort(x)
Out[4]: array([1, 2, 3, 4, 5])
Этот алгоритм сортировки опирается в своей работе на чистое везение: он много-
кратно перетасовывает массив случайным образом до т ех пор, пока результат не 
окажется отсортированным. При средней сложности порядка O [ N × N !]: (это N ум-
ножить на N факториал) его не стоит использовать ни для каких реальных расчетов.
В Python имеются намного более эффективные встроенные алгоритмы сортировки. 
Начнем с изучения встроенных алгоритмов языка Python, после чего рассмотрим ути-
литы, включенные в библиотеку NumPy и оптимизированные под NumPy-массивы.
Быстрая сортировка в библиотеке NumPy: функции 
np.sort и np.argsort
Хотя в языке Python имеются встроенные функции sort и sorted для работы со 
списками, мы не будем их рассматривать, поскольку функция библиотеки NumPy 
np.sort оказывается намного более эффективной и подходящей для наших целей. 
По умолчанию функция np.sort использует имеющий сложность O [ N log N ]: 
алгоритм быстрой сортировки (quicksort), хотя доступны для использования 
также алгоритмы сортировки слиянием (mergesort) и пирамидальной сортировки 
(heapsort). Для большинства приложений используемой по умолчанию быстрой 
сортировки более чем достаточно.
Чтобы получить отсортированную версию входного массива без его изменения, 
можно использовать функцию np.sort:
In[5]: x = np.array([2, 1, 4, 3, 5])
 np.sort(x)
Out[5]: array([1, 2, 3, 4, 5])

\1
In[6]: x.sort()
 print(x)
[1 2 3 4 5]

\1
In[7]: x = np.array([2, 1, 4, 3, 5])
 i = np.argsort(x)
 print(i)
[1 0 3 2 4]

--- СТРАНИЦА 118 ---

\1n[8]: x[i]
Out[8]: array([1, 2, 3, 4, 5])

\1
In[9]: rand = np.random.RandomState(42)
 X = rand.randint(0, 10, (4, 6))
 print(X)
[[6 3 7 4 6 9]
 [2 6 7 4 3 7]
 [7 2 5 4 1 7]
 [5 1 4 0 9 5]]
In[10]: # Сортируем все столбцы массива X
 np.sort(X, axis=0)
Out[10]: array([[2, 1, 4, 0, 1, 5],
 [5, 2, 5, 4, 3, 7],
 [6, 3, 7, 4, 6, 7],
 [7, 6, 7, 4, 9, 9]])
In[11]: # Сортируем все строки массива X
 np.sort(X, axis=1)
Out[11]: array([[3, 4, 6, 6, 7, 9],
 [2, 3, 4, 6, 7, 7],
 [1, 2, 4, 5, 7, 7],
 [0, 1, 4, 5, 5, 9]])
Не забывайте, что при этом все строки или столбцы рассматриваются как отдель-
ные массивы, так что любые возможные взаимосвязи между значениями строк или 
столбцов будут утеряны.
Частичные сортировки: секционирование
Иногда нам не требуется сортировать весь массив, а просто нужно найти K наи -
меньших значений в нем. Библиотека NumPy предоставляет для этой цели функ-
цию np.partition. Функция np.partition принимает на входе массив и число K. 

\1

--- СТРАНИЦА 119 ---
Сортировка массивов 119
In[12]: x = np.array([7, 2, 3, 1, 6, 5, 4])
 np.partition(x, 3)
Out[12]: array([2, 1, 3, 4, 6, 5, 7])

\1
In[13]: np.partition(X, 2, axis=1)
Out[13]: array([[3, 4, 6, 7, 6, 9],
 [2, 3, 4, 7, 6, 7],
 [1, 2, 4, 5, 7, 7],
 [0, 1, 4, 5, 9, 5]])
Результат представляет собой массив, в котором на первых двух позициях в каж-
дой строке находятся наименьшие значения из этой строки, а остальные значения 
заполняют прочие места.
Наконец, аналогично функции np.argsort, вычисляющей индексы для сортировки, 
существует функция np.argpartition, вычисляющая индексы для секции. Мы уви-
дим ее в действии в следующем разделе.
Пример: K ближайших соседей
Давайте вкратце рассмотрим, как можно использовать функцию np.argpartition 
по нескольким осям для поиска ближайших соседей каждой точки из определен-
ного набора. Начнем с создания случайного набора из десяти точек на двумерной 
плоскости. По стандартным соглашениям образуем из них массив 10 × 2:
In[14]: X = rand.rand(10, 2)

\1
In[15]: %matplotlib inline
 import matplotlib.pyplot as plt
 import seaborn; seaborn.set() # Plot styling
 plt.scatter(X[:, 0], X[:, 1], s=100);
Теперь можно вычислить расстояние между всеми парами точек. Вспоминаем, 
что квадрат расстояния между двумя точками равен су мме квадратов расстоя -
ний между ними по каждой из координат. Воспользовавшись возможностями 
эффективного транслирования (см. «Операции над массивами. Транслирование» 
этой главы) и агрегирования (см. «Агрегирование: минимум, максимум и все, что 

--- СТРАНИЦА 120 ---

\1n[16]: dist_sq = np.sum((X[:,np.newaxis,:] - X[np.newaxis,:,:])
 ** 2, axis=-1)

\1
In[17]: # Для каждой пары точек вычисляем разности их координат
 differences = X[:, np.newaxis, :] - X[np.newaxis, :, :]
 differences.shape
Out[17]: (10, 10, 2)
In[18]: # Возводим разности координат в квадрат
 sq_differences = differences ** 2
 sq_differences.shape
Out[18]: (10, 10, 2)
In[19]: # Суммируем квадраты разностей координат 
 # для получения квадрата расстояния
 dist_sq = sq_differences.sum(-1)
 dist_sq.shape
Out[19]: (10, 10)

\1

--- СТРАНИЦА 121 ---
Сортировка массивов 121
In[20]: dist_sq.diagonal()
Out[20]: array([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
Проверка пройдена! Теперь, получив матрицу квадратов расстояний между взя -
тыми попарно точками, мы можем воспользоваться функцией np.argsort для 
сортировки по каждой строке. Крайние слева столбцы будут представлять собой 
индексы ближайших соседей:
In[21]: nearest = np.argsort(dist_sq, axis=1)
 print(nearest)
[[0 3 9 7 1 4 2 5 6 8]
 [1 4 7 9 3 6 8 5 0 2]
 [2 1 4 6 3 0 8 9 7 5]
 [3 9 7 0 1 4 5 8 6 2]
 [4 1 8 5 6 7 9 3 0 2]
 [5 8 6 4 1 7 9 3 2 0]
 [6 8 5 4 1 7 9 3 2 0]
 [7 9 3 1 4 0 5 8 6 2]
 [8 5 6 4 1 7 9 3 2 0]
 [9 7 3 0 1 4 5 8 6 2]]
Обратите внимание, что первый столбец представляет собой числа с 0 до 9 в по-
рядке возрастания: это происходит из-за того, что ближайший сосед каждой точ-
ки — она сама, как и можно было ожидать.
Выполнив полную сортировку, мы проделали лишнюю работу. Если нас интере-
совали K ближайших соседей, было достаточно секционировать все строки так, 
чтобы сначала шли K+1 минимальных квадратов расстояний, а большие расстояния 
заполняли оставшиеся позиции массива. Сделать это можно с помощью функции 
np.argpartition:
In[22]: K = 2
 nearest_partition = np.argpartition(dist_sq, K + 1, axis=1)

\1
In[23]: plt.scatter(X[:, 0], X[:, 1], s=100)
 # Рисуем линии из каждой точки к ее двум ближайшим соседям
 K = 2
 for i in range(X.shape[0]):
 for j in nearest_partition[i, :K+1]:

--- СТРАНИЦА 122 ---

\1n весьма эффективен. 
Как бы ни было заманчиво сделать то же самое, вручную организовав цикл по 
данным и сортировку каждого набора соседей отдельно, получившийся в итоге 
алгоритм почти наверняка будет работать медленнее, чем рассмотренная выше 
векторизованная версия. Красота такого подхода — в его независимости от раз -
мера входных данных: можно с одинаковой легкостью вычислить соседей среди 
100 или 1 000 000 точек в любом количестве измерений, и код будет выглядеть 
точно так же.
Рис. 2.11. Визуализация соседей каждой точки
Наконец, отмечу, что для выполнения поисков соседей в очень больших массивах 
данных существуют основанные на деревьях и/или аппроксимационные алго -
ритмы, масштабирующиеся как O [ N log N ]: или даже лучше, в отличие от грубого 
подхода O [ N 2 ]:. Один из примеров таких алгоритмов — K-мерное дерево (KD-tree), 
реализованное в библиотеке Scikit-Learn.

--- СТРАНИЦА 123 ---
Структурированные данные: структурированные массивы библиотеки NumPy 123
Нотация «О-большого»
Нотация «О-большого» — средство, позволяющее описывать рост числа опера -
ций, необходимых для выполнения алгоритма, по мере роста объема входных дан-
ных. Для правильного использования нужно немного углубиться в теорию вы -
числительной техники и уметь отличать данную нотацию от родственных нотаций 
«о-маленького», « θ-большого», « Ω-большого» и, вероятно, множества их гибридов. 
Хотя эти варианты позволяют с большей точностью выражать информацию о мас-
штабируемости алгоритмов, помимо экзаменов по теории вычислительной техники 
и замечаний педантичных комментаторов в блогах, их редко где можно увидеть. 
Намного более распространенной в мире науки о данных является более гибкая но-
тация «О-большого»: общее (хотя и не такое точное) описание масштабируемости 
алгоритма. Простите нас, теоретики и педанты, но именно эту интерпретацию мы 
и будем использовать в данной книге.
Нотация «О-большого» показывает, во сколько раз больше времени будет занимать 
выполнение алгоритма при росте количества данных. Если ваш алгоритм 
O [ N ] (чи-
тается «порядка N ») выполняется 1 секунду при работе со списком длины N = 1000, 
то следует ожидать, что его выполнение займет примерно 5 секунд для списка дли-
ной 
N = 5000. Если же у вас алгоритм O [ N 2 ] (читается «порядка N квадрат») выпол-
няется 1 секунду при работе со списком длины N = 1000, то следует ожидать, что его 
выполнение займет примерно 25 секунд для списка длиной N = 5000.
Для наших целей N будет обычно обозначать какой-либо аспект размера набора 
данных (количество точек, количество измерений и т. п.). При попытке анализа мил -
лиардов или триллионов выборок разница между сложностью 
O [ N ] и O [ N 2 ] может 
быть более чем существенной!
Обратите внимание, что нотация «О-большого» сама по себе ничего не говорит 
о фактическом времени выполнения вычислений, а только о его масштабировании 
при изменении 
N . Обычно, например, алгоритм со сложностью O [ N ] считается луч -
ше масштабируемым, чем алгоритм с O [ N 2 ], и на то есть веские причины. Но, в част -
ности, для маленьких наборов данных лучше масштабируемый алгоритм не обяза -
тельно будет более быстрым. Например, при работе с конкретной задачей алгоритм 
с 
O [ N 2 ] может выполняться 0,01 секунды, а «лучший» алгоритм O [ N ] — 1 секунду. 
Увеличьте, однако, N на три порядка, и алгоритм O [ N ] окажется победителем.
Даже такая упрощенная версия нотации «О-большого» может оказаться очень 
удобной для сравнения производительности алгоритмов, и мы будем использовать 
эту нотацию в нашей книге, где будет идти речь о масштабируемости алгоритмов.
Структурированные данные: структурированные 
массивы библиотеки NumPy
Часто данные можно представить с помощью однородног о массива значений, но 
иногда это не удается. В этом разделе демонстрируется использование таких воз-
можностей библиотеки NumPy, как структурированные массивы (structured arrays) 

--- СТРАНИЦА 124 ---

\1ndas, которые мы рассмотрим в главе 3.
Пускай у нас имеется несколько категорий данных (например, имя, возраст и вес) 
о нескольких людях и мы хотели бы хранить эти значения для использования 
в программе на языке Python. Можно сохранить их в отдельных массивах:
In[2]: name = ['Alice', 'Bob', 'Cathy', 'Doug']
 age = [25, 45, 37, 19]
 weight = [55.0, 85.5, 68.0, 61.5]

\1
In[3]: x = np.zeros(4, dtype=int)

\1
In[4]: # Используем для структурированного массива составной тип данных 
 data = np.zeros(4, dtype={'names':('name', 'age', 'weight'),
 'formats':('U10', 'i4', 'f8')})
 print(data.dtype)
[('name', '<U10'), ('age', '<i4'), ('weight', '<f8')]
'U10' означает «строку в кодировке Unicode максимальной длины 10», 'i4' — 
«4-байтное (то есть 32-битное) целое число», а 'f8' — «8-байтное (то есть 64-бит-
ное) число с плавающей точкой». Мы обсудим другие варианты подобного коди-
рования типов в следующем разделе.

\1
In[5]: data['name'] = name
 data['age'] = age
 data['weight'] = weight
 print(data)
[('Alice', 25, 55.0) ('Bob', 45, 85.5) ('Cathy', 37, 68.0)
 ('Doug', 19, 61.5)]

--- СТРАНИЦА 125 ---

\1
In[6]: # Извлечь все имена
 data['name']
Out[6]: array(['Alice', 'Bob', 'Cathy', 'Doug'],
 dtype='<U10')
In[7]: # Извлечь первую строку данных
 data[0]
Out[7]: ('Alice', 25, 55.0)
In[8]: # Извлечь имя из последней строки
 data[-1]['name']
Out[8]: 'Doug'

\1
In[9]: # Извлечь имена людей с возрастом менее 30
 data[data['age'] < 30]['name']
Out[9]: array(['Alice', 'Doug'],
 dtype='<U10')
Для выполнения более сложных операций лучше использовать пакет Pandas, 
который будет рассмотрен в главе 3. Библиотека Pandas предоставляет объект 
DataFrame — основанную на массивах библиотеки NumPy структуру, обладающую 
массой полезной функциональности по работе с данными.

\1
In[10]: np.dtype({'names':('name', 'age', 'weight'),
 'formats':('U10', 'i4', 'f8')})
Out[10]: dtype([('name', '<U10'), ('age', '<i4'), ('weight', '<f8')])
Для ясности можно задавать числовые типы как с прим енением типов данных 
языка Python, так и типов dtype библиотеки NumPy:

--- СТРАНИЦА 126 ---

\1n[11]: np.dtype({'names':('name', 'age', 'weight'),
 'formats':((np.str_, 10), int, np.float32)})
Out[11]: dtype([('name', '<U10'), ('age', '<i8'), ('weight', '<f4')])

\1
In[12]: np.dtype([('name', 'S10'), ('age', 'i4'), ('weight', 'f8')])
Out[12]: dtype([('name', 'S10'), ('age', '<i4'), ('weight', '<f8')])

\1
In[13]: np.dtype('S10,i4,f8')
Out[13]: dtype([('f0', 'S10'), ('f1', '<i4'), ('f2', '<f8')])
Сокращенные строковые коды форматов могут показаться запутанными, но они 
основаны на простых принципах. Первый (необязательный) символ — < или >, 
означает «число с прямым порядком байтов» или «число с обратным порядком бай-
тов» соответственно и задает порядок значащих битов. Следующий символ задает 
тип данных: символы, байтовый тип, целые числа, числа с плавающей точкой и т. д. 
(табл. 2.6). Последний символ или символы отражают размер объекта в байтах.
Таблица 2.6. Типы данных библиотеки NumPy
Символ Описание Пример
'b' Байтовый тип np.dtype('b')
'i' Знаковое целое число np.dtype('i4') == np.int32
'u' Беззнаковое целое число np.dtype('u1') == np.uint8
'f ' Число с плавающей точкой np.dtype('f8') == np.int64
'c' Комплексное число с плавающей точкой np.dtype('c16') == np.complex128
'S', 'a' Строка np.dtype('S5')
'U' Строка в кодировке Unicode np.dtype('U') == np.str_
'V' Неформатированные данные (тип void) np.dtype('V') == np.void

\1
In[14]: tp = np.dtype([('id', 'i8'), ('mat', 'f8', (3, 3))])
 X = np.zeros(1, dtype=tp)

--- СТРАНИЦА 127 ---
Структурированные данные: структурированные массивы библиотеки NumPy 127
 print(X[0])
 print(X['mat'][0])
(0, [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])
[[ 0. 0. 0.]
 [ 0. 0. 0.]
 [ 0. 0. 0.]]
Теперь каждый элемент массива X состоит из целого числа id и матрицы 3 × 3. По -
чему такой массив может оказаться предпочтительнее, чем простой многомерный 
массив или, возможно, словарь языка Python? Дело в том, что dtype библиотеки 
NumPy напрямую соответствует описанию структуры из языка C, так что можно 
обращаться к содержащему этот массив буферу памяти непосредственно из соот-
ветствующим образом написанной программы на языке C. Если вам понадобится 
написать на языке Python интерфейс к уже существующей библиотеке на язы -
ке C или Fortran, которая работает со структурирова нными данными, вероятно, 
структурированные массивы будут вам весьма полезны!
Массивы записей: структурированные массивы 
с дополнительными возможностями
Библиотека NumPy предоставляет класс np.recarray, практически идентичный 
только что описанным структурированным массивам, но с одной дополнительной 
возможностью: доступ к полям можно осуществлять как к атрибутам, а не только 
как к ключам словаря. Как вы помните, ранее мы обращались к значениям возраста 
путем написания следующей строки кода:
In[15]: data['age']
Out[15]: array([25, 45, 37, 19], dtype=int32)

\1
In[16]: data_rec = data.view(np.recarray)
 data_rec.age
Out[16]: array([25, 45, 37, 19], dtype=int32)

\1
In[17]: %timeit data['age']
 %timeit data_rec['age']
 %timeit data_rec.age

--- СТРАНИЦА 128 ---

\1ns per loop
100000 loops, best of 3: 4.61 µs per loop
100000 loops, best of 3: 7.27 µs per loop
Имеет ли смысл жертвовать дополнительным временем ради более удобного син-
таксиса — зависит от вашего приложения.
Вперед, к Pandas
Этот раздел по структурированным массивам и массивам записей не случайно 
стоит в конце этой главы, поскольку он удачно подводит нас к теме следующего 
рассматриваемого пакета — библиотеки Pandas. В определенных случаях не поме -
шает знать о существовании обсуждавшихся здесь структурированных массивов, 
особенно если вам нужно, чтобы массивы библиотеки NumPy соответствовали 
двоичным форматам данных в C, Fortran или другом языке программирования. 
Для регулярной работы со структурированными данными намного удобнее ис -
пользовать пакет Pandas, который мы подробно рассмотрим в следующей главе.

--- СТРАНИЦА 129 ---

\1ndas
В предыдущей главе мы рассмотрели библиотеку NumPy и ее объект ndarray, обе -
спечивающий эффективное хранение плотных массивов и манипуляции над ними 
в Python. В этой главе мы, основываясь на этих знаниях, детально ознакомимся со 
структурами данных библиотеки Pandas.
Pandas — более новый пакет, надстройка над библиотекой NumPy, обеспечивающий 
эффективную реализацию класса DataFrame. Объекты DataFrame — многомерные 
массивы с метками для строк и столбцов, а также зачастую с неоднородным типом 
данных и/или пропущенными данными. Помимо удобного интерфейса для хра -
нения маркированных данных, библиотека Pandas реализует множество операций 
для работы с данными хорошо знакомых пользователям фреймворков баз данных 
и электронных таблиц.
Структура данных ndarray библиотеки NumPy предоставляет все необходимые 
возможности для работы с хорошо упорядоченными данными в задачах численных 
вычислений. Для этой цели библиотека NumPy отлично подходит, однако имеет 
свои ограничения, которые становятся заметными, чуть только нам потребуется не-
много больше гибкости (маркирование данных, работа с пропущенными данными 
и т. д.). Эти ограничения проявляются также при попытках выполнения опера -
ций, неподходящих для поэлементного транслирования (группировки, создание 
сводных таблиц и т. д.). Такие операции являются важной частью анализа данных 
с меньшей степенью структурированности, содержащихся во многих формах 
окружающего мира. Библиотека Pandas, особенно ее объекты Series и DataFrame, 
основана на структурах массивов библиотеки NumPy и обеспечивает эффективную 
работу над подобными задачами «очистки данных».

--- СТРАНИЦА 130 ---

\1ndas 
В этой главе мы сосредоточимся на стандартных приемах использования объектов 
Series, DataFrame и связанных с ними структур. По мере возможности мы будем 
применять взятые из реальных наборов данных примеры, но они не являются 
нашей целью.
Установка и использование библиотеки Pandas
Для установки пакета Pandas необходимо наличие в вашей системе пакета 
NumPy, а если вы выполняете сборку библиотеки из исходного кода, то и соот -
ветствующих утилит для компиляции исходных кодов на языках С и Cython, 
из которых состоит Pandas. Подробные инструкции по установке можно найти 
в документации пакета Pandas ( http://pandas.pydata.org/). Если же вы последовали 
совету из предисловия и воспользовались стеком Anaconda, то пакет Pandas 
у вас уже имеется.
После установки пакета Pandas можно импортировать его и проверить версию:
In[1]: import pandas
 pandas.__version__
Out[1]: '0.18.1'
Аналогично тому, как мы импортировали пакет NumPy под псевдонимом np, пакет 
Pandas импортируем под псевдонимом pd:
In[2]: import pandas as pd
Мы будем использовать эти условные обозначения для импорта далее в книге.
Напоминание о встроенной документации
Оболочка IPython предоставляет возможность быстро просматривать содержимое 
пакетов (с помощью клавиши Tab), а также документацию по различным функциям 
(используя символ ?). Загляните в раздел «Справка и документация в оболочке 
Python» главы 1, если вам нужно освежить в памяти эти возможности.
Для отображения всего содержимого пространства имен numpy можете ввести сле-
дующую команду:
In [3]: np.<TAB>

\1
In [4]: np?
Более подробную документацию, а также руководства и другие источники информа-
ции можно найти на сайте http://pandas.pydata.org.

--- СТРАНИЦА 131 ---
Знакомство с объектами библиотеки Pandas 131
Знакомство с объектами библиотеки Pandas
На самом примитивном уровне объекты библиотеки Pandas можно считать рас -
ширенной версией структурированных массивов библиотеки NumPy, в которых 
строки и столбцы идентифицируются метками, а не простыми числовыми инде-
ксами. Библиотека Pandas предоставляет множество полезных утилит, методов 
и функциональности в дополнение к базовым структурам данных, но все по -
следующее изложение потребует понимания этих базовых структур. Позвольте 
познакомить вас с тремя фундаментальными структурами данных библиотеки 
Pandas: классами Series, DataFrame и Index.
Начнем наш сеанс программирования с обычных импортов библиотек NumPy 
и Pandas:
In[1]: import numpy as np
 import pandas as pd
Объект Series библиотеки Pandas
Объект Series библиотеки Pandas — одномерный массив индексированных дан -
ных. Его можно создать из списка или массива следующим образом:
In[2]: data = pd.Series([0.25, 0.5, 0.75, 1.0])
 data
Out[2]: 0 0.25
 1 0.50
 2 0.75
 3 1.00
 dtype: float64
Как мы видели из предыдущего результата, объект Series служит адаптером как 
для последовательности значений, так и последовательности индексов, к которым 
можно получить доступ посредством атрибутов values и index. Атрибут values 
представляет собой уже знакомый нам массив NumPy:
In[3]: data.values
Out[3]: array([ 0.25, 0.5 , 0.75, 1. ])
index — массивоподобный объект типа pd.Index, который мы рассмотрим подробнее 
далее:
In[4]: data.index
Out[4]: RangeIndex(start=0, stop=4, step=1)1

\1ndas 
Аналогично массивам библиотеки NumPy, к данным можно обращаться по соот-
ветствующему им индексу посредством нотации с использованием квадратных 
скобок языка Python:
In[5]: data[1]
Out[5]: 0.5
In[6]: data[1:3]
Out[6]: 1 0.50
 2 0.75
 dtype: float64
Однако объект Series библиотеки Pandas намного универсальнее и гибче, чем 
эмулируемый им одномерный массив библиотеки NumPy.
Объект Series как обобщенный массив NumPy
Может показаться, что объект Series и одномерный массив библиотеки NumPy 
взаимозаменяемы. Основное различие между ними — индекс. В то время как ин-
декс массива NumPy, используемый для доступа к значениям, — целочисленный 
и описывается неявно, индекс объекта Series библиотеки Pandas описывается явно 
и связывается со значениями.

\1
In[7]: data = pd.Series([0.25, 0.5, 0.75, 1.0],
 index=['a', 'b', 'c', 'd'])
 data
Out[7]: a 0.25
 b 0.50
 c 0.75
 d 1.00
 dtype: float64

\1
In[8]: data['b']
Out[8]: 0.5

\1

--- СТРАНИЦА 133 ---
Знакомство с объектами библиотеки Pandas 133
In[9]: data = pd.Series([0.25, 0.5, 0.75, 1.0],
 index=[2, 5, 3, 7])
 data
Out[9]: 2 0.25
 5 0.50
 3 0.75
 7 1.00
 dtype: float64
In[10]: data[5]
Out[10]: 0.5
Объект Series как специализированный словарь
Объект Series библиотеки Pandas можно рассматривать как специализированную 
разновидность словаря языка Python. Словарь — структура, задающая соответствие 
произвольных ключей набору произвольных значений, а объект Series — струк -
тура, задающая соответствие типизированных ключей набору типизированных 
значений. Типизация важна: точно так же, как соответствующий типу специали-
зированный код для массива библиотеки NumPy при выполнении определенных 
операций делает его эффективнее, чем стандартный список Python, информация 
о типе в объекте Series библиотеки Pandas делает его намного более эффективным 
для определенных операций, чем словари Python.
Можно сделать аналогию «объект Series — словарь» еще более наглядной, скон-
струировав объект Series непосредственно из словаря Python:
In[11]: population_dict = {'California': 38332521,
 'Texas': 26448193,
 'New York': 19651127,
 'Florida': 19552860,
 'Illinois': 12882135}
 population = pd.Series(population_dict)
 population
Out[11]: California 38332521
 Florida 19552860
 Illinois 12882135
 New York 19651127
 Texas 26448193
 dtype: int64

\1
In[12]: population['California']
Out[12]: 383 32521

--- СТРАНИЦА 134 ---

\1ndas 

\1
In[13]: population['California':'Illinois']
Out[13]: California 38332521
 Florida 19552860
 Illinois 12882135
 dtype: int64
Мы рассмотрим некоторые нюансы индексации и срезов в библиотеке Pandas в раз -
деле «Индексация и выборка данных» этой главы.
Создание объектов Series
Мы уже изучили несколько способов создания объектов Series библиотеки Pandas 
с нуля. Все они представляют собой различные варианты следующего синтаксиса:
>>> pd.Series(data, index=index)
где index — необязательный аргумент, а data может быть одной из множества сущ-
ностей.
Например, аргумент data может быть списком или массивом NumPy. В этом случае 
index по умолчанию будет целочисленной последовательностью:
In[14]: pd.Series([2, 4, 6])
Out[14]: 0 2
 1 4
 2 6
 dtype: int64

\1
In[15]: pd.Series(5, index=[100, 200, 300])
Out[15]: 100 5
 200 5
 300 5
 dtype: int64
Аргумент data может быть словарем, в котором index по умолчанию является от-
сортированными ключами этого словаря:
In[16]: pd.Series({2:'a', 1:'b', 3:'c'})
Out[16]: 1 b
 2 a
 3 c
 dtype: object

--- СТРАНИЦА 135 ---
Знакомство с объектами библиотеки Pandas 135

\1
In[17]: pd.Series({2:'a', 1:'b', 3:'c'}, index=[3, 2])
Out[17]: 3 c
 2 a
 dtype: object
Обратите внимание, что объект Series заполняется только заданными явным об-
разом ключами.
Объект DataFrame библиотеки Pandas
Следующая базовая структура библиотеки Pandas — объ ект DataFrame. Как и объ-
ект Series , обсуждавшийся в предыдущем разделе, объект DataFrame можно 
рассматривать или как обобщение массива NumPy, или как специализированную 
версию словаря Python. Изучим оба варианта.
DataFrame как обобщенный массив NumPy

\1
In[18]:
area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,
 'Florida': 170312, 'Illinois': 149995}
area = pd.Series(area_dict)
area
Out[18]: California 423967
 Florida 170312
 Illinois 149995
 New York 141297
 Texas 695662
 dtype: int64
Воспользовавшись объектом population класса Series, сконструируем на основе 
словаря единый двумерный объект, содержащий всю эту информацию:

--- СТРАНИЦА 136 ---

\1ndas 
In[19]: states = pd.DataFrame({'population': population,
 'area': area})
 states
Out[19]: area population
 California 423967 38332521
 Florida 170312 19552860
 Illinois 149995 12882135
 New York 141297 19651127
 Texas 695662 26448193
Аналогично объекту Series у объекта DataFrame имеется атрибут index, обеспечи-
вающий доступ к меткам индекса:
In[20]: states.index
Out[20]:
Index(['California', 'Florida', 'Illinois', 'New York', 'Texas'],
 dtype='object')
Помимо этого, у объекта DataFrame есть атрибут columns, представляющий собой 
содержащий метки столбцов объект Index:
In[21]: states.columns
Out[21]: Index(['area', 'population'], dtype='object')

\1
In[22]: states['area']
Out[22]: California 423967
 Florida 170312
 Illinois 149995
 New York 141297
 Texas 695662
 Name: area, dtype: int64
Обратите внимание на возможный источник путаницы: в двумерном массиве 
NumPy data[0] возвращает первую строку. По этой причине объекты DataFrame 

--- СТРАНИЦА 137 ---
Знакомство с объектами библиотеки Pandas 137
лучше рассматривать как обобщенные словари, а не обобщенные массивы, хотя обе 
точки зрения имеют право на жизнь. Мы изучим более гибкие средства индексации 
объектов DataFrame в разделе «Индексация и выборка данных» этой главы.
Создание объектов DataFrame
Существует множество способов создания объектов DataFrame библиотеки Pandas. 

\1
In[23]: pd.DataFrame(population, columns=['population'])
Out[23]: population
 California 38332521
 Florida 19552860
 Illinois 12882135
 New York 19651127
 Texas 26448193

\1
In[24]: data = [{'a': i, 'b': 2 * i}
 for i in range(3)]
 pd.DataFrame(data)
Out[24]: a b
 0 0 0
 1 1 2
 2 2 4
Даже если некоторые ключи в словаре отсутствуют, библиотека Pandas просто за -
полнит их значениями NaN (то есть Not a number — «не является числом»):
In[25]: pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])
Out[25]: a b c
 0 1.0 2 NaN
 1 NaN 3 4.0

\1
In[26]: pd.DataFrame({'population': population,
 'area': area})
Out[26]: area population
 California 423967 38332521

--- СТРАНИЦА 138 ---

\1ndas 
 Florida 170312 19552860
 Illinois 149995 12882135
 New York 141297 19651127
 Texas 695662 26448193

\1
In[27]: pd.DataFrame(np.random.rand(3, 2),
 columns=['foo', 'bar'],
 index=['a', 'b', 'c'])
Out[27]: foo bar
 a 0.865257 0.213169
 b 0.442759 0.108267
 c 0.047110 0.905718
Из структурированного массива NumPy. Мы рассматривали структурированные 
массивы в разделе «Структурированные данные: структурированные массивы 
библиотеки NumPy» главы 2. Объект DataFrame библиотеки Pandas ведет себя во 
многом аналогично структурированному массиву и может быть создан непосред-
ственно из него:
In[28]: A = np.zeros(3, dtype=[('A', 'i8'), ('B', 'f8')])
 A 
Out[28]: array([(0, 0.0), (0, 0.0), (0, 0.0)],
 dtype=[('A', '<i8'), ('B', '<f8')])
In[29]: pd.DataFrame(A)
Out[29]: A B
 0 0 0.0
 1 0 0.0
 2 0 0.0
Объект Index библиотеки Pandas
Как объект Series, так и объект DataFrame содержат явный индекс , обеспечива -
ющий возможность ссылаться на данные и модифицировать их. Объект Index 
можно рассматривать или как неизменяемый массив (immutable array), или как 
упорядоченное множество (ordered set) (формально мультимножество, так как 
объекты Index могут содержать повторяющиеся значения). Из этих способов 
его представления следуют некоторые интересные возможности операций над 
объектами Index. В качестве простого примера создадим Index из списка целых 
чисел:

--- СТРАНИЦА 139 ---
Знакомство с объектами библиотеки Pandas 139
In[30]: ind = pd.Index([2, 3, 5, 7, 11])
 ind
Out[30]: Int64Index([2, 3, 5, 7, 11], dtype='int64')
Объект Index как неизменяемый массив
Объект Index во многом ведет себя аналогично массиву. Например, для извлечения 
из него значений или срезов можно использовать стандартную нотацию индекса-
ции языка Python:
In[31]: ind[1]
Out[31]: 3
In[32]: ind[::2]
Out[32]: Int64Index([2, 5, 11], dtype='int64')
У объектов Index есть много атрибутов, знакомых нам по массивам NumPy:
In[33]: print(ind.size, ind.shape, ind.ndim, ind.dtype)
5 (5,) 1 int64
Одно из различий между объектами Index и массивами NumPy — неизменяемость 
индексов, то есть их нельзя модифицировать стандартными средствами:
In[34]: ind[1] = 0
---------------------------------------------------------------------------
TypeError Traceback (most recent call last)
<ipython-input-34-40e631c82e8a> in <module>()
----> 1 ind[1] = 0
/Users/jakevdp/anaconda/lib/python3.5/site-packages/pandas/indexes/base.py ...
 1243
 1244 def __setitem__(self, key, value):
-> 1245 raise TypeError("Index does not support mutable operations")
 1246
 1247 def __getitem__(self, key):
TypeError: Index does not support mutable operations
Неизменяемость делает безопаснее совместное использование индексов несколь -
кими объектами DataFrame и массивами, исключая возможность побочных эффек-
тов в виде случайной модификации индекса по неосторожности.

--- СТРАНИЦА 140 ---

\1ndas 
Index как упорядоченное множество
Объекты библиотеки Pandas спроектированы с прицелом на упрощение таких 
операций, как соединения наборов данных, зависящие от многих аспектов ариф -
метики множеств. Объект Index следует большинству соглашений, использу -
емых встроенной структурой данных set языка Python, так что объединения, 
пересечения, разности и другие операции над множествами м ожно выполнять 
привычным образом:
In[35]: indA = pd.Index([1, 3, 5, 7, 9])
 indB = pd.Index([2, 3, 5, 7, 11])
In[36]: indA & indB # пересечение
Out[36]: Int64Index([3, 5, 7], dtype='int64')
In[37]: indA | indB # объединение
Out[37]: Int64Index([1, 2, 3, 5, 7, 9, 11], dtype='int64')
In[38]: indA ^ indB # симметричная разность
Out[38]: Int64Index([1, 2, 9, 11], dtype='int64')
Эти операции можно выполнять также методами объектов, например indA.in-
tersection(indB).
Индексация и выборка данных
В главе 2 мы подробно рассмотрели методы и инструменты доступ а, задания и из -
менения значений в массивах библиотеки NumPy: индексацию ( arr[2, 1]), срезы 
массивов ( arr[:,1:5]), маскирование ( arr[arr > 0]), «прихотливую» индексацию 
( arr[0,[1,5]]), а также их комбинации ( arr[:,[1,5]]). Здесь мы изучим анало -
гичные средства доступа и изменения значений в объектах Series и DataFrame 
библиотеки Pandas. Если вы использовали паттерны библиотеки NumPy, то соот-
ветствующие паттерны библиотеки Pandas будут для вас привычны.
Начнем с простого случая одномерного объекта Series, после чего перейдем к более 
сложному двумерному объекту DataFrame.
Выборка данных из объекта Series
Объект Series во многом ведет себя подобно одномерному массиву библиотеки 
NumPy и стандартному словарю языка Python. Это поможет нам лучше понимать 
паттерны индексации и выборки данных из этих массивов.

--- СТРАНИЦА 141 ---

\1
In[1]: import pandas as pd
 data = pd.Series([0.25, 0.5, 0.75, 1.0],
 index=['a', 'b', 'c', 'd'])
 data
Out[1]: a 0.25
 b 0.50
 c 0.75
 d 1.00
 dtype: float64
In[2]: data['b']
Out[2]: 0.5
Для просмотра ключей/индексов и значений выражения можно также использо-
вать методы языка Python, аналогичные таковым для словарей:
In[3]: 'a' in data
Out[3]: True
In[4]: data.keys()
Out[4]: Index(['a', 'b', 'c', 'd'], dtype='object')
In[5]: list(data.items())
Out[5]: [('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)]

\1
In[6]: data['e'] = 1.25
 data
Out[6]: a 0.25
 b 0.50
 c 0.75
 d 1.00
 e 1.25
 dtype: float64
Такая легкая изменяемость объектов — удобная возможность: библиотека Pandas 
сама, незаметно для нас, принимает решения о размещении в памяти и необходимости 

--- СТРАНИЦА 142 ---

\1ndas 
копирования данных. Пользователю, как правило, не приходится заботиться о по -
добных вопросах.

\1
In[7]: # срез посредством явного индекса
 data['a':'c']
Out[7]: a 0.25
 b 0.50
 c 0.75
 dtype: float64
In[8]: # срез посредством неявного целочисленного индекса
 data[0:2]
Out[8]: a 0.25
 b 0.50
 dtype: float64
In[9]: # маскирование
 data[(data > 0.3) & (data < 0.8)]
Out[9]: b 0.50
 c 0.75
 dtype: float64
In[10]: # «прихотливая» индексация
 data[['a', 'e']]
Out[10]: a 0.25
 e 1.25
 dtype: float64
Наибольшие затруднения среди них могут вызвать срезы. Обратите внимание, 
что при выполнении среза с помощью явного индекса ( data['a':'c']) значение, 
соответствующее последнему индексу, включается в срез, а при срезе неявным 
индексом ( data[0:2]) — не включается.
Индексаторы: loc, iloc и ix
Подобные обозначения для срезов и индексации могут привести к путанице. 
Например, при наличии у объекта Series явного целочисленного индекса опера-

--- СТРАНИЦА 143 ---
Индексация и выборка данных 143
ция индексации ( data[1]) будет использовать явные индексы, а операция среза 
( data[1:3]) — неявный индекс в стиле языка Python.
In[11]: data = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])
 data
Out[11]: 1 a
 3 b
 5 c
 dtype: object
In[12]: # Использование явного индекса при индексации
 data[1]
Out[12]: 'a'
In[13]: # Использование неявного индекса при срезе
 data[1:3]
Out[13]: 3 b
 5 c
 dtype: object
Из-за этой потенциальной путаницы в случае целочисленных индексов в библио-
теке Pandas предусмотрены специальные атрибуты- индексаторы , позволяющие 
явным образом применять определенные схемы индексации. Они являются не 
функциональными методами, а именно атрибутами, предоставляющими для дан-
ных из объекта Series определенный интерфейс для выполнения срезов.

\1
In[14]: data.loc[1]
Out[14]: 'a'
In[15]: data.loc[1:3]
Out[15]: 1 a
 3 b
 dtype: object
Атрибут iloc дает возможность выполнить индексацию и срезы, применяя неявный 
индекс в стиле языка Python:
In[16]: data.iloc[1]
Out[16]: 'b'
In[17]: data.iloc[1:3]
Out[17]: 3 b

--- СТРАНИЦА 144 ---

\1ndas 
 5 c
 dtype: object
Третий атрибут-индексатор ix представляет собой гибрид первых двух и для 
объектов Series эквивалентен обычной индексации с помощью []. Назначение 
индексатора ix станет понятнее в контексте объектов DataFrame, которые мы рас -
смотрим далее.
Один из руководящих принципов написания кода на языке Python — «лучше явно, 
чем неявно». То, что атрибуты loc и iloc по своей природе явные, делает их очень 
удобными для обеспечения «чистоты» и удобочитаемости кода. Я рекомендую 
использовать оба, особенно в случае целочисленных индексов, чтобы сделать код 
более простым для чтения и понимания и избежать слу чайных малозаметных 
ошибок при обозначении индексации и срезов.

\1
In[18]: area = pd.Series({'California': 423967, 'Texas': 695662,
 'New York': 141297, 'Florida': 170312,
 'Illinois': 149995})
 pop = pd.Series({'California': 38332521, 'Texas': 26448193,
 'New York': 19651127, 'Florida': 19552860,
 'Illinois': 12882135})
 data = pd.DataFrame({'area':area, 'pop':pop})
 data
Out[18]: area pop
 California 423967 38332521
 Florida 170312 19552860
 Illinois 149995 12882135
 New York 141297 19651127
 Texas 695662 26448193

\1

--- СТРАНИЦА 145 ---
Индексация и выборка данных 145
In[19]: data['area']
Out[19]: California 423967
 Florida 170312
 Illinois 149995
 New York 141297
 Texas 695662
 Name: area, dtype: int64

\1
In[20]: data.area
Out[20]: California 423967
 Florida 170312
 Illinois 149995
 New York 141297
 Texas 695662
 Name: area, dtype: int64

\1
In[21]: data.area is data['area']
Out[21]: True

\1
In[22]: data.pop is data['pop']
Out[22]: False

\1
In[23]: data['density'] = data['pop'] / data['area']
 data
Out[23]: area pop density
 California 423967 38332521 90.413926
 Florida 170312 19552860 114.806121

--- СТРАНИЦА 146 ---

\1ndas 
 Illinois 149995 12882135 85.883763
 New York 141297 19651127 139.076746
 Texas 695662 26448193 38.018740
Приведенный пример демонстрирует простоту синтаксиса поэлементных опера -
ций над объектами Series. Этот вопрос мы изучим подробнее в разделе «Опера -
ции над данными в библиотеке Pandas» текущей главы.

\1
In[24]: data.values
Out[24]: array([[ 4.23967000e+05, 3.83325210e+07, 9.04139261e+01],
 [ 1.70312000e+05, 1.95528600e+07, 1.14806121e+02],
 [ 1.49995000e+05, 1.28821350e+07, 8.58837628e+01],
 [ 1.41297000e+05, 1.96511270e+07, 1.39076746e+02],
 [ 6.95662000e+05, 2.64481930e+07, 3.80187404e+01]])

\1
In[25]: data.T 
Out[25]:
 California Florida Illinois New York Texas
area 4.239670e+05 1.703120e+05 1.499950e+05 1.412970e+05 6.956620e+05
pop 3.833252e+07 1.955286e+07 1.288214e+07 1.965113e+07 2.644819e+07
density 9.041393e+01 1.148061e+02 8.588376e+01 1.390767e+02 3.801874e+01

\1
In[26]: data.values[0]
Out[26]: array([ 4.23967000e+05, 3.83325210e+07, 9.04139261e+01])
а указание отдельного «индекса» для объекта DataFrame — доступ к столбцу:
In[27]: data['area']
Out[27]: California 423967
 Florida 170312

--- СТРАНИЦА 147 ---
Индексация и выборка данных 147
 Illinois 149995
 New York 141297
 Texas 695662
 Name: area, dtype: int64
Таким образом, нам необходим еще один тип синтаксиса для индексации, анало-
гичной по стилю индексации массивов. Библиотека Pandas применяет упомянутые 
ранее индексаторы loc, iloc и ix. С помощью индексатора iloc можно индексиро-
вать исходный массив, как будто это простой массив NumPy (используя неявный 
синтаксис языка Python), но с сохранением в результирующих данных меток объ -
екта DataFrame для индекса и столбцов:
In[28]: data.iloc[:3, :2]
Out[28]: area pop
 California 423967 38332521
 Florida 170312 19552860
 Illinois 149995 12882135
In[29]: data.loc[:'Illinois', :'pop']
Out[29]: area pop
 California 423967 38332521
 Florida 170312 19552860
 Illinois 149995 12882135

\1
In[30]: data.ix[:3, :'pop']
Out[30]: area pop
 California 423967 38332521
 Florida 170312 19552860
 Illinois 149995 12882135

\1
In[31]: data.loc[data.density > 100, ['pop', 'density']]
Out[31]: pop density
 Florida 19552860 114.806121
 New York 19651127 139.076746

--- СТРАНИЦА 148 ---

\1ndas 

\1
In[32]: data.iloc[0, 2] = 90
 data
Out[32]: area pop density
 California 423967 38332521 90.000000
 Florida 170312 19552860 114.806121
 Illinois 149995 12882135 85.883763
 New York 141297 19651127 139.076746
 Texas 695662 26448193 38.018740
Чтобы достичь уверенности при манипуляции данными с помощью библиотеки 
Pandas, рекомендую потратить немного времени на экс перименты над простым 
объектом DataFrame и пробы типов индексации, срезов, маскирования и «прихот-
ливой» индексации.

\1
In[33]: data['Florida':'Illinois']
Out[33]: area pop density
 Florida 170312 19552860 114.806121
 Illinois 149995 12882135 85.883763

\1
In[34]: data[1:3]
Out[34]: area pop density
 Florida 170312 19552860 114.806121
 Illinois 149995 12882135 85.883763

\1
In[35]: data[data.density > 100]
Out[35]: area pop density
 Florida 170312 19552860 114.806121
 New York 141297 19651127 139.076746
Эти два варианта обозначений синтаксически подобны таковым для массивов 
библиотеки NumPy, и они, возможно, хоть и не вполне вписываются в шаблоны 
синтаксиса библиотеки Pandas, но весьма удобны на практике.

--- СТРАНИЦА 149 ---
Операции над данными в библиотеке Pandas 149
Операции над данными в библиотеке Pandas
Одна из важнейших составляющих библиотеки NumPy — способность выполнять 
быстрые поэлементные операции — как простейшие арифметические (сложение, 
вычитание, умножение и т. д.), так и более сложные (тр игонометрические, по -
казательные и логарифмические функции и т. п.). Библиотека Pandas наследует 
от NumPy немалую часть этой функциональности, и ключ к ее использованию — 
универсальные функции, с которыми мы познакомились в разделе «Выполне -
ние вычислений над массивами библиотеки NumPy: универсальные функции» 
главы 2.
Однако библиотека Pandas включает несколько полезных трюков: для унарных 
операций, например изменения знака и тригонометрических функций, при ис -
пользовании ее универсальных функций в выводе будут сохранены индекс и метки 
столбцов, а для бинарных операций, например сложения и умножения, библиотека 
Pandas будет автоматически выравнивать индексы при передаче объектов универ-
сальной функции. Это значит, что сохранение контекста данных и объединение 
данных из различных источников — две задачи, потенциально чреватые ошибками 
при работе с исходными массивами библиотеки NumPy, — становятся надежно 
защищенными от ошибок благодаря библиотеке Pandas. Кроме того, в библиотеке 
заданы операции между одномерными структурами объектов Series и двумерными 
структурами объектов DataFrame.
Универсальные функции: сохранение индекса
В силу того что библиотека Pandas предназначена для работы с библиотекой 
NumPy, все универсальные функции библиотеки NumPy будут работать с объек-
тами Series и DataFrame библиотеки Pandas. Начнем с описания простых объектов 
Series и DataFrame для демонстрации этого:
In[1]: import pandas as pd
 import numpy as np
In[2]: rng = np.random.RandomState(42)
 ser = pd.Series(rng.randint(0, 10, 4))
 ser
Out[2]: 0 6
 1 3
 2 7
 3 4
 dtype: int64
In[3]: df = pd.DataFrame(rng.randint(0, 10, (3, 4)),
 columns=['A', 'B', 'C', 'D'])
 df

--- СТРАНИЦА 150 ---

\1ndas 
Out[3]: A B C D
 0 6 9 2 6
 1 7 4 3 7
 2 7 2 5 4
Если применить универсальную функцию NumPy к любому из этих объектов, ре-
зультатом будет другой объект библиотеки Pandas с сохранением индексов:
In[4]: np.exp(ser)
Out[4]: 0 403.428793
 1 20.085537
 2 1096.633158
 3 54.598150
 dtype: float64

\1
In[5]: np.sin(df * np.pi / 4)
Out[5]: A B C D 
 0 -1.000000 7.071068e-01 1.000000 -1.000000e+00
 1 -0.707107 1.224647e-16 0.707107 -7.071068e-01
 2 -0.707107 1.000000e+00 -0.707107 1.224647e-16
Все описанные в разделе «Выполнение вычислений над массивами библиотеки 
NumPy: универсальные функции» главы 2 универсальные функции можно ис -
пользовать аналогично вышеприведенным.
Универсальные функции: выравнивание индексов
При бинарных операциях над двумя объектами Series или DataFrame библиотека 
Pandas будет выравнивать индексы в процессе выполнения операции. Это очень 
удобно при работе с неполными данными.

\1
In[6]: area = pd.Series({'Alaska': 1723337, 'Texas': 695662,
 'California': 423967}, name='area')
 population = pd.Series({'California': 38332521, 'Texas': 26448193,
 'New York': 19651127}, name='population')

\1

--- СТРАНИЦА 151 ---
Операции над данными в библиотеке Pandas 151
In[7]: population / area
Out[7]: Alaska NaN
 California 90.413926
 New York NaN
 Texas 38.018740
 dtype: float64
Получившийся в итоге массив содержит объединение индексов двух исходных мас-
сивов, которое можно определить посредством стандартной арифметики множеств 
языка Python для этих индексов:
In[8]: area.index | population.index
Out[8]: Index(['Alaska', 'California', 'New York', 'Texas'], dtype='object')
Ни один из относящихся к ним обоим элементов не содержит значения NaN («нечис -
ловое значение»), с помощью которого библиотека Pandas отмечает пропущенные 
данные (см. дальнейшее обсуждение вопроса отсутствующих данных в разделе «Об-
работка отсутствующих данных» этой главы). Аналогичным образом реализовано 
сопоставление индексов для всех встроенных арифметич еских выражений языка 
Python: все отсутствующие значения заполняются по умолчанию значением NaN:
In[9]: A = pd.Series([2, 4, 6], index=[0, 1, 2])
 B = pd.Series([1, 3, 5], index=[1, 2, 3])
 A + B 
Out[9]: 0 NaN
 1 5.0
 2 9.0
 3 NaN
 dtype: float64

\1
In[10]: A.add(B, fill_value=0)
Out[10]: 0 2.0
 1 5.0
 2 9.0
 3 5.0
 dtype: float64

\1

--- СТРАНИЦА 152 ---

\1ndas 
In[11]: A = pd.DataFrame(rng.randint(0, 20, (2, 2)),
 columns=list('AB'))
 A 
Out[11]: A B
 0 1 11
 1 5 1
In[12]: B = pd.DataFrame(rng.randint(0, 10, (3, 3)),
 columns=list('BAC'))
 B 
Out[12]: B A C
 0 4 0 9
 1 5 8 0
 2 9 2 6
In[13]: A + B 
Out[13]: A B C
 0 1.0 15.0 NaN
 1 13.0 6.0 NaN
 2 NaN NaN NaN

\1
In[14]: fill = A.stack().mean()
 A.add(B, fill_value=fill)
Out[14]: A B C
 0 1.0 15.0 13.5
 1 13.0 6.0 4.5
 2 6.5 13.5 10.5

\1n [45]: A.stack()
 Out[45]:
 0 A 1
 B 11
 1 A 5
 B 1
 dtype: int32
 См. также подраздел «Мультииндекс как дополнительное измерение» раздела «Мульти-
индексированный объект Series» текущей главы.

--- СТРАНИЦА 153 ---
Операции над данными в библиотеке Pandas 153
В табл. 3.1 приведен перечень операторов языка Python и эквивалентных им мето -
дов объектов библиотеки Pandas.
Таблица 3.1. Соответствие между операторами языка Python и методами библиотеки Pandas
Оператор языка Python Метод (-ы) библиотеки Pandas
+ add()
– sub(), subtract()
* mul(), multiply()
/ truediv(), div(), divide()
// floordiv()
% mod()
** pow()

\1
In[15]: A = rng.randint(10, size=(3, 4))
 A 
Out[15]: array([[3, 8, 2, 4],
 [2, 6, 4, 8],
 [6, 1, 3, 8]])
In[16]: A - A[0]
Out[16]: array([[ 0, 0, 0, 0],
 [-1, -2, 2, 4],
 [ 3, -7, 1, 4]])
В соответствии с правилами транслирования библиотеки NumPy («Операции над 
массивами. Транслирование» главы 2), вычитание из двумерного массива одной 
из его строк выполняется построчно.
В библиотеке Pandas вычитание по умолчанию также происходит построчно:
In[17]: df = pd.DataFrame(A, columns=list('QRST'))
 df - df.iloc[0]
Out[17]: Q R S T
 0 0 0 0 0

--- СТРАНИЦА 154 ---

\1ndas 
 1 -1 -2 2 4
 2 3 -7 1 4

\1
In[18]: df.subtract(df['R'], axis=0)
Out[18]: Q R S T
 0 -5 0 -6 -4
 1 -4 0 -2 2
 2 5 0 2 7

\1
In[19]: halfrow = df.iloc[0, ::2]
 halfrow
Out[19]: Q 3
 S 2
 Name: 0, dtype: int64
In[20]: df - halfrow
Out[20]: Q R S T
 0 0.0 NaN 0.0 NaN
 1 -1.0 NaN 2.0 NaN
 2 3.0 NaN 1.0 NaN
Подобное сохранение и выравнивание индексов и столбцов означает, что операции 
над данными в библиотеке Pandas всегда сохраняют контекст данных, предотвра-
щая возможные ошибки при работе с неоднородными и/или неправильно/неоди-
наково выровненными данными в исходных массивах NumPy.
Обработка отсутствующих данных
Реальные данные редко бывают очищенными и однородными. В частности, во мно-
гих интересных наборах данных некоторое количество дан ных отсутствует. Еще 
более затрудняет работу то, что в различных источниках данных отсутствующие 
данные могут быть помечены различным образом.
В этом разделе мы обсудим общие соображения, касающиеся отсутствующих 
данных, обсудим способы представления их библиотекой Pandas и продемонстри -
руем встроенные инструменты библиотеки Pandas для обработки отсутствующих 
данных в языке Python. Здесь и далее мы будем называть отсутствующие данные 
null , NaN или NA -значениями.

--- СТРАНИЦА 155 ---
Обработка отсутствующих данных 155
Компромиссы при обозначении отсутствующих данных
Разработано несколько схем для обозначения наличия пропущенных данных в та-
блицах или объектах DataFrame. Они основываются на одной из двух стратегий: 
использование маски, отмечающей глобально отсутствующие значения, или выбор 
специального значения-индикатора (sentinel value), обозначающего пропущенное 
значение.
Маска может быть или совершенно отдельным булевым массивом или может 
включать выделение одного бита представления данных на локальную индикацию 
отсутствия значения.
Значение-индикатор может быть особым условным обозначением, подходящим для 
конкретных данных, например указывать на отсутствующее целое число с помощью 
значения –9999 или какой-то редкой комбинации битов. Или же оно может быть 
более глобальным обозначением, например обозначать отсутствующее значение 
с плавающей точкой с помощью NaN — специального значения, включенного 
в спецификации IEEE для чисел с плавающей точкой.
В каждом из подходов есть свои компромиссы: использование отдельного массива-
маски требует выделения памяти под дополнительный булев массив, приводящего 
к накладным расходам в смысле как оперативной памяти, так и процессорного 
времени. Значение-индикатор сокращает диапазон доступных для представления 
допустимых значений и может потребовать выполнения дополнительной (зачастую 
неоптимизированной) логики при арифметических операциях на CPU и GPU. 
Общие специальные значения, такие как NaN , доступны не для всех типов данных.
Как и в большинстве случаев, где нет универсального оптимального варианта, раз-
личные языки программирования и системы используют различные обозначения. 
Например, язык R применяет зарезервированные комбинации битов для каждого 
типа данных в качестве значений-индикаторов для отсутствующих данных. А си-
стема SciDB использует для индикации состояния NA дополнительный байт, при -
соединяемый к каждой ячейке.
Отсутствующие данные в библиотеке Pandas
Способ обработки отсутствующих данных библиотекой Pandas определяется тем, 
что она основана на пакете NumPy, в котором отсутствует встроенное понятие NA-
значений для всех типов данных, кроме данных с плавающей точкой.
Библиотека Pandas могла бы последовать примеру языка R и задавать комбинации 
битов для каждого конкретного типа данных для индикации отсутствия значения, 
но этот подход оказывается довольно громоздким. Ведь если в языке R насчитыва -
ется всего четыре базовых типа данных, то NumPy поддерживает намного больше. 
Например, в языке R есть только один целочисленный тип, а библиотека NumPy 

--- СТРАНИЦА 156 ---

\1ndas 
поддерживает четырнадцать простых целочисленных типов, с учетом различной 
точности, знаковости/беззнаковости и порядка байтов числа. Резервирование спе -
циальной комбинации битов во всех доступных в библиотеке NumPy типах данных 
привело бы к громадным накладным расходам в разнообразных частных случаях 
операций для различных типов и, вероятно, потребовало бы даже отдельной ветви 
пакета NumPy. Кроме того, для небольших типов данных (например, 8-битных 
целых чисел) потеря одного бита на маску существенно сузит диапазон доступных 
для представления этим типом значений.
Библиотека NumPy поддерживает использование маскированных массивов, то 
есть массивов, к которым присоединены отдельные булевы массивы-маски, пред-
назначенные для маркирования как «плохих» или «хороших» данных. Библиотека 
Pandas могла унаследовать такую возможность, но накладные расходы на хране -
ние, вычисления и поддержку кода сделали этот вариант малопривлекательным.
По этой причине в библиотеке Pandas было решено использовать для отсутству-
ющих данных индикаторы, а также два уже существующих в Python пустых зна-
чения: специальное значение NaN с плавающей точкой и объект None языка Python. 
У этого решения есть свои недостатки, но на практике в большинстве случаев оно 
представляет собой удачный компромисс.
None: отсутствующие данные в языке Python
Первое из используемых библиотекой Pandas значений-индикаторов — None, 
объект-одиночка Python, часто применяемый для обозначения отсутствующих 
данных в коде на языке Python. В силу того что None — объект Python, его нельзя 
использовать в произвольных массивах библиотек NumPy/Pandas, а только в мас -
сивах с типом данных 'object' (то есть массивах объектов языка Python):
In[1]: import numpy as np
 import pandas as pd
In[2]: vals1 = np.array([1, None, 3, 4])
 vals1
Out[2]: array([1, None, 3, 4], dtype=object)
Выражение dtype=object означает, что наилучший возможный вывод об общем 
типе элементов содержимого данного массива, который только смогла сделать 
библиотека NumPy, — то, что они все являются объектами Python. Хотя такая 
разновидность массивов полезна для определенных целей, все операции над ними 
будут выполняться на уровне языка Python, с накладными расходами, значительно 
превышающими расходы на выполнение быстрых операций над массивами с на-
тивными типами данных:
In[3]: for dtype in ['object', 'int']:

--- СТРАНИЦА 157 ---
Обработка отсутствующих данных 157
 print("dtype =", dtype)
 %timeit np.arange(1E6, dtype=dtype).sum()
 print()
dtype = object
10 loops, best of 3: 78.2 ms per loop
dtype = int
100 loops, best of 3: 3.06 ms per loop
Использование объектов языка Python в массивах означает также, что при выпол-
нении функций агрегирования по массиву со значениями None, например sum() или 
min(), вам будет возвращена ошибка:
In[4]: vals1.sum()
TypeError Traceback (most recent call last)
<ipython-input-4-749fd8ae6030> in <module>()
----> 1 vals1.sum()
/Users/jakevdp/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py ...
 30
 31 def _sum(a, axis=None, dtype=None, out=None, keepdims=False):
---> 32 return umr_sum(a, axis, dtype, out, keepdims)
 33
 34 def _prod(a, axis=None, dtype=None, out=None, keepdims=False):
TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'
Эта ошибка отражает тот факт, что операция между целочисленным значением 
и значением None не определена.
NaN: отсутствующие числовые данные

\1
In[5]: vals2 = np.array([1, np.nan, 3, 4])
 vals2.dtype
Out[5]: dtype('float64')
Обратите внимание, что библиотека NumPy выбрала для этого массива нативный 
тип с плавающей точкой: это значит, что, в отличие от вышеупомянутого массива 
объектов, этот массив поддерживает быстрые операции, передаваемые на выпол-
нение скомпилированному коду. Вы должны отдавать себе отчет, что значение 

--- СТРАНИЦА 158 ---

\1ndas 
NaN в чем-то подобно «вирусу данных»: оно «заражает» любой объект, к которому 
«прикасается». Вне зависимости от операции результат арифметического действия 
с участием NaN будет равен NaN:
In[6]: 1 + np.nan
Out[6]: nan
In[7]: 0 * np.nan
Out[7]: nan

\1
In[8]: vals2.sum(), vals2.min(), vals2.max()
Out[8]: (nan, nan, nan)

\1
In[9]: np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)
Out[9]: (8.0, 1.0, 4.0)
Не забывайте, что NaN — именно значение с плавающей точкой, аналога значения 
NaN для целочисленных значений, строковых и других типов не существует.
Значения NaN и None в библиотеке Pandas
Как у значения NaN, так и у None есть свое назначение, и библиотека Pandas делает 
их практически взаимозаменяемыми путем преобразования одного в другое в опре-
деленных случаях:
In[10]: pd.Series([1, np.nan, 2, None])
Out[10]: 0 1.0
 1 NaN
 2 2.0
 3 NaN
 dtype: float64
Библиотека Pandas автоматически выполняет преобразование при обнаружении 
NA -значений для тех типов, у которых отсутствует значение-индикатор. Напри -
мер, если задать значение элемента целочисленного массива равным np.nan, для 
соответствия типу отсутствующего значения будет автоматически выполнено по-
вышающее приведение типа этого массива к типу с плавающей точкой:

--- СТРАНИЦА 159 ---
Обработка отсутствующих данных 159
In[11]: x = pd.Series(range(2), dtype=int)
 x 
Out[11]: 0 0
 1 1
 dtype: int64
In[12]: x[0] = None
 x 
Out[12]: 0 NaN
 1 1.0
 dtype: float64
Обратите внимание, что, помимо приведения типа целочисленного массива к мас -
сиву значений с плавающей точкой, библиотека Pandas автоматически преобразует 
значение None в NaN. Замечу, что существует план по внесению в будущем нативного 
целочисленного NA в библиотеку Pandas, но на момент написания данной книги 
оно еще не было включено.
Хотя подобный подход со значениями-индикаторами/приведением типов библио-
теки Pandas может показаться несколько вычурным по сравнению с более уни -
фицированным подходом к NA -значениям в таких предметно-ориентированных 
языках, как R, на практике он прекрасно работает и на моей памяти лишь изредка 
вызывал проблемы.
В табл. 3.2 перечислены правила повышающего приведения типов в библиотеке 
Pandas в случае наличия NA -значений.
Таблица 3.2. Правила повышающего приведения типов в библиотеке Pandas
Класс типов Преобразование при хранении 
NA-значений
Значение-индикатор NA
С плавающей точкой Без изменений np.nan
Объект (object) Без изменений None или np.nan
Целое число Приводится к float64 np.nan
Булево значение Приводится к object None или np.nan
Имейте в виду, что строковые данные в библиотеке Pandas всегда хранятся с типом 
данных (dtype) object.
Операции над пустыми значениями
Библиотека Pandas рассматривает значения None и NaN как взаимозаменяемые 
средства указания на отсутствующие или пустые значения. Существует несколько 
удобных методов для обнаружения, удаления и замены пустых значений в струк-
турах данных библиотеки Pandas, призванных упростить работу с ними.

--- СТРАНИЦА 160 ---

\1ndas 
 isnull() — генерирует булеву маску для отсутствующих значений.
 notnull() — противоположность метода isnull().
 dropna() — возвращает отфильтрованный вариант данных.
 fillna() — возвращает копию данных, в которой пропущенные значения за -
полнены или восстановлены.
Завершим раздел кратким рассмотрением и демонстрацией этих методов.
Выявление пустых значений
У структур данных библиотеки Pandas имеются два удо бных метода для выявления 
пустых значений: isnull() и notnull(). Каждый из них возвращает булеву маску 
для данных. Например:
In[13]: data = pd.Series([1, np.nan, 'hello', None])
In[14]: data.isnull()
Out[14]: 0 False
 1 True
 2 False
 3 True
 dtype: bool

\1
In[15]: data[data.notnull()]
Out[15]: 0 1
 2 hello
 dtype: object
Аналогичные булевы результаты дает использование методов isnull() и notnull() 
для объектов DataFrame.
Удаление пустых значений
Помимо продемонстрированного выше маскирования, сущ ествуют удобные ме -
тоды: dropna() (отбрасывающий NA -значения) и fillna() (заполняющий NA -
значения). Для объекта Series результат вполне однозначен:
In[16]: data.dropna()
Out[16]: 0 1
 2 hello
 dtype: object

--- СТРАНИЦА 161 ---

\1
In[17]: df = pd.DataFrame([[1, np.nan, 2],
 [2, 3, 5],
 [np.nan, 4, 6]])
 df
Out[17]: 0 1 2
 0 1.0 NaN 2
 1 2.0 3.0 5
 2 NaN 4.0 6
Нельзя выбросить из DataFrame отдельные значения, только целые строки или 
столбцы. В зависимости от приложения может понадобиться тот или иной вариант, 
так что функция dropna() предоставляет для случая объектов DataFrame несколько 
параметров.
По умолчанию dropna() отбрасывает все строки, в которых присутствует хотя бы 
одно пустое значение:
In[18]: df.dropna()
Out[18]: 0 1 2
 1 2.0 3.0 5

\1
In[19]: df.dropna(axis='columns')
Out[19]: 2
 0 2
 1 5
 2 6
Однако при этом отбрасываются также и некоторые «хорошие» данные. Возмож-
но, вам захочется отбросить строки или столбцы, все значения (или большинство) 
в которых представляют собой NA . Такое поведение можно задать с помощью па-
раметров how и thresh, обеспечивающих точный контроль допустимого количества 
пустых значений.
По умолчанию how='any', то есть отбрасываются все строки или столбцы (в зави-
симости от ключевого слова axis), содержащие хоть одно пустое значение. Можно 
также указать значение how='all', при нем будут отбрасываться только строки/
столбцы, все значения в которых пустые:
In[20]: df[3] = np.nan
 df
Out[20]: 0 1 2 3

--- СТРАНИЦА 162 ---

\1ndas 
 0 1.0 NaN 2 NaN
 1 2.0 3.0 5 NaN
 2 NaN 4.0 6 NaN
In[21]: df.dropna(axis='columns', how='all')
Out[21]: 0 1 2
 0 1.0 NaN 2
 1 2.0 3.0 5
 2 NaN 4.0 6

\1
In[22]: df.dropna(axis='rows', thresh=3)
Out[22]: 0 1 2 3
 1 2.0 3.0 5 NaN
В данном случае отбрасываются первая и последняя строки, поскольку в них со-
держится только по два непустых значения.
Заполнение пустых значений
Иногда предпочтительнее вместо отбрасывания пустых значений заполнить их 
каким-то допустимым значением. Это значение может быть фиксированным, 
например нулем, или интерполированным или восстановленным на основе «хо -
роших» данных значением. Это можно сделать путем замены в исходных данных, 
используя результат метода isnull() в качестве маски. Но это настолько распро-
страненная операция, что библиотека Pandas предоставляет метод fillna(), воз-
вращающий копию массива с замененными пустыми значениями.

\1
In[23]: data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))
 data
Out[23]: a 1.0
 b NaN
 c 2.0
 d NaN
 e 3.0
 dtype: float64

\1
In[24]: data.fillna(0)
Out[24]: a 1.0

--- СТРАНИЦА 163 ---

\1
In[25]: # заполнение по направлению «вперед»
 data.fillna(method='ffill')
Out[25]: a 1.0
 b 1.0
 c 2.0
 d 2.0
 e 3.0
 dtype: float64

\1
In[26]: # заполнение по направлению «назад»
 data.fillna(method='bfill')
Out[26]: a 1.0
 b 2.0
 c 2.0
 d 3.0
 e 3.0
 dtype: float64

\1
In[27]: df
Out[27]: 0 1 2 3
 0 1.0 NaN 2 NaN
 1 2.0 3.0 5 NaN
 2 NaN 4.0 6 NaN
In[28]: df.fillna(method='ffill', axis=1)
Out[28]: 0 1 2 3
 0 1.0 1.0 2.0 2.0
 1 2.0 3.0 5.0 5.0
 2 NaN 4.0 6.0 6.0
Обратите внимание, что если при заполнении по направлению «вперед» предыду-
щего значения нет, то NA -значение остается незаполненным.

--- СТРАНИЦА 164 ---

\1ndas 
Иерархическая индексация
До сих пор мы рассматривали главным образом одномерные и двумерные данные, 
находящиеся в объектах Series и DataFrame библиотеки Pandas. Часто бывает удобно 
выйти за пределы двух измерений и хранить многомерные данные, то есть данные, 
индексированные по более чем двум ключам. Хотя библиотека Pandas предоставля -
ет объекты Panel и Panel4D, позволяющие нативным образом хранить трехмерные 
и четырехмерные данные (см. врезку «Данные объектов Panel» на с. 178), на прак -
тике намного чаще используется иерархическая индексация (hierarchical indexing), 
или мультииндексация (multi-indexing), для включения в один индекс нескольких 
уровней. При этом многомерные данные могут быть компактно представлены в уже 
привычных нам одномерных объектах Series и двумерных объектах DataFrame.
В этом разделе мы рассмотрим создание объектов MultiIndex напрямую, приведем 
соображения относительно индексации, срезов и вычисления статистических по-
казателей по мультииндексированным данным, а также полезные методы для пре-
образования между простым и иерархически индексированным представлением 
данных.

\1
In[1]: import pandas as pd
 import numpy as np
Мультииндексированный объект Series
Рассмотрим, как можно представить двумерные данные в одномерном объекте 
Series. Для конкретики изучим ряд данных, в котором у каждой точки имеются 
символьный и числовой ключи.
Плохой способ
Пускай нам требуется проанализировать данные о штатах за два разных года. Вам 
может показаться соблазнительным, воспользовавшись утилитами библиотеки 
Pandas, применить в качестве ключей кортежи языка Python:
In[2]: index = [('California', 2000), ('California', 2010),
 ('New York', 2000), ('New York', 2010),
 ('Texas', 2000), ('Texas', 2010)]
 populations = [33871648, 37253956,
 18976457, 19378102,
 20851820, 25145561]
 pop = pd.Series(populations, index=index)
 pop
Out[2]: (California, 2000) 33871648

--- СТРАНИЦА 165 ---
Иерархическая индексация 165
 (California, 2010) 37253956
 (New York, 2000) 18976457
 (New York, 2010) 19378102
 (Texas, 2000) 20851820
 (Texas, 2010) 25145561
 dtype: int64

\1
In[3]: pop[('California', 2010):('Texas', 2000)]
Out[3]: (California, 2010) 37253956
 (New York, 2000) 18976457
 (New York, 2010) 19378102
 (Texas, 2000) 20851820
 dtype: int64

\1
In[4]: pop[[i for i in pop.index if i[1] == 2010]]
Out[4]: (California, 2010) 37253956
 (New York, 2010) 19378102
 (Texas, 2010) 25145561
 dtype: int64
Это хоть и приводит к желаемому результату, но гораздо менее изящно (и далеко 
не так эффективно), как использование синтаксиса срезов, столь полюбившегося 
нам в библиотеке Pandas.
Лучший способ
В библиотеке Pandas есть лучший способ выполнения таких операций. Наша ин-
дексация, основанная на кортежах, по сути, является примитивным мультииндек -
сом, и тип MultiIndex библиотеки Pandas как раз обеспечивает необходимые нам 
операции. Создать мультииндекс из кортежей можно следующим образом:
In[5]: index = pd.MultiIndex.from_tuples(index)
 index
Out[5]: MultiIndex(levels=[['California', 'New York', 'Texas'], [2000, 2010]],
 labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]])
Обратите внимание, что MultiIndex содержит несколько уровней (levels) индекса-
ции. В данном случае названия штатов и годы, а также несколько кодирующих эти 
уровни меток (labels) для каждой точки данных.

--- СТРАНИЦА 166 ---

\1ndas 
Проиндексировав заново наши ряды данных с помощью MultiIndex, мы увидим 
иерархическое представление данных:
In[6]: pop = pop.reindex(index)
 pop
Out[6]: California 2000 33871648
 2010 37253956
 New York 2000 18976457
 2010 19378102
 Texas 2000 20851820
 2010 25145561
 dtype: int64
Здесь первые два столбца представления объекта Series отражают значения муль-
тииндекса, а третий столбец — данные. Обратите внимание, что в первом столбце 
отсутствуют некоторые элементы: в этом мультииндексном представлении все 
пропущенные элементы означают то же значение, что и строкой выше.
Теперь для выбора всех данных, второй индекс которых равен 2010, можно просто 
воспользоваться синтаксисом срезов библиотеки Pandas:
In[7]: pop[:, 2010]
Out[7]: California 37253956
 New York 19378102
 Texas 25145561
 dtype: int64
Результат представляет собой массив с одиночным индексом и только теми клю-
чами, которые нас интересуют. Такой синтаксис намного удобнее (а операция вы-
полняется гораздо быстрее!), чем мультииндексное решение на основе кортежей, 
с которого мы начали. Сейчас мы обсудим подробнее подобные операции индек-
сации над иерархически индексированными данными.
Мультииндекс как дополнительное измерение
Мы могли с легкостью хранить те же самые данные с помощью простого объекта 
DataFrame с индексом и метками столбцов. На самом деле библиотека Pandas создана 
с учетом этой равнозначности. Метод unstack() может быстро преобразовать муль-
тииндексный объект Series в индексированный обычным образом объект DataFrame:
In[8]: pop_df = pop.unstack()
 pop_df
Out[8]: 2000 2010
 California 33871648 37253956
 New York 18976457 19378102
 Texas 20851820 25145561

--- СТРАНИЦА 167 ---

\1
In[9]: pop_df.stack()
Out[9]: California 2000 33871648
 2010 37253956
 New York 2000 18976457
 2010 19378102
 Texas 2000 20851820
 2010 25145561
 dtype: int64
Почему вообще имеет смысл возиться с иерархической индексацией? Причина 
проста: аналогично тому, как мы использовали мультииндексацию для представ-
ления двумерных данных в одномерном объекте Series, можно использовать ее 
для представления данных с тремя или более измерениями в объектах Series или 
DataFrame. Каждый новый уровень в мультииндексе представляет дополнительное 
измерение данных. Благодаря использованию этого свойства мы получаем намного 
больше свободы в представлении типов данных. Например, нам может понадо -
биться добавить в демографические данные по каждому штату за каждый год еще 
один столбец (допустим, количество населения младше 18 лет). Благодаря типу 
MultiIndex это сводится к добавлению еще одного столбца в объект DataFrame:
In[10]: pop_df = pd.DataFrame({'total': pop,
 'under18': [9267089, 9284094,
 4687374, 4318033,
 5906301, 6879014]})
 pop_df
Out[10]: total under18
 California 2000 33871648 9267089
 2010 37253956 9284094
 New York 2000 18976457 4687374
 2010 19378102 4318033
 Texas 2000 20851820 5906301
 2010 25145561 6879014
Помимо этого, все универсальные функции и остальная функциональность, об -
суждавшаяся в разделе «Операции над данными в библиотеке Pandas» этой главы, 
также прекрасно работают с иерархическими индексами. В следующем фрагменте 
кода мы вычисляем по годам долю населения младше 18 лет на основе вышепри-
веденных данных:
In[11]: f_u18 = pop_df['under18'] / pop_df['total']
 f_u18.unstack()
Out[11]: 2000 2010
 California 0.273594 0.249211
 New York 0.247010 0.222831
 Texas 0.283251 0.273568

--- СТРАНИЦА 168 ---

\1ndas 

\1
In[12]: df = pd.DataFrame(np.random.rand(4, 2),
 index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],
 columns=['data1', 'data2'])
 df
Out[12]: data1 data2
 a 1 0.554233 0.356072
 2 0.925244 0.219474
 b 1 0.441759 0.610054
 2 0.171495 0.886688
Вся работа по созданию мультииндекса выполняется в фоновом режиме.
Если передать словарь с соответствующими кортежами в качестве ключей, библи-
отека Pandas автоматически распознает такой синтаксис и будет по умолчанию 
использовать мультииндекс:
In[13]: data = {('California', 2000): 33871648,
 ('California', 2010): 37253956,
 ('Texas', 2000): 20851820,
 ('Texas', 2010): 25145561,
 ('New York', 2000): 18976457,
 ('New York', 2010): 19378102}
 pd.Series(data)
Out[13]: California 2000 33871648
 2010 37253956
 New York 2000 18976457
 2010 19378102
 Texas 2000 20851820
 2010 25145561
 dtype: int64
Тем не менее иногда бывает удобно создавать объекты MultiIndex явным образом. 
Далее мы рассмотрим несколько методов для этого.
Явные конструкторы для объектов MultiIndex
При формировании индекса для большей гибкости можно воспользоваться име-
ющимися в классе pd.MultiIndex конструкторами-методами класса. Например, 
можно сформировать объект MultiIndex из простого списка массивов, задающих 
значения индекса в каждом из уровней:

--- СТРАНИЦА 169 ---
Иерархическая индексация 169
In[14]: pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]])
Out[14]: MultiIndex(levels=[['a', 'b'], [1, 2]],
 labels=[[0, 0, 1, 1], [0, 1, 0, 1]])

\1
In[15]: pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])
Out[15]: MultiIndex(levels=[['a', 'b'], [1, 2]],
 labels=[[0, 0, 1, 1], [0, 1, 0, 1]])

\1
In[16]: pd.MultiIndex.from_product([['a', 'b'], [1, 2]])
Out[16]: MultiIndex(levels=[['a', 'b'], [1, 2]],
 labels=[[0, 0, 1, 1], [0, 1, 0, 1]])
Можно сформировать объект MultiIndex непосредственно с помощью его внутрен-
него представления, передав в конструктор levels (список списков, содержащих 
имеющиеся значения индекса для каждого уровня) и labels (список списков меток):
In[17]: pd.MultiIndex(levels=[['a', 'b'], [1, 2]],
 labels=[[0, 0, 1, 1], [0, 1, 0, 1]])
Out[17]: MultiIndex(levels=[['a', 'b'], [1, 2]],
 labels=[[0, 0, 1, 1], [0, 1, 0, 1]])
Любой из этих объектов можно передать в качестве аргумента метода index при 
создании объектов Series или DataFrame или методу reindex уже существующих 
объектов Series или DataFrame.
Названия уровней мультииндексов
Иногда бывает удобно задать названия для уровней объекта MultiIndex. Сделать 
это можно, передав аргумент names любому из вышеперечисленных конструкторов 
класса MultiIndex или задав значения атрибута names постфактум:
In[18]: pop.index.names = ['state', 'year']
 pop
Out[18]: state year
 California 2000 33871648
 2010 37253956
 New York 2000 18976457
 2010 19378102
 Texas 2000 20851820
 2010 25145561
 dtype: int64

--- СТРАНИЦА 170 ---

\1ndas 

\1
In[19]:
# Иерархические индексы и столбцы
index = pd.MultiIndex.from_product([[2013, 2014], [1, 2]],
 names=['year', 'visit'])
columns = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'],
 ['HR', 'Temp']],
 names=['subject', 'type'])
# Создаем имитационные данные
data = np.round(np.random.randn(4, 6), 1)
data[:, ::2] *= 10
data += 37
# Создаем объект DataFrame
health_data = pd.DataFrame(data, index=index, columns=columns)
health_data
Out[19]: subject Bob Guido Sue
 type HR Temp HR Temp HR Temp
 year visit
 2013 1 31.0 38.7 32.0 36.7 35.0 37.2
 2 44.0 37.7 50.0 35.0 29.0 36.7
 2014 1 30.0 37.4 39.0 37.8 61.0 36.9
 2 47.0 37.8 48.0 37.3 51.0 36.5

\1
In[20]: health_data['Guido']
Out[20]: type HR Temp
 year visit
 2013 1 32.0 36.7
 2 50.0 35.0
 2014 1 39.0 37.8
 2 48.0 37.3

\1ndex спроектирован так, чтобы индексация и срезы по мул ьтиин-
дексу были интуитивно понятны, особенно если думать об индексах как о допол-
нительных измерениях. Изучим сначала индексацию мультииндекс ированного 
объекта Series, а затем мультииндексированного объекта DataFrame.

\1
In[21]: pop
Out[21]: state year
 California 2000 33871648
 2010 37253956
 New York 2000 18976457
 2010 19378102
 Texas 2000 20851820
 2010 25145561
 dtype: int64

\1
In[22]: pop['California', 2000]
Out[22]: 33871648
Объект MultiIndex поддерживает также частичную индексацию (partial indexing), 
то есть индексацию только по одному из уровней индекса. Результат — тоже объект 
Series, с более низкоуровневыми индексами:
In[23]: pop['California']
Out[23]: year
 2000 33871648
 2010 37253956
 dtype: int64

\1

--- СТРАНИЦА 172 ---

\1ndas 
In[24]: pop.loc['California':'New York']
Out[24]: state year
 California 2000 33871648
 2010 37253956
 New York 2000 18976457
 2010 19378102
 dtype: int64

\1
In[25]: pop[:, 2000]
Out[25]: state
 California 33871648
 New York 18976457
 Texas 20851820
 dtype: int64

\1
In[26]: pop[pop > 22000000]
Out[26]: state year
 California 2000 33871648
 2010 37253956
 Texas 2010 25145561
 dtype: int64

\1
In[27]: pop[['California', 'Texas']]
Out[27]: state year
 California 2000 33871648
 2010 37253956
 Texas 2000 20851820
 2010 25145561
 dtype: int64

\1
In[28]: health_data
Out[28]: subject Bob Guido Sue
 type HR Temp HR Temp HR Temp
 year visit

--- СТРАНИЦА 173 ---

\1
In[29]: health_data['Guido', 'HR']
Out[29]: year visit
 2013 1 32.0
 2 50.0
 2014 1 39.0
 2 48.0
 Name: (Guido, HR), dtype: float64

\1
In[30]: health_data.iloc[:2, :2]
Out[30]: subject Bob
 type HR Temp
 year visit
 2013 1 31.0 38.7
 2 44.0 37.7

\1
In[31]: health_data.loc[:, ('Bob', 'HR')]
Out[31]: year visit
 2013 1 31.0
 2 44.0
 2014 1 30.0
 2 47.0
 Name: (Bob, HR), dtype: float64

\1
In[32]: health_data.loc[(:, 1), (:, 'HR')]
 File "<IPython-input-32-8e3cc151e316>", line 1
 health_data.loc[(:, 1), (:, 'HR')]
 ^
SyntaxError: invalid syntax

--- СТРАНИЦА 174 ---

\1ndas 
Избежать этого можно, сформировав срез явным образом с помощью встроен -
ной функции Python slice() , но лучше в данном случае использовать объект 
IndexSlice, предназначенный библиотекой Pandas как раз для подобной ситуации. 

\1
In[33]: idx = pd.IndexSlice
 health_data.loc[idx[:, 1], idx[:, 'HR']]
Out[33]: subject Bob Guido Sue
 type HR HR HR
 year visit
 2013 1 31.0 32.0 35.0
 2014 1 30.0 39.0 61.0
Существует множество способов взаимодействия с данными в мультииндексиро-
ванных объектах Series и DataFrame, и лучший способ привыкнуть к ним — начать 
с ними экспериментировать!
Перегруппировка мультииндексов
Один из ключей к эффективной работе с мультииндексирова нными данными — 
умение эффективно преобразовывать данные. Существует немало операций, сохра-
няющих всю информацию из набора данных, но преобразующих ее ради удобства 
проведения различных вычислений. Мы рассмотрели небольшой пример этого 
с методами stack() и unstack(), но есть гораздо больше способов точного контроля 
над перегруппировкой данных между иерархическими индексами и столбцами.

\1
In[34]: index = pd.MultiIndex.from_product([['a', 'c', 'b'], [1, 2]])
 data = pd.Series(np.random.rand(6), index=index)
 data.index.names = ['char', 'int']
 data
Out[34]: char int
 a 1 0.003001
 2 0.164974
 c 1 0.741650
 2 0.569264
 b 1 0.001693
 2 0.526226
 dtype: float64

--- СТРАНИЦА 175 ---

\1
In[35]: try:
 data['a':'b']
 except KeyError as e:
 print(type(e))
 print(e)
<class 'KeyError'>
'Key length (1) was greater than MultiIndex lexsort depth (0)'
Хотя из сообщения об ошибке 1 это не вполне понятно, ошибка генерируется, по-
тому что объект MultiIndex не отсортирован. По различным причинам частичные 
срезы и другие подобные операции требуют, чтобы уровни мультииндекса были 
отсортированы (лексикографически упорядочены). Библиотека Pandas предостав -
ляет множество удобных инструментов для выполнения подобной сортировки. 
В качестве примеров можем указать методы sort_index() и sortlevel() объекта 
DataFrame. Мы воспользуемся простейшим из них — методом sort_index():
In[36]: data = data.sort_index()
 data
Out[36]: char int
 a 1 0.003001
 2 0.164974
 b 1 0.001693
 2 0.526226
 c 1 0.741650
 2 0.569264
 dtype: float64

\1
In[37]: data['a':'b']
Out[37]: char int
 a 1 0.003001
 2 0.164974
 b 1 0.001693
 2 0.526226
 dtype: float64
Выполнение над индексами операций stack и unstack

\1
1 «Длина ключа была больше, чем глубина лексикографической сортировки объекта Multi -
Index».

--- СТРАНИЦА 176 ---

\1ndas 
In[38]: pop.unstack(level=0)
Out[38]: state California New York Texas
 year
 2000 33871648 18976457 20851820
 2010 37253956 19378102 25145561
In[39]: pop.unstack(level=1)
Out[39]: year 2000 2010
 state
 California 33871648 37253956
 New York 18976457 19378102
 Texas 20851820 25145561
Методу unstack() противоположен по действию метод stack(), которым можно 
воспользоваться, чтобы получить обратно исходный ряд данных:
In[40]: pop.unstack().stack()
Out[40]: state year
 California 2000 33871648
 2010 37253956
 New York 2000 18976457
 2010 19378102
 Texas 2000 20851820
 2010 25145561
 dtype: int64
Создание и перестройка индексов
Еще один способ перегруппировки иерархических данных — преобразовать мет -
ки индекса в столбцы с помощью метода reset_index. Результатом вызова этого 
метода для нашего ассоциативного словаря населения будет объект DataFrame со 
столбцами state (штат) и year (год), содержащими информацию, ранее находи-
вшуюся в индексе. Для большей ясности можно при желании задать название для 
представленных в виде столбцов данных:
In[41]: pop_flat = pop.reset_index(name='population')
 pop_flat
Out[41]: state year population
 0 California 2000 33871648
 1 California 2010 37253956
 2 New York 2000 18976457
 3 New York 2010 19378102
 4 Texas 2000 20851820
 5 Texas 2010 25145561
При работе с реальными данными исходные входные данные очень часто выгля-
дят подобным образом, поэтому удобно создать объект MultiIndex из значений 

--- СТРАНИЦА 177 ---
Иерархическая индексация 177
столбцов. Это можно сделать с помощью метода set_index объекта DataFrame, воз -
вращающего мультииндексированный объект DataFrame:
In[42]: pop_flat.set_index(['state', 'year'])
Out[42]: population
 state year
 California 2000 33871648
 2010 37253956
 New York 2000 18976457
 2010 19378102
 Texas 2000 20851820
 2010 25145561
На практике, я полагаю, этот тип перестройки индекса — один из самых удобных 
паттернов при работе с реальными наборами данных.
Агрегирование по мультииндексам
В библиотеке Pandas имеются встроенные методы для агрегирования данных, на-
пример mean(), sum() и max(). В случае иерархически индексированных данных им 
можно передать параметр level для указания подмножества данных, на котором 
будет вычисляться сводный показатель.

\1
In[43]: health_data
Out[43]: subject Bob Guido Sue
 type HR Temp HR Temp HR Temp
 year visit
 2013 1 31.0 38.7 32.0 36.7 35.0 37.2
 2 44.0 37.7 50.0 35.0 29.0 36.7
 2014 1 30.0 37.4 39.0 37.8 61.0 36.9
 2 47.0 37.8 48.0 37.3 51.0 36.5

\1
In[44]: data_mean = health_data.mean(level='year')
 data_mean
Out[44]: subject Bob Guido Sue
 type HR Temp HR Temp HR Temp
 year
 2013 37.5 38.2 41.0 35.85 32.0 36.95
 2014 38.5 37.6 43.5 37.55 56.0 36.70

--- СТРАНИЦА 178 ---

\1ndas 

\1
In[45]: data_mean.mean(axis=1, level='type')
Out[45]: type HR Temp
 year
 2013 36.833333 37.000000
 2014 46.000000 37.283333
Так, всего двумя строками кода мы смогли найти средний пульс и температуру по 
всем субъектам и визитам за каждый год. Такой синтаксис представляет собой со-
кращенную форму функциональности GroupBy, о которой мы поговорим в разделе 
«Агрегирование и группировка» главы 3. Хотя наш пример — всего лишь модель, 
структура многих реальных наборов данных аналогичным образом иерархична.
Данные объектов Panel
В библиотеке Pandas есть еще несколько пока не охваченных нами структур данных, 
а именно объекты pd.Panel и pd.Panel4D. Их можно рассматривать как соответ -
ственно трехмерное и четырехмерное обобщение (одномерной структуры) объекта 
Series и (двумерной структуры) объекта DataFrame. Раз вы уже знакомы с индекса-
цией данных в объектах Series и DataFrame и манипуляциями над ними, то исполь-
зование объектов Panel и Panel4D не должно вызвать у вас затруднений. В частности, 
возможности индексаторов loc, iloc и ix, обсуждавшихся в разделе «Индексация 
и выборка данных» текущей главы, с легкостью распространяются на эти структуры 
более высоких размерностей.
Мы не будем рассматривать многомерные структуры дал ее в данном тексте, по -
скольку, как я обнаружил, в большинстве случаев мультииндексация — удобное 
и концептуально простое представление для данных более высоких размерностей. 
Помимо этого, многомерные данные по существу — плотное представление данных, 
в то время как мультииндексация — разреженное представление данных. По мере 
увеличения размерности плотное представление становится все менее эффектив -
ным для большинства реальных наборов данных. В некоторых специализированных 
приложениях, однако, эти структуры данных могут быть полезны. Если вы захотите 
узнать больше о структурах Panel и Panel4D, загляните в ссылки, приведенные в раз-
деле «Дополнительные источники информации» данной главы.
Объединение наборов данных: конкатенация 
и добавление в конец
Некоторые наиболее интересные исследования выполняются благодаря объеди -
нению различных источников данных. Эти операции могут включать в себя что 
угодно, начиная с простейшей конкатенации двух различных наборов данных до 

--- СТРАНИЦА 179 ---
Объединение наборов данных: конкатенация и добавление в конец 179
более сложных соединений и слияний в стиле баз данных, корректно обрабатыва-
ющих все возможные частичные совпадения наборов. Объекты Series и DataFrame 
созданы в расчете на подобные операции, и библиотека Pandas содержит функции 
и методы для быстрого и удобного выполнения таких манипуляций.
Мы рассмотрим простую конкатенацию объектов Series и DataFrame с помощью 
функции pd.concat, углубимся в реализованные в библиотеке Pandas более запу-
танные слияния и соединения, выполняемые в оперативной памяти.

\1
In[1]: import pandas as pd
 import numpy as np

\1
In[2]: def make_df(cols, ind):
 """Быстро создаем объект DataFrame"""
 data = {c: [str(c) + str(i) for i in ind]
 for c in cols}
 return pd.DataFrame(data, ind)
 # Экземпляр DataFrame
 make_df('ABC', range(3))
Out[2]: A B C
 0 A0 B0 C0
 1 A1 B1 C1
 2 A2 B2 C2
Напоминание: конкатенация массивов NumPy
Конкатенация объектов Series и DataFrame очень похожа на конкатенацию мас -
сивов библиотеки NumPy, которую можно осуществить посредством функции 
np.concatenate , обсуждавшейся в разделе «Введение в массивы библиотеки 
NumPy» главы 2. Напомним, что таким образом можно объединять содержимое 
двух или более массивов в один:
In[4]: x = [1, 2, 3]
 y = [4, 5, 6]
 z = [7, 8, 9]
 np.concatenate([x, y, z])
Out[4]: array([1, 2, 3, 4, 5, 6, 7, 8, 9])

\1

--- СТРАНИЦА 180 ---

\1ndas 
In[5]: x = [[1, 2],
 [3, 4]]
 np.concatenate([x, x], axis=1)
Out[5]: array([[1, 2, 1, 2],
 [3, 4, 3, 4]])
Простая конкатенация с помощью метода pd.concat
В библиотеке Pandas имеется функция, pd.concat(), синтаксис которой аналогичен 
функции np.concatenate, однако она содержит множество параметров, которые мы 
вскоре обсудим:
# Сигнатура функции pd.concat в библиотеке Pandas v0.18
pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,
 keys=None, levels=None, names=None, verify_integrity=False,
 copy=True)
Функцию pd.concat можно использовать для простой конкатенации объектов 
Series или DataFrame аналогично тому, как функцию np.concatenate() можно 
применять для простой конкатенации массивов:
In[6]: ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])
 ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])
 pd.concat([ser1, ser2])
Out[6]: 1 A
 2 B
 3 C
 4 D
 5 E
 6 F
 dtype: object

\1
In[7]: df1 = make_df('AB', [1, 2])
 df2 = make_df('AB', [3, 4])
 print(df1); print(df2); print(pd.concat([df1, df2]))
df1 df2 pd.concat([df1, df2])
 A B A B A B
 1 A1 B1 3 A3 B3 1 A1 B1
 2 A2 B2 4 A4 B4 2 A2 B2
 3 A3 B3
 4 A4 B4
По умолчанию конкатенация происходит в объекте DataFrame построчно, то есть 
axis=0. Аналогично функции np.concatenate() функция pd.concat() позволяет 
указывать ось, по которой будет выполняться конкатенация. Рассмотрим следу -
ющий пример:

--- СТРАНИЦА 181 ---
Объединение наборов данных: конкатенация и добавление в конец 181
In[8]: df3 = make_df('AB', [0, 1])
 df4 = make_df('CD', [0, 1])
 print(df3); print(df4); print(pd.concat([df3, df4], axis='col'))
df3 df4 pd.concat([df3, df4], axis='col')
 A B C D A B C D
 0 A0 B0 0 C0 D0 0 A0 B0 C0 D0
 1 A1 B1 1 C1 D1 1 A1 B1 C1 D1
Мы могли задать ось и эквивалентным образом: axis=1, здесь же мы использовали 
более интуитивно понятный вариант axis='col'1 .
Дублирование индексов
Важное различие между функциями np.concatenate() и pd.concat() состоит 
в том, что конкатенация из библиотеки Pandas сохраняет индексы , даже если 
в результате некоторые индексы будут дублироваться. Рассмотрим следующий 
пример:
In[9]: x = make_df('AB', [0, 1])
 y = make_df('AB', [2, 3])
 y.index = x.index # Дублируем индексы!
 print(x); print(y); print(pd.concat([x, y]))
x y pd.concat([x, y])
 A B A B A B
 0 A0 B0 0 A2 B2 0 A0 B0
 1 A1 B1 1 A3 B3 1 A1 B1
 0 A2 B2
 1 A3 B3
Обратите внимание на повторяющиеся индексы. Хотя в объектах DataFrame это до-
пустимо, подобный результат часто может быть нежелателен. Функция pd.concat() 
предоставляет нам несколько способов решения этой проблемы.
Перехват повторов как ошибки. Если вам нужно просто гарантировать, что инде-
ксы в возвращаемом функцией pd.concat() результате не перекрываются, можно 
задать флаг verify_integrity. В случае равного True значения этого флага конка-
тенация приведет к генерации ошибки при наличии дублирующихся индексов. Вот 
пример, в котором мы для большей ясности перехватываем и выводим в консоль 
сообщение об ошибке:

\1ndas допустимы следующие варианты синта-
ксиса:
 axis='columns'
 или
 axis=1
 Указанный вариант синтаксиса приведет к ошибке. Впрочем, в в ерсии 0.18.1 библиотеки 
Pandas, используемой автором книги, документированный синтаксис для этой функции 
допускает только применение числовых значений для параметра axis.

--- СТРАНИЦА 182 ---

\1ndas 
In[10]: try:
 pd.concat([x, y], verify_integrity=True)
 except ValueError as e:
 print("ValueError:", e)
ValueError: Indexes have overlapping values: [0, 1]
Игнорирование индекса. Иногда индекс сам по себе не имеет значения и лучше 
его просто проигнорировать. Для этого достаточно установить флаг ignore_index. 

\1
In[11]: print(x); print(y); print(pd.concat([x, y], ignore_index=True))
x y pd.concat([x, y], ignore_index=True)
 A B A B A B
 0 A0 B0 0 A2 B2 0 A0 B0
 1 A1 B1 1 A3 B3 1 A1 B1
 2 A2 B2
 3 A3 B3

\1
In[12]: print(x); print(y); print(pd.concat([x, y], keys=['x', 'y']))
x y pd.concat([x, y], keys=['x', 'y'])
 A B A B A B
 0 A0 B0 0 A2 B2 x 0 A0 B0
 1 A1 B1 1 A3 B3 1 A1 B1
 y 0 A2 B2
 1 A3 B3
Результат представляет собой мультииндексированный объект DataFrame, так что 
мы сможем воспользоваться описанным в разделе «Иерархическая индексация» 
этой главы, чтобы преобразовать эти данные в требуемое нам представление.
Конкатенация с использованием соединений
В рассматриваемых примерах в основном производится конкатенация объектов 
DataFrame с общими названиями столбцов. На практике у данных из разных ис -
точников могут быть различные наборы имен столбцов. На этот случай у функции 
pd.concat() имеется несколько опций. Изучим объединение следующих двух 
объектов DataFrame, у которых столбцы (но не все!) называются одинаково:
In[13]: df5 = make_df('ABC', [1, 2])
 df6 = make_df('BCD', [3, 4])
 print(df5); print(df6); print(pd.concat([df5, df6]))

--- СТРАНИЦА 183 ---
Объединение наборов данных: конкатенация и добавление в конец 183
df5 df6 pd.concat([df5, df6])
 A B C B C D A B C D
 1 A1 B1 C1 3 B3 C3 D3 1 A1 B1 C1 NaN
 2 A2 B2 C2 4 B4 C4 D4 2 A2 B2 C2 NaN
 3 NaN B3 C3 D3
 4 NaN B4 C4 D4
По умолчанию элементы, данные для которых отсутствуют, заполняются NA -
значе ниями. Чтобы поменять это поведение, можно указать одну из нескольких 
опций для параметров join и join_axes функции конкатенации. По умолча -
нию соединение — объединение входных столбцов ( join='outer'), но есть воз -
можность поменять это поведение на пересечение столбцов с помощью опции 
join='inner':
In[14]: print(df5); print(df6);
 print(pd.concat([df5, df6], join='inner'))
df5 df6 pd.concat([df5, df6], join='inner')
 A B C B C D B C
 1 A1 B1 C1 3 B3 C3 D3 1 B1 C1
 2 A2 B2 C2 4 B4 C4 D4 2 B2 C2
 3 B3 C3
 4 B4 C4
Еще одна опция предназначена для указания явным образом индекса оставшихся 
столбцов с помощью аргумента join_axes, которому присваивается список объек -
тов индекса. В данном случае мы указываем, что возвращаемые столбцы должны 
совпадать со столбцами первого из конкатенируемых объектов DataFrame:
In[15]: print(df5); print(df6);
 print(pd.concat([df5, df6], join_axes=[df5.columns]))
df5 df6 pd.concat([df5, df6], join_axes=[df5.columns])
 A B C B C D A B C
 1 A1 B1 C1 3 B3 C3 D3 1 A1 B1 C1
 2 A2 B2 C2 4 B4 C4 D4 2 A2 B2 C2
 3 NaN B3 C3
 4 NaN B4 C4
Различные сочетания опций функции pd.concat() обеспечивают широкий диа -
пазон возможных поведений при соединении двух наборов данных. Не забывайте 
об этом при использовании ее для ваших собственных данных.
Метод append()
Непосредственная конкатенация массивов настолько распространена, что в объ -
екты Series и DataFrame был включен метод append(), позволяющий выполнить то 
же самое с меньшими усилиями. Например, вместо вызова pd.concat([df1, df2]) 
можно вызвать df1.append(df2):

--- СТРАНИЦА 184 ---

\1ndas 
In[16]: print(df1); print(df2); print(df1.append(df2))
df1 df2 df1.append(df2)
 A B A B A B
 1 A1 B1 3 A3 B3 1 A1 B1
 2 A2 B2 4 A4 B4 2 A2 B2
 3 A3 B3
 4 A4 B4
Не забывайте, что, в отличие от методов append() и extend() списков языка 
Python, метод append() в библиотеке Pandas не изменяет исходный объект. Вместо 
этого он создает новый объект с объединенными данными, что делает этот метод 
не слишком эффективным, поскольку означает создание нового индекса и буфера 
данных. Следовательно, если вам необходимо выполнить несколько операций 
append, лучше создать список объектов DataFrame и передать их все сразу функции 
concat().
В следующем разделе мы рассмотрим другой подход к о бъединению данных из 
нескольких источников, обладающий еще более широкими возможностями: вы -
полнение слияний/объединений в стиле баз данных с помощью функции pd.merge. 
Для получения дополнительной информации о методах concat(), append() и от -
носящейся к ним функциональности см. раздел Merge, Join and Concatenate («Сли -
яние, соединение и конкатенация», http://pandas.pydata.org/pandas-docs/stable/merging.
html) документации библиотеки Pandas.
Объединение наборов данных: слияние 
и соединение
Одно из важных свойств библиотеки Pandas — ее высокопроизводительные, вы -
полняемые в оперативной памяти операции соединения и слияния. Если вы когда-
либо работали с базами данных, вам должен быть знаком такой вид взаимодействия 
с данными. Основной интерфейс для них — функция pd.merge. Несколько приме-
ров ее работы на практике мы рассмотрим далее.
Реляционная алгебра
Реализованное в методе pd.merge поведение представляет собой подмножество 
того, что известно под названием «реляционная алгебра» (relational algebra). Ре -
ляционная алгебра — формальный набор правил манипуляции реляционными 
данными, формирующий теоретические основания для имеющихся в большинстве 
баз данных операций. Сила реляционного подхода состоит в предоставлении им 
нескольких простейших операций — своеобразных «кирпичиков» для построения 
более сложных операций над любым набором данных. При наличии эффективно 

--- СТРАНИЦА 185 ---
Объединение наборов данных: слияние и соединение 185
реализованного в базе данных или другой программе подобного базового набора 
операций можно выполнять широкий диапазон весьма сложных составных опе -
раций.
Библиотека Pandas реализует несколько из этих базовых «кирпичиков» в функции 
pd.merge() и родственном ей методе join() объектов Series и DataFrame. Они обе-
спечивают возможность эффективно связывать данные из различных источников.

\1
In[2]:
df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],
 'group': ['Accounting', 'Engineering', 'Engineering',
 'HR']})
df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],
 'hire_date': [2004, 2008, 2012, 2014]})
print(df1); print(df2)
df1 df2
 employee group employee hire_date
0 Bob Accounting 0 Lisa 2004
1 Jake Engineering 1 Bob 2008
2 Lisa Engineering 2 Jake 2012
3 Sue HR 3 Sue 2014

\1
In[3]: df3 = pd.merge(df1, df2)
 df3
Out[3]: employee group hire_date
 0 Bob Accounting 2008

--- СТРАНИЦА 186 ---

\1ndas 
 1 Jake Engineering 2012
 2 Lisa Engineering 2004
 3 Sue HR 2014
Функция pd.merge() распознает, что в обоих объектах DataFrame имеется столбец 
employee, и автоматически выполняет соединение, используя этот столбец в качестве 
ключа. Результатом слияния становится новый объект DataFrame, объединяющий 
информацию из двух входных объектов. Обратите внимание, что порядок записей 
в столбцах не обязательно сохраняется: в данном случае сортировка столбца employee 
различна в объектах df1 и df2 и функция pd.merge() обрабатывает эту ситуацию 
корректным образом. Кроме того, не забывайте, что слияние игнорирует индекс, 
за исключением особого случая слияния по индексу (см. пункт «Ключевые слова 
left_index и right_index» подраздела «Задание ключа слияния» данного раздела).

\1
In[4]: df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],
 'supervisor': ['Carly', 'Guido', 'Steve']})
 print(df3); print(df4); print(pd.merge(df3, df4))
df3 df4
 employee group hire_date group supervisor
0 Bob Accounting 2008 0 Accounting Carly
1 Jake Engineering 2012 1 Engineering Guido
2 Lisa Engineering 2004 2 HR Steve
3 Sue HR 2014
pd.merge(df3, df4)
 employee group hire_date supervisor
0 Bob Accounting 2008 Carly
1 Jake Engineering 2012 Guido
2 Lisa Engineering 2004 Guido
3 Sue HR 2014 Steve

\1
In[5]: df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',
 'Engineering', 'Engineering', 
 'HR', 'HR'],
 'skills': ['math', 'spreadsheets', 'coding',
 'linux',
 'spreadsheets', 'organization']})
print(df1); print(df5); print(pd.merge(df1, df5))
df1 df5
 employee group group skills
0 Bob Accounting 0 Accounting math
1 Jake Engineering 1 Accounting spreadsheets
2 Lisa Engineering 2 Engineering coding
3 Sue HR 3 Engineering linux
 4 HR spreadsheets
 5 HR organization
pd.merge(df1, df5)
 employee group skills
0 Bob Accounting math
1 Bob Accounting spreadsheets
2 Jake Engineering coding
3 Jake Engineering linux
4 Lisa Engineering coding
5 Lisa Engineering linux
6 Sue HR spreadsheets
7 Sue HR organization
Эти три типа соединений можно использовать и в других инструментах библиотеки 
Pandas, что дает возможность реализовать широкий диапазон функциональности. 
Однако на практике наборы данных редко оказываются такими же «чистыми», как 
те, с которыми мы имели дело. В следующем разделе мы рассмотрим параметры 
метода pd.merge() , позволяющие более тонко описывать желаемое поведе ние 
операций соединения.
Задание ключа слияния
Мы рассмотрели поведение метода pd.merge() по умолчанию: он выполняет поиск 
в двух входных объектах соответствующих названий столбцов и использует най-
денное в качестве ключа. Однако зачастую имена столбцов не совпадают добуквен-
но точно, и в методе pd.merge() имеется немало параметров для такой ситуации.

--- СТРАНИЦА 188 ---

\1ndas 
Ключевое слово on
Проще всего указать название ключевого столбца с помощью ключевого слова on, 
в котором указывается название или список названий столбцов:
In[6]: print(df1); print(df2); print(pd.merge(df1, df2, on='employee'))
df1 df2
 employee group employee hire_date
0 Bob Accounting 0 Lisa 2004
1 Jake Engineering 1 Bob 2008
2 Lisa Engineering 2 Jake 2012
3 Sue HR 3 Sue 2014
pd.merge(df1, df2, on='employee')
 employee group hire_date
0 Bob Accounting 2008
1 Jake Engineering 2012
2 Lisa Engineering 2004
3 Sue HR 2014
Этот параметр работает только в том случае, когда в левом и правом объектах 
DataFrame имеется указанное название столбца.
Ключевые слова left_on и right_on
Иногда приходится выполнять слияние двух наборов данных с различными именами 
столбцов. Например, у нас может быть набор данных, в котором столбец для имени 
служащего называется Name, а не Employee. В этом случае можно воспользоваться клю-
чевыми словами left_on и right_on для указания названий двух нужных столбцов:
In[7]:
df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
 'salary': [70000, 80000, 120000, 90000]})
print(df1); print(df3);
print(pd.merge(df1, df3, left_on="employee", right_on="name"))
df1 df3
 employee group name salary
0 Bob Accounting 0 Bob 70000
1 Jake Engineering 1 Jake 80000
2 Lisa Engineering 2 Lisa 120000
3 Sue HR 3 Sue 90000
pd.merge(df1, df3, left_on="employee", right_on="name")
 employee group name salary
0 Bob Accounting Bob 70000
1 Jake Engineering Jake 80000
2 Lisa Engineering Lisa 120000
3 Sue HR Sue 90000

--- СТРАНИЦА 189 ---

\1
In[8]:
pd.merge(df1, df3, left_on="employee", right_on="name").drop('name', axis=1)
Out[8]: employee group salary
 0 Bob Accounting 70000
 1 Jake Engineering 80000
 2 Lisa Engineering 120000
 3 Sue HR 90000
Ключевые слова left_index и right_index

\1
In[9]: df1a = df1.set_index('employee')
 df2a = df2.set_index('employee')
 print(df1a); print(df2a)
df1a df2a
 group hire_date
employee employee
Bob Accounting Lisa 2004
Jake Engineering Bob 2008
Lisa Engineering Jake 2012
Sue HR Sue 2014
Можно использовать индекс в качестве ключа слияния путем указания в методе 
pd.merge() флагов left_index и/или right_index:
In[10]:
print(df1a); print(df2a);
print(pd.merge(df1a, df2a, left_index=True, right_index=True))
df1a df2a
 group hire_date
employee employee
Bob Accounting Lisa 2004
Jake Engineering Bob 2008
Lisa Engineering Jake 2012
Sue HR Sue 2014
pd.merge(df1a, df2a, left_index=True, right_index=True)
 group hire_date
employee
Lisa Engineering 2004
Bob Accounting 2008
Jake Engineering 2012
Sue HR 2014

--- СТРАНИЦА 190 ---

\1ndas 
Для удобства в объектах DataFrame реализован метод join(), выполняющий по 
умолчанию слияние по индексам:
In[11]: print(df1a); print(df2a); print(df1a.join(df2a))
df1a df2a
 group hire_date
employee employee
Bob Accounting Lisa 2004
Jake Engineering Bob 2008
Lisa Engineering Jake 2012
Sue HR Sue 2014
df1a.join(df2a)
 group hire_date
employee
Bob Accounting 2008
Jake Engineering 2012
Lisa Engineering 2004
Sue HR 2014
Если требуется комбинация слияния по столбцам и индексам, можно для дости-
жения нужного поведения воспользоваться сочетанием флага left_index с пара-
метром right_on или параметра left_on с флагом right_index:
In[12]:
print(df1a); print(df3);
print(pd.merge(df1a, df3, left_index=True, right_on='name'))
df1a df3
 group
employee name salary
Bob Accounting 0 Bob 70000
Jake Engineering 1 Jake 80000
Lisa Engineering 2 Lisa 120000
Sue HR 3 Sue 90000
pd.merge(df1a, df3, left_index=True, right_on='name')
 group name salary
0 Accounting Bob 70000
1 Engineering Jake 80000
2 Engineering Lisa 120000
3 HR Sue 90000
Все эти параметры работают и в случае нескольких индексов и/или столбцов, 
синтаксис для этого интуитивно понятен. Более подробную информацию по 
этому вопросу см. в разделе Merge, Join and Concatenate («Слияние, соединение 
и конкатенация», http://pandas.pydata.org/pandas-docs/stable/merging.html) документации 
библиотеки Pandas.

--- СТРАНИЦА 191 ---

\1
In[13]: df6 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],
 'food': ['fish', 'beans', 'bread']},
 columns=['name', 'food'])
 df7 = pd.DataFrame({'name': ['Mary', 'Joseph'],
 'drink': ['wine', 'beer']},
 columns=['name', 'drink'])
 print(df6); print(df7); print(pd.merge(df6, df7))
df6 df7 pd.merge(df6, df7)
 name food name drink name food drink
0 Peter fish 0 Mary wine 0 Mary bread wine
1 Paul beans 1 Joseph beer
2 Mary bread
Здесь мы слили воедино два набора данных, у которых совпадает только одна за-
пись name: Mary. По умолчанию результат будет содержать пересечение двух вход-
ных множеств — внутреннее соединение (inner join). Можно указать это явным об -
разом, с помощью ключевого слова how, имеющего по умолчанию значение 'inner':
In[14]: pd.merge(df6, df7, how='inner')
Out[14]: name food drink
 0 Mary bread wine
Другие возможные значения ключевого слова how: 'outer', 'left' и 'right'. Внеш-
нее соединение (outer join) означает соединение по объединению входных столбцов 
и заполняет значениями NA все пропуски значений:
In[15]: print(df6); print(df7); print(pd.merge(df6, df7, how='outer'))
df6 df7 pd.merge(df6, df7, how='outer')
 name food name drink name food drink
0 Peter fish 0 Mary wine 0 Peter fish NaN
1 Paul beans 1 Joseph beer 1 Paul beans NaN
2 Mary bread 2 Mary bread wine
 3 Joseph NaN beer
Левое соединение (left join) и правое соединение (right join) выполняют соединение 
по записям слева и справа соответственно. Например:
In[16]: print(df6); print(df7); print(pd.merge(df6, df7, how='left'))
df6 df7 pd.merge(df6, df7, how='left')
 name food name drink name food drink

--- СТРАНИЦА 192 ---

\1ndas 
0 Peter fish 0 Mary wine 0 Peter fish NaN
1 Paul beans 1 Joseph beer 1 Paul beans NaN
2 Mary bread 2 Mary bread wine

\1
In[17]: df8 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
 'rank': [1, 2, 3, 4]})
 df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
 'rank': [3, 1, 4, 2]})
 print(df8); print(df9); print(pd.merge(df8, df9, on="name"))
df8 df9 pd.merge(df8, df9, on="name")
 name rank name rank name rank_x rank_y
0 Bob 1 0 Bob 3 0 Bob 1 3
1 Jake 2 1 Jake 1 1 Jake 2 1
2 Lisa 3 2 Lisa 4 2 Lisa 3 4
3 Sue 4 3 Sue 2 3 Sue 4 2

\1
In[18]:
print(df8); print(df9);
print(pd.merge(df8, df9, on="name", suffixes=["_L", "_R"]))
df8 df9
 name rank name rank
0 Bob 1 0 Bob 3
1 Jake 2 1 Jake 1
2 Lisa 3 2 Lisa 4
3 Sue 4 3 Sue 2
pd.merge(df8, df9, on="name", suffixes=["_L", "_R"])
 name rank_L rank_R
0 Bob 1 3

--- СТРАНИЦА 193 ---
Объединение наборов данных: слияние и соединение 193
1 Jake 2 1
2 Lisa 3 4
3 Sue 4 2
Эти суффиксы будут работать для всех возможных вариантов соединений, в том 
числе и в случае нескольких пересекающихся по названию столбцов.
За более подробной информацией об этих вариантах загляните в раздел «Агреги-
рование и группировка» данной главы, в котором мы подробнее изучим реляцион-
ную алгебру, а также в раздел Merge, Join and Concatenate («Слияние, соединение 
и конкатенация», http://pandas.pydata.org/pandas-docs/stable/merging.html) документации 
библиотеки Pandas.

\1
In[19]:
# Инструкции системного командного процессора для скачивания данных:
# !curl -O https://raw.githubusercontent.com/jakevdp/
# data-USstates/master/state-population.csv
# !curl -O https://raw.githubusercontent.com/jakevdp/
# data-USstates/master/state-areas.csv
# !curl -O https://raw.githubusercontent.com/jakevdp/
# data-USstates/master/state-abbrevs.csv
Посмотрим на эти наборы данных с помощью функции read_csv() библиотеки 
Pandas:
In[20]: pop = pd.read_csv('state-population.csv')
 areas = pd.read_csv('state-areas.csv')
 abbrevs = pd.read_csv('state-abbrevs.csv')
 print(pop.head()); print(areas.head()); print(abbrevs.head())
pop.head() areas.head()
 state/region ages year population state area (sq. mi)
0 AL under18 2012 1117489.0 0 Alabama 52423
1 AL total 2012 4817528.0 1 Alaska 656425
2 AL under18 2010 1130966.0 2 Arizona 114006
3 AL total 2010 4785570.0 3 Arkansas 53182
4 AL under18 2011 1125763.0 3 Arkansas 53182
 4 California 163707
abbrevs.head()
 state abbreviation
0 Alabama AL

--- СТРАНИЦА 194 ---

\1ndas 
1 Alaska AK
2 Arizona AZ
3 Arkansas AR
4 California CA1
Допустим, нам нужно на основе этой информации отсортировать штаты и терри-
торию США по плотности населения в 2010 году. Информации для этого у нас 
достаточно, но для достижения цели придется объединить наборы данных.
Начнем со слияния «многие-ко-многим», которое позволит получить полное имя 
штата в объекте DataFrame для населения. Выполнить слияние нужно на основе 
столбца state/region объекта pop и столбца abbreviation объекта abbrevs. Мы вос-
пользуемся опцией how='outer', чтобы гарантировать, что не упустим никаких 
данных из-за несовпадения меток.
In[21]: merged = pd.merge(pop, abbrevs, how='outer',
 left_on='state/region', right_on='abbreviation')
 merged = merged.drop('abbreviation', 1) # Удаляем дублирующуюся
 # информацию
 merged.head()
Out[21]: state/region ages year population state
 0 AL under18 2012 1117489.0 Alabama
 1 AL total 2012 4817528.0 Alabama
 2 AL under18 2010 1130966.0 Alabama
 3 AL total 2010 4785570.0 Alabama
 4 AL under18 2011 1125763.0 Alabama

\1
In[22]: merged.isnull().any()
Out[22]: state/region False
 ages False
 year False
 population True
 state True
 dtype: bool

\1
In[23]: merged[merged['population'].isnull()].head()
Out[23]: state/region ages year population state
 2448 PR under18 1990 NaN NaN
 2449 PR total 1990 NaN NaN
 2450 PR total 1991 NaN NaN
 2451 PR under18 1991 NaN NaN
 2452 PR total 1993 NaN NaN

\1n строк набора данных. По умолчанию n = 5.

--- СТРАНИЦА 195 ---

\1
In[24]: merged.loc[merged['state'].isnull(), 'state/region'].unique()
Out[24]: array(['PR', 'USA'], dtype=object)

\1
In[25]: merged.loc[merged['state/region'] == 'PR', 'state'] = 'Puerto Rico'
 merged.loc[merged['state/region'] == 'USA', 'state'] = 'United States'
 merged.isnull().any()
Out[25]: state/region False
 ages False
 year False
 population True
 state False
 dtype: bool

\1
In[26]: final = pd.merge(merged, areas, on='state', how='left')
 final.head()
Out[26]: state/region ages year population state area (sq. mi)
 0 AL under18 2012 1117489.0 Alabama 52423.0
 1 AL total 2012 4817528.0 Alabama 52423.0
 2 AL under18 2010 1130966.0 Alabama 52423.0
 3 AL total 2010 4785570.0 Alabama 52423.0
 4 AL under18 2011 1125763.0 Alabama 52423.0

\1
In[27]: final.isnull().any()
Out[27]: state/region False
 ages False
 year False

--- СТРАНИЦА 196 ---

\1ndas 
 population True
 state False
 area (sq. mi) True
 dtype: bool

\1
In[28]: final['state'][final['area (sq. mi)'].isnull()].unique()
Out[28]: array(['United States'], dtype=object)

\1
In[29]: final.dropna(inplace=True)
 final.head()
Out[29]: state/region ages year population state area (sq. mi)
 0 AL under18 2012 1117489.0 Alabama 52423.0
 1 AL total 2012 4817528.0 Alabama 52423.0
 2 AL under18 2010 1130966.0 Alabama 52423.0
 3 AL total 2010 4785570.0 Alabama 52423.0
 4 AL under18 2011 1125763.0 Alabama 52423.0
Теперь у нас есть все необходимые нам данные. Чтобы ответить на интересующий 
вопрос, сначала выберем часть данных, соответствующих 2010 году и всему на -
селению. Воспользуемся функцией query() (для этого должен быть установлен 
пакет numexpr, см. раздел «Увеличение производительности библиотеки Pandas: 
eval() и query()» данной главы):
In[30]: data2010 = final.query("year == 2010 & ages == 'total'")
 data2010.head()
Out[30]: state/region ages year population state area (sq. mi)
 3 AL total 2010 4785570.0 Alabama 52423.0
 91 AK total 2010 713868.0 Alaska 656425.0
 101 AZ total 2010 6408790.0 Arizona 114006.0
 189 AR total 2010 2922280.0 Arkansas 53182.0
 197 CA total 2010 37333601.0 California 163707.0

\1
In[31]: data2010.set_index('state', inplace=True)
 density = data2010['population'] / data2010['area (sq. mi)']

--- СТРАНИЦА 197 ---
Агрегирование и группировка 197
In[32]: density.sort_values(ascending=False, inplace=True)
 density.head()
Out[32]: state
 District of Columbia 8898.897059
 Puerto Rico 1058.665149
 New Jersey 1009.253268
 Rhode Island 681.339159
 Connecticut 645.600649
 dtype: float64

\1
In[33]: density.tail()
Out[33]: state
 South Dakota 10.583512
 North Dakota 9.537565
 Montana 6.736171
 Wyoming 5.768079
 Alaska 1.087509
 dtype: float64
Как видим, штатом с наименьшей плотностью населения, причем с больш им от -
рывом от остальных, оказалась Аляска, насчитывающая в среднем одного жителя 
на квадратную милю.
Подобное громоздкое слияние данных — распространенная задача при ответе на 
вопросы, связанные с реальными источниками данных. Надеюсь, что этот пример 
дал вам представление, какими способами можно комбинировать вышеописанные 
инструменты, чтобы почерпнуть полезную информацию из данных!
Агрегирование и группировка
Важная часть анализа больших данных — их эффективное обобщение: вычисление 
сводных показателей, например sum(), mean(), median(), min() и max(), в которых 
одно число позволяет понять природу, возможно, огромного набора данных. В этом 
разделе мы займемся изучением сводных показателей в библиотеке Pandas, на -
чиная с простых операций, подобных тем, с которыми мы уже имели дело при 
работе с массивами NumPy, и заканчивая более сложными операциями на основе 
понятия groupby.

--- СТРАНИЦА 198 ---

\1ndas 
Данные о планетах
Воспользуемся набором данных «Планеты» (Planets), доступным через пакет 
Seaborn (см. раздел «Визуализация с помощью библиотеки Seaborn» главы 4). 
Он включает информацию об открытых астрономами планетах, вращающихся во-
круг других звезд, известных под названием внесолнечных планет или экзопланет 
(exoplanets). Скачать его можно с помощью команды пакета Seaborn:
In[2]: import seaborn as sns
 planets = sns.load_dataset('planets')
 planets.shape
Out[2]: (1035, 6)
In[3]: planets.head()
Out[3]: method number orbital_period mass distance year
 0 Radial Velocity 1 269.300 7.10 77.40 2006
 1 Radial Velocity 1 874.774 2.21 56.95 2008
 2 Radial Velocity 1 763.000 2.60 19.84 2011
 3 Radial Velocity 1 326.030 19.40 110.62 2007
 4 Radial Velocity 1 516.220 10.50 119.47 2009
Этот набор данных содержит определенную информацию о более чем 1000 экзо-
планет, открытых до 2014 года.
Простое агрегирование в библиотеке Pandas
Ранее мы рассмотрели некоторые доступные для массивов NumPy возможности по 
агрегированию данных (см. раздел «Агрегирование: минимум, максимум и все, что 
посередине» главы 2). Как и в случае одномерных массивов библиотеки NumPy, 
для объектов Series библиотеки Pandas агрегирующие функции возвращают ска-
лярное значение:
In[4]: rng = np.random.RandomState(42)
 ser = pd.Series(rng.rand(5))
 ser
Out[4]: 0 0.374540
 1 0.950714
 2 0.731994
 3 0.598658
 4 0.156019
 dtype: float64
In[5]: ser.sum()
Out[5]: 2.8119254917081569

--- СТРАНИЦА 199 ---
Агрегирование и группировка 199
In[6]: ser.mean()
Out[6]: 0.56238509834163142

\1
In[7]: df = pd.DataFrame({'A': rng.rand(5),
 'B': rng.rand(5)})
 df
Out[7]: A B
 0 0.155995 0.020584
 1 0.058084 0.969910
 2 0.866176 0.832443
 3 0.601115 0.212339
 4 0.708073 0.181825
In[8]: df.mean()
Out[8]: A 0.477888
 B 0.443420
 dtype: float64

\1
In[9]: df.mean(axis='columns')
Out[9]: 0 0.088290
 1 0.513997
 2 0.849309
 3 0.406727
 4 0.444949
 dtype: float64
Объекты Series и DataFrame библиотеки Pandas содержат методы, соответству-
ющие всем упомянутым в разделе «Агрегирование: минимум, максимум и все, 
что посередине» главы 2 распространенным агрегирующим функциям. В них есть 
удобный метод describe() , вычисляющий сразу несколько самых распростра -
ненных сводных показателей для каждого столбца и возвращающий результат. 

\1
In[10]: planets.dropna().describe()
Out[10]: number orbital_period mass distance year
 count 498.00000 498.000000 498.000000 498.000000 498.000000
 mean 1.73494 835.778671 2.509320 52.068213 2007.377510
 std 1.17572 1469.128259 3.636274 46.596041 4.167284
 min 1.00000 1.328300 0.003600 1.350000 1989.000000
 25% 1.00000 38.272250 0.212500 24.497500 2005.000000

--- СТРАНИЦА 200 ---

\1ndas 
 50% 1.00000 357.000000 1.245000 39.940000 2009.000000
 75% 2.00000 999.600000 2.867500 59.332500 2011.000000
 max 6.00000 17337.500000 25.000000 354.000000 2014.000000
Эта возможность очень удобна для первоначального знакомства с общими ха -
рактеристиками нашего набора данных. Например, мы видим в столбце year, что, 
хотя первая экзопланета была открыта еще в 1989 году, половина всех известных 
экзопланет открыта не ранее 2010 года. В значительной степени мы обязаны этим 
миссии «Кеплер», представляющей собой космический телескоп, специально раз-
работанный для поиска затмений от планет, вращающих вокруг других звезд.
В табл. 3.3 перечислены основные встроенные агрегирующие методы библиотеки 
Pandas.
Таблица 3.3. Список агрегирующих методов библиотеки Pandas
Агрегирующая функция Описание
count() Общее количество элементов
first(), last() Первый и последний элементы
mean(), median() Среднее значение и медиана
min(), max() Минимум и максимум
std(), var() Стандартное отклонение и дисперсия
mad() Среднее абсолютное отклонение
prod() Произведение всех элементов
sum() Сумма всех элементов
Это все методы объектов DataFrame и Series.
Для более глубокого исследования данных простых сводных показателей часто 
недостаточно. Следующий уровень обобщения данных — операция groupby, позво -
ляющая быстро и эффективно вычислять сводные показатели по подмножествам 
данных.
GroupBy: разбиение, применение, объединение
Простые агрегирующие функции дают возможность «прочувствовать» набор дан-
ных, но зачастую бывает нужно выполнить условное агрегирование по какой-либо 
метке или индексу. Это действие реализовано в так называемой операции GroupBy. 
Название group by («сгруппировать по») ведет начало от одноименной команды 
в языке SQL баз данных, но, возможно, будет понятнее говорить о ней в терминах, 
придуманных Хэдли Викхэмом, более известным своими библиотеками для язы-
ка R: разбиение, применение и объединение.

--- СТРАНИЦА 201 ---
Агрегирование и группировка 201
Разбиение, применение и объединение
Канонический пример операции «разбить, применить, объединить», в которой 
«применить» — обобщающее агрегирование, показан на рис. 3.1.
Рисунок 3.1 демонстрирует, что именно делает операция GroupBy.
 Шаг разбиения включает разделение на части и группировку объекта DataFrame 
на основе значений заданного ключа.
 Шаг применения включает вычисление какой-либо функции, обычно агрегиру -
ющей, преобразование или фильтрацию в пределах отдельных групп.
 На шаге объединения выполняется слияние результатов этих операций в вы -
ходной массив.
Рис. 3.1. Визуальное представление операции GroupBy
Хотя мы, конечно, могли бы сделать это вручную с помощью какого-либо соче -
тания описанных выше команд маскирования, агрегирования и слияния, важно 
понимать, что не обязательно создавать объекты для промежуточных разбиений . 
Операция GroupBy может проделать все это за один проход по данным, вычисляя 
сумму, среднее значение, количество, минимум и другие сводные показатели для 

--- СТРАНИЦА 202 ---

\1ndas 
каждой группы. Мощь операции GroupBy состоит в абстрагировании этих шагов: 
пользователю не нужно заботиться о том, как фактически выполняются вычисле-
ния, а можно вместо этого думать об операции в целом.
В качестве примера рассмотрим использование библиотеки Pandas для выполнения 
показанных на рис. 3.1 вычислений. Начнем с создания входного объекта DataFrame:
In[11]: df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],
 'data': range(6)}, columns=['key', 'data'])
 df
Out[11]: key data
 0 A 0
 1 B 1
 2 C 2
 3 A 3
 4 B 4
 5 C 5

\1
In[12]: df.groupby('key')
Out[12]: <pandas.core.groupby.DataFrameGroupBy object at 0x117272160>

\1
In[13]: df.groupby('key').sum()
Out[13]: data
 key
 A 3
 B 5
 C 7
Метод sum() — лишь один из возможных вариантов в этой команде. Здесь можно 
использовать практически любую распространенную агрегирующую функцию 
библиотек Pandas или NumPy, равно как и практически любую корректную опе-
рацию объекта DataFrame.

--- СТРАНИЦА 203 ---

\1
In[14]: planets.groupby('method')
Out[14]: <pandas.core.groupby.DataFrameGroupBy object at 0x1172727b8>
In[15]: planets.groupby('method')['orbital_period']
Out[15]: <pandas.core.groupby.SeriesGroupBy object at 0x117272da0>

\1
In[16]: planets.groupby('method')['orbital_period'].median()
Out[16]: method
 Astrometry 631.180000
 Eclipse Timing Variations 4343.500000
 Imaging 27500.000000
 Microlensing 3300.000000
 Orbital Brightness Modulation 0.342887
 Pulsar Timing 66.541900
 Pulsation Timing Variations 1170.000000
 Radial Velocity 360.200000
 Transit 5.714932
 Transit Timing Variations 57.011000
 Name: orbital_period, dtype: float64

\1

--- СТРАНИЦА 204 ---

\1ndas 
In[17]: for (method, group) in planets.groupby('method'):
 print("{0:30s} shape={1}".format(method, group.shape))
Astrometry shape=(2, 6)
Eclipse Timing Variations shape=(9, 6)
Imaging shape=(38, 6)
Microlensing shape=(23, 6)
Orbital Brightness Modulation shape=(3, 6)
Pulsar Timing shape=(5, 6)
Pulsation Timing Variations shape=(1, 6)
Radial Velocity shape=(553, 6)
Transit shape=(397, 6)
Transit Timing Variations shape=(4, 6)
Это может пригодиться для выполнения некоторых вещей вручную, хотя обычно 
быстрее воспользоваться встроенной функциональностью apply.
Методы диспетчеризации. Благодаря определенной магии классов языка Python все 
методы, не реализованные явным образом объектом GroupBy, будут передаваться далее 
и выполняться для групп, вне зависимости от того, являются ли они объектами Series 
или DataFrame. Например, можно использовать метод describe() объекта DataFrame 
для вычисления набора сводных показателей, описывающих каждую группу в данных:
In[18]: planets.groupby('method')['year'].describe().unstack()
Out[18]:
 count mean std min 25% 
\\
method
Astrometry 2.0 2011.500000 2.121320 2010.0 2010.75
Eclipse Timing Variations 9.0 2010.000000 1.414214 2008.0 2009.00
Imaging 38.0 2009.131579 2.781901 2004.0 2008.00
Microlensing 23.0 2009.782609 2.859697 2004.0 2008.00
Orbital Brightness Modulation 3.0 2011.666667 1.154701 2011.0 2011.00
Pulsar Timing 5.0 1998.400000 8.384510 1992.0 1992.00
Pulsation Timing Variations 1.0 2007.000000 NaN 2007.0 2007.00
Radial Velocity 553.0 2007.518987 4.249052 1989.0 2005.00
Transit 397.0 2011.236776 2.077867 2002.0 2010.00
Transit Timing Variations 4.0 2012.500000 1.290994 2011.0 2011.75
 50% 75% max
method
Astrometry 2011.5 2012.25 2013.0
Eclipse Timing Variations 2010.0 2011.00 2012.0
Imaging 2009.0 2011.00 2013.0
Microlensing 2010.0 2012.00 2013.0
Orbital Brightness Modulation 2011.0 2012.00 2013.0
Pulsar Timing 1994.0 2003.00 2011.0
Pulsation Timing Variations 2007.0 2007.00 2007.0
Radial Velocity 2009.0 2011.00 2014.0
Transit 2012.0 2013.00 2014.0
Transit Timing Variations 2012.5 2013.25 2014.0

--- СТРАНИЦА 205 ---
Агрегирование и группировка 205
Эта таблица позволяет получить лучшее представление о наших данных. Напри-
мер, большинство планет было открыто методом измерения лучевой скорости 
(radial velocity method) и транзитным методом (transit method), хотя последний 
стал распространенным благодаря новым более точным телескопам только в по-
следнее десятилетие. Похоже, что новейшими методами являются метод вариации 
времени транзитов (transit timing variation method) и метод модуляции орбиталь-
ной яркости (orbital brightness modulation method), которые до 2011 года не ис -
пользовались для открытия новых планет.
Это всего лишь один пример полезности методов диспе тчеризации. Обратите 
внимание, что они применяются к каждой отдельной группе, после чего результаты 
объединяются в объект GroupBy и возвращаются. Можно использовать для соответ-
ствующего объекта GroupBy любой допустимый метод объектов Series/ DataFrame, 
что позволяет выполнять многие весьма гибкие и мощные операции!
Агрегирование, фильтрация, преобразование, применение
Предыдущее обсуждение касалось агрегирования применительно к операции объ-
единения, но доступны и другие возможности. В частности, у объектов GroupBy 
имеются методы aggregate(), filter(), transform() и apply(), эффективно выпол -
няющие множество полезных операций до объединения сгруппированных данных.

\1
In[19]: rng = np.random.RandomState(0)
 df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],
 'data1': range(6),
 'data2': rng.randint(0, 10, 6)},
 columns = ['key', 'data1', 'data2'])
 df
Out[19]: key data1 data2
 0 A 0 5
 1 B 1 0
 2 C 2 3
 3 A 3 3
 4 B 4 7
 5 C 5 9
Агрегирование. Мы уже знакомы со сводными показателями объекта GroupBy, вы -
числяемыми с помощью методов sum(), median() и т. п., но метод aggregate() обе-
спечивает еще большую гибкость. Он может принимать на входе строку, функцию 
или список и вычислять все сводные показатели сразу. Вот пример, включающий 
все вышеупомянутое:
In[20]: df.groupby('key').aggregate(['min', np.median, max])
Out[20]: data1 data2

--- СТРАНИЦА 206 ---

\1ndas 
 min median max min median max
 key
 A 0 1.5 3 3 4.0 5
 B 1 2.5 4 0 3.5 7
 C 2 3.5 5 3 6.0 9

\1
In[21]: df.groupby('key').aggregate({'data1': 'min',
 'data2': 'max'})
Out[21]: data1 data2
 key
 A 0 5
 B 1 7
 C 2 9

\1
In[22]:
def filter_func(x):
 return x['data2'].std() > 4
print(df); print(df.groupby('key').std());
print(df.groupby('key').filter(filter_func))
df df.groupby('key').std()
 key data1 data2 key data1 data2
0 A 0 5 A 2.12132 1.414214
1 B 1 0 B 2.12132 4.949747
2 C 2 3 C 2.12132 4.242641
3 A 3 3
4 B 4 7
5 C 5 9
df.groupby('key').filter(filter_func)
 key data1 data2
1 B 1 0
2 C 2 3
4 B 4 7
5 C 5 9

\1
In[23]: df.groupby('key').transform(lambda x: x - x.mean())
Out[23]: data1 data2
 0 -1.5 1.0
 1 -1.5 -3.5
 2 -1.5 -3.0
 3 1.5 -1.0
 4 1.5 3.5
 5 1.5 3.0
Метод apply(). Метод apply() позволяет применять произвольную функцию к ре-
зультатам группировки. В качестве параметра эта функция должна получать объ-
ект DataFrame, а возвращать или объект библиотеки Pandas (например, DataFrame, 
Series), или скалярное значение, в зависимости от возвращаемого значения будет 
вызвана соответствующая операция объединения.

\1
In[24]: def norm_by_data2(x):
 # x – объект DataFrame сгруппированных значений
 x['data1'] /= x['data2'].sum()
 return x 
 print(df); print(df.groupby('key').apply(norm_by_data2))
df df.groupby('key').apply(norm_by_data2)
 key data1 data2 key data1 data2
0 A 0 5 0 A 0.000000 5
1 B 1 0 1 B 0.142857 0
2 C 2 3 2 C 0.166667 3
3 A 3 3 3 A 0.375000 3
4 B 4 7 4 B 0.571429 7
5 C 5 9 5 C 0.416667 9
Функция apply() в GroupBy достаточно гибка. Единственное требование, чтобы она 
принимала на входе объект DataFrame и возвращала объект библиотеки Pandas или 
скалярное значение; что вы делаете внутри, остается на ваше усмотрение!
Задание ключа разбиения
В представленных ранее простых примерах мы разбивали объект DataFrame по 
одному столбцу. Это лишь один из многих вариантов задания принципа формиро-
вания групп, и мы сейчас рассмотрим некоторые другие возможности.

--- СТРАНИЦА 208 ---

\1ndas 

\1
In[25]: L = [0, 1, 0, 1, 2, 0]
print(df); print(df.groupby(L).sum())
df df.groupby(L).sum()
 key data1 data2 data1 data2
0 A 0 5 0 7 17
1 B 1 0 1 4 3
2 C 2 3 2 4 7
3 A 3 3
4 B 4 7
5 C 5 9

\1
In[26]: print(df); print(df.groupby(df['key']).sum())
df df.groupby(df['key']).sum()
 key data1 data2 data1 data2
0 A 0 5 A 3 8
1 B 1 0 B 5 7
2 C 2 3 C 7 12
3 A 3 3
4 B 4 7
5 C 5 9

\1
In[27]: df2 = df.set_index('key')
 mapping = {'A': 'vowel', 'B': 'consonant', 'C': 'consonant'}
 print(df2); print(df2.groupby(mapping).sum())
df2 df2.groupby(mapping).sum()
key data1 data2 data1 data2
A 0 5 consonant 12 19
B 1 0 vowel 3 8
C 2 3
A 3 3
B 4 7
C 5 9
Любая функция языка Python. Аналогично заданию соответствия можно пере -
дать функции groupby любую функцию, принимающую на входе значение индекса 
и возвращающую группу:

--- СТРАНИЦА 209 ---
Агрегирование и группировка 209
In[28]: print(df2); print(df2.groupby(str.lower).mean())
df2 df2.groupby(str.lower).mean()
key data1 data2 data1 data2
A 0 5 a 1.5 4.0
B 1 0 b 2.5 3.5
C 2 3 c 3.5 6.0
A 3 3
B 4 7
C 5 9

\1
In[29]: df2.groupby([str.lower, mapping]).mean()
Out[29]: data1 data2
 a vowel 1.5 4.0
 b consonant 2.5 3.5
 c consonant 3.5 6.0
Пример группировки
В качестве примера соберем все это вместе в нескольких строках кода на языке 
Python и подсчитаем количество открытых планет по методу открытия и десяти-
летию:
In[30]: decade = 10 * (planets['year'] // 10)
 decade = decade.astype(str) + 's'
 decade.name = 'decade'
 planets.groupby(['method', decade])
 ['number'].sum().unstack().fillna(0)
Out[30]: decade 1980s 1990s 2000s 2010s
 method
 Astrometry 0.0 0.0 0.0 2.0
 Eclipse Timing Variations 0.0 0.0 5.0 10.0
 Imaging 0.0 0.0 29.0 21.0
 Microlensing 0.0 0.0 12.0 15.0
 Orbital Brightness Modulation 0.0 0.0 0.0 5.0
 Pulsar Timing 0.0 9.0 1.0 1.0
 Pulsation Timing Variations 0.0 0.0 1.0 0.0
 Radial Velocity 1.0 52.0 475.0 424.0
 Transit 0.0 0.0 64.0 712.0
 Transit Timing Variations 0.0 0.0 0.0 9.0
Это демонстрирует возможности комбинирования нескольких из вышеописанных 
операций применительно к реальным наборам данных. Мы мгновенно получили 
представление о том, когда и как открывались экзопланеты в последние несколько 
десятилетий!

--- СТРАНИЦА 210 ---

\1ndas 
Теперь же я предложил бы углубиться в эти несколько строк кода и выполнить 
их пошагово, чтобы убедиться, что вы действительно понимаете, какой вклад 
в результат они вносят. Это в чем-то непростой пример, но благодаря хорошему 
пониманию элементов кода у вас появятся средства для исследования ваших соб-
ственных данных.
Сводные таблицы
Мы уже видели возможности по исследованию отношений в наборе данных, 
предоставляемые абстракцией GroupBy. Сводная таблица (pivot table) — схожая 
операция, часто встречающаяся в электронных таблицах и других программах, 
работающих с табличными данными. Сводная таблица получает на входе простые 
данные в виде столбцов и группирует записи в двумерную таблицу, обеспечива-
ющую многомерное представление данных. Различие между сводными табли -
цами и операцией GroupBy иногда неочевидно. Лично мне помогает представлять 
сводные таблицы как многомерную версию агрегирующей функции GroupBy . 
То есть вы выполняете операцию «разбить, применить, объединить», но как раз -
биение, так и объединение происходят не на одномерном индексе, а на двумерной 
координатной сетке.
Данные для примеров работы со сводными таблицами
Для примеров из этого раздела мы воспользуемся базой данных пассажиров паро-
хода «Титаник», доступной через библиотеку Seaborn (см. раздел «Визуализация 
с помощью библиотеки Seaborn» главы 4):
In[1]: import numpy as np
 import pandas as pd
 import seaborn as sns
 titanic = sns.load_dataset('titanic')
In[2]: titanic.head()
Out[2]:
 survived pclass sex age sibsp parch fare embarked class \\
0 0 3 male 22.0 1 0 7.2500 S Third
1 1 1 female 38.0 1 0 71.2833 C First
2 1 3 female 26.0 0 0 7.9250 S Third
3 1 1 female 35.0 1 0 53.1000 S First
4 0 3 male 35.0 0 0 8.0500 S Third
 who adult_male deck embark_town alive alone
0 man True NaN Southampton no False
1 woman False C Cherbourg yes False
2 woman False NaN Southampton yes True

--- СТРАНИЦА 211 ---
Сводные таблицы 211
3 woman False C Southampton yes False
4 man True NaN Southampton no True

\1
In[3]: titanic.groupby('sex')[['survived']].mean()
Out[3]: survived
 sex
 female 0.742038
 male 0.188908
Это сразу же дает нам некоторое представление о наборе данных: в целом, три че-
тверти находившихся на борту женщин выжило, в то время как из мужчин выжил 
только каждый пятый!
Однако хотелось бы заглянуть немного глубже и увидеть распределение выжив -
ших по полу и классу. Говоря языком GroupBy, можно было бы идти следующим 
путем: сгруппировать по классу и полу, выбрать выживших, применить агрегиру-
ющую функцию среднего значения, объединить получившиеся группы, после чего 
выполнить операцию unstack иерархического индекса, чтобы обнажить скрытую 
многомерность. В виде кода:
In[4]: titanic.groupby(['sex', 'class'])
 ['survived'].aggregate('mean').unstack()
Out[4]: class First Second Third
 sex
 female 0.968085 0.921053 0.500000
 male 0.368852 0.157407 0.135447
Это дает нам лучшее представление о том, как пол и класс влияли на выжива-
емость, но код начинает выглядеть несколько запутанным. Хотя каждый шаг этого 
конвейера представляется вполне осмысленным в свете ранее рассмотренных ин-
струментов, такая длинная строка кода не особо удобна для чтения или использо-
вания. Двумерный GroupBy встречается настолько часто, что в состав библиотеки 
Pandas был включен удобный метод, pivot_table, позволяющий описывать более 
кратко данную разновидность многомерного агрегирования.

--- СТРАНИЦА 212 ---

\1ndas 

\1
In[5]: titanic.pivot_table('survived', index='sex', columns='class')
Out[5]: class First Second Third
 sex
 female 0.968085 0.921053 0.500000
 male 0.368852 0.157407 0.135447

\1
In[6]: age = pd.cut(titanic['age'], [0, 18, 80])
 titanic.pivot_table('survived', ['sex', age], 'class')
Out[6]: class First Second Third
 sex age
 female (0, 18] 0.909091 1.000000 0.511628
 (18, 80] 0.972973 0.900000 0.423729
 male (0, 18] 0.800000 0.600000 0.215686
 (18, 80] 0.375000 0.071429 0.133663

\1
In[7]: fare = pd.qcut(titanic['fare'], 2)
 titanic.pivot_table('survived', ['sex', age], [fare, 'class'])
Out[7]:
fare [0, 14.454]
class First Second Third \\
sex age
female (0, 18] NaN 1.000000 0.714286
 (18, 80] NaN 0.880000 0.444444
male (0, 18] NaN 0.000000 0.260870
 (18, 80] 0.0 0.098039 0.125000

--- СТРАНИЦА 213 ---
Сводные таблицы 213
fare (14.454, 512.329]
class First Second Third
sex age
female (0, 18] 0.909091 1.000000 0.318182
 (18, 80] 0.972973 0.914286 0.391304
male (0, 18] 0.800000 0.818182 0.178571
 (18, 80] 0.391304 0.030303 0.192308

\1
# сигнатура вызова в версии 0.181 библиотеки Pandas
DataFrame.pivot_table(data, values=None, index=None, columns=None,
 aggfunc='mean', fill_value=None, margins=False,
 dropna=True, margins_name='All')
Мы уже видели примеры первых трех аргументов, в данном подразделе рас -
смотрим остальные. Два из параметров, fill_value и dropna, относятся к про -
пущенным значениям и интуитивно понятны, примеры их использования мы 
приводить не будем.
Ключевое слово aggfunc управляет тем, какой тип агрегирования применяется, 
по умолчанию это среднее значение. Как и в GroupBy, спецификация агрегирую -
щей функции может быть строкой с одним из нескольких обычных вариантов 
( 'sum', 'mean', 'count', 'min', 'max' и т. д.) или функцией, реализующей агреги -
рование ( np.sum(), min(), sum() и т. п.). Кроме того, агрегирование может быть 
задано в виде словаря, связывающего столбец с любым из вышеперечисленных 
вариантов:
In[8]: titanic.pivot_table(index='sex', columns='class',
 aggfunc={'survived':sum, 'fare':'mean'})
Out[8]: fare survived
 class First Second Third First Second Third
 sex
 female 106.125798 21.970121 16.118810 91.0 70.0 72.0
 male 67.226127 19.741782 12.661633 45.0 17.0 47.0
Обратите внимание, что мы опустили ключевое слово values, при задании aggfunc 
происходит автоматическое определение.

\1ndas 
Иногда бывает полезно вычислять итоги по каждой группе. Это можно сделать 
с помощью ключевого слова margins:
In[9]: titanic.pivot_table('survived', index='sex', columns='class',
 margins=True)
Out[9]: class First Second Third All
 sex
 female 0.968085 0.921053 0.500000 0.742038
 male 0.368852 0.157407 0.135447 0.188908
 All 0.629630 0.472826 0.242363 0.383838
Такие итоги автоматически дают нам информацию о выживаемости вне зависи -
мости от класса, коэффициенте выживаемости по классу вне зависимости от пола 
и общем коэффициенте выживаемости 38 %. Метки для этих итогов можно задать 
с помощью ключевого слова margins_name, по умолчанию имеющего значение "All".
Пример: данные о рождаемости
В качестве примера взглянем на находящиеся в открытом доступе данные о рожда-
емости в США, предоставляемые центрами по контролю заболеваний (Centers for 
Disease Control, CDC). Данные можно найти по адресу https://raw.githubusercontent.
com/jakevdp/data-CDCbirths/master/births.csv (этот набор данных довольно широко 
 исследовался Эндрю Гелманом и его группой (см., например, сообщение в блоге 
http://bit.ly/2fZzW8K)):
In[10]:
# Инструкция системного командного процессора для скачивания данных:
# !curl -O https://raw.githubusercontent.com/jakevdp/data-CDCbirths/
# master/births.csv
In[11]: births = pd.read_csv('births.csv')

\1
In[12]: births.head()
Out[12]: year month day gender births
 0 1969 1 1 F 4046
 1 1969 1 1 M 4440
 2 1969 1 2 F 4454
 3 1969 1 2 M 4548
 4 1969 1 3 F 4548

\1

--- СТРАНИЦА 215 ---
Сводные таблицы 215
In[13]:
births['decade'] = 10 * (births['year'] // 10)
births.pivot_table('births', index='decade', columns='gender', aggfunc='sum')
Out[13]: gender F M decade
 1960 1753634 1846572
 1970 16263075 17121550
 1980 18310351 19243452
 1990 19479454 20420553
 2000 18229309 19106428
Сразу же видим, что в каждом десятилетии мальчиков рождается больше, чем де-
вочек. Воспользуемся встроенными средствами построения графиков библиотеки 
Pandas для визуализации общего количества новорожденных в зависимости от года 
(рис. 3.2; см. обсуждение построения графиков с помощью библиотеки Matplotlib 
в главе 4):
In[14]:
%matplotlib inline
import matplotlib.pyplot as plt
sns.set() # Используем стили библиотеки Seaborn
births.pivot_table('births', index='year', columns='gender', 
 aggfunc='sum').plot()
plt.ylabel('total births per year'); # общее количество новорожденных
 # в течение года
Благодаря сводной таблице и методу plot() мы можем сразу же увидеть ежегодный 
тренд новорожденных по полу. В последние 50 с лишним лет мальчиков рождалось 
больше, чем девочек, примерно на 5 %.
Дальнейшее исследование данных. Хотя это, возможно, и не имеет отношения 
к сводным таблицам, есть еще несколько интересных вещей, которые можно 
извлечь из этого набора данных с помощью уже рассмотренных инструментов 
библиотеки Pandas. Нам придется начать с небольшой очистки данных, удалив 
аномальные значения, возникшие из-за неправильно набранных дат (например, 
31 июня) или отсутствующих значений (например, 99 июня). Простой способ 
убрать сразу их все — отсечь аномальные значения. Мы сделаем это с помощью 
надежного алгоритма сигма-отсечения (sigma-clipping) 1 :
In[15]: quartiles = np.percentile(births['births'], [25, 50, 75])
 mu = quartiles[1]
 sig = 0.74 * (quartiles[2] - quartiles[0])

\1ning and Machine Learning in Astronomy: A Practical Python Guide for the Analysis 
of Survey Data (Princeton University Press, 2014).

--- СТРАНИЦА 216 ---

\1ndas 
Рис. 3.2. Общее количество новорожденных в США в зависимости от года и пола
Последняя строка представляет собой грубую оценку среднего значения выборки, 
в котором 0.74 — межквартильный размах Гауссового распределения. Теперь можно 
воспользоваться методом query() (обсуждаемым далее в разделе «Увеличение про-
изводительности библиотеки Pandas: eval() и query()» этой главы) для фильтрации 
строк, в которых количество новорожденных выходит за пределы этих значений:
In[16]:
births = births.query('(births > @mu - 5 * @sig) & (births < @mu + 5 * @sig)')
Далее мы устанавливаем целочисленный тип столбца для day. Ранее он был 
строчным, поскольку некоторые столбцы в наборе данных содержат значение 
'null':
In[17]: # делаем тип столбца 'day' целочисленным; 
# изначально он был строчным из-за пустых значений
 births['day'] = births['day'].astype(int)

\1
In[18]: # создаем индекс для даты из года, месяца и дня
 births.index = pd.to_datetime(10000 * births.year +
 100 * births.month +
 births.day, format='%Y%m%d')
 births['dayofweek'] = births.index.dayofweek # День недели

--- СТРАНИЦА 217 ---

\1
In[19]:
import matplotlib.pyplot as plt
import matplotlib as mpl
births.pivot_table('births', index='dayofweek',
 columns='decade', aggfunc='mean').plot()
plt.gca().set_xticklabels(['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 
'Sun'])
plt.ylabel('mean births by day'); # среднее количество новорожденных в день

\1
In[20]:
births_by_date = births.pivot_table('births',
 [births.index.month, births.index.day])
births_by_date.head()
Out[20]: 1 1 4009.225
 2 4247.400

--- СТРАНИЦА 218 ---

\1ndas 
 3 4500.900
 4 4571.350
 5 4603.625
 Name: births, dtype: float64
Результат представляет собой мультииндекс по месяцам и дням. Чтобы упростить 
построение графика, преобразуем эти месяцы и дни в даты путем связывания их 
с фиктивным годом (обязательно выберите високосный год, чтобы обработать 
29 февраля корректным образом!)
In[21]: births_by_date.index = [pd.datetime(2012, month, day)
 for (month, day) in births_by_date.index]
 births_by_date.head()
Out[21]: 2012-01-01 4009.225
 2012-01-02 4247.400
 2012-01-03 4500.900
 2012-01-04 4571.350
 2012-01-05 4603.625
 Name: births, dtype: float64

\1
In[22]: # Строим график результатов
 fig, ax = plt.subplots(figsize=(12, 4))
 births_by_date.plot(ax=ax);
Рис. 3.4. Среднее количество новорожденных в зависимости от месяца
В частности, на графике удивляет резкое падение количества рождений в госу -
дарственные праздники США (например, День независимости, День труда, День 
благодарения, Рождество, Новый год). Хотя оно отражает скорее тенденции, 
 относящиеся к заранее запланированным/искусственным родам, а не глубокое 

--- СТРАНИЦА 219 ---
Векторизованные операции над строками 219
психосоматическое влияние на естественные роды. Дальнейшее обсуждение 
данной тенденции, ее анализ и ссылки на эту тему смотрите в сообщении по 
адресу http://bit.ly/2fZzW8K из блога Эндрю Гелмана. Мы вернемся к этому графику 
в подразделе «Пример: влияние выходных на рождения детей в США» раздела 
«Текст и поясняющие надписи» главы 4, в котором воспользуемся инструментами 
библиотеки Matplotlib для добавления меток на график.
Глядя на этот краткий пример, вы могли заметить, что многие из рассмотренных 
нами инструментов языка Python и библиотеки Pandas можно комбинировать 
между собой и использовать, чтобы почерпнуть полезную информацию из мно -
жества наборов данных. Более сложные манипуляции над данными мы увидим 
в следующих разделах!
Векторизованные операции над строками
Одна из сильных сторон языка Python — относительное удобство работы в нем со 
строковыми данными и манипуляций ими. Библиотека Pandas вносит в это свою 
лепту и предоставляет набор векторизованных операций над строками , ставших 
существенной частью очистки данных, необходимой при работе с реальными 
данными. В этом разделе мы изучим некоторые строковые операции библиотеки 
Pandas, после чего рассмотрим их использование для частичной очистки очень за -
шумленного набора данных рецептов, собранных в Интернете.
Знакомство со строковыми операциями 
библиотеки Pandas
В предыдущем разделе мы видели, как обобщают арифметические операции та -
кие инструменты, как библиотека NumPy и библиотека Pandas, позволяя легко 
и быстро выполнять одну и ту же операцию над множеством элементов массива. 

\1
In[1]: import numpy as np
 x = np.array([2, 3, 5, 7, 11, 13])
 x * 2
Out[1]: array([ 4, 6, 10, 14, 22, 26])

\1

--- СТРАНИЦА 220 ---

\1ndas 
In[2]: data = ['peter', 'Paul', 'MARY', 'gUIDO']
 [s.capitalize() for s in data]
Out[2]: ['Peter', 'Paul', 'Mary', 'Guido']

\1
In[3]: data = ['peter', 'Paul', None, 'MARY', 'gUIDO']
 [s.capitalize() for s in data]
---------------------------------------------------------------------------
---------------------------------------------------------------------------
AttributeError Traceback (most recent call last)
<ipython-input-3-fc1d891ab539> in <module>()
 1 data = ['peter', 'Paul', None, 'MARY', 'gUIDO']
----> 2 [s.capitalize() for s in data]
<ipython-input-3-fc1d891ab539> in <listcomp>(.0)
 1 data = ['peter', 'Paul', None, 'MARY', 'gUIDO']
----> 2 [s.capitalize() for s in data]
AttributeError: 'NoneType' object has no attribute 'capitalize'
---------------------------------------------------------------------------
Библиотека Pandas включает средства как для работы с векторизованными стро-
ковыми операциями, так и для корректной обработки отсутствующих значений 
посредством атрибута str объектов Series библиотеки Pandas и содержащих стро -
ки объектов Index. Так, допустим, мы создали объект Series библиотеки Pandas 
с теми же данными:
In[4]: import pandas as pd
 names = pd.Series(data)
 names
Out[4]: 0 peter
 1 Paul
 2 None
 3 MARY
 4 gUIDO
 dtype: object

\1
In[5]: names.str.capitalize()

--- СТРАНИЦА 221 ---
Векторизованные операции над строками 221
Out[5]: 0 Peter
 1 Paul
 2 None
 3 Mary
 4 Guido
 dtype: object
С помощью Tab-автодополнения для этого атрибута str можно получить список 
всех векторизованных строковых методов, доступных в библиотеке Pandas.
Таблицы методов работы со строками 
библиотеки Pandas
Если вы хорошо разбираетесь в манипуляции строковыми д анными в языке 
Python, львиная доля синтаксиса работы со строками библиотеки Pandas будет вам 
интуитивно понятна настолько, что достаточно, наверное, просто привести таблицу 
имеющихся методов. С этого и начнем, прежде чем углубимся в некоторые нюансы. 

\1
In[6]: monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',
 'Eric Idle', 'Terry Jones', 'Michael Palin'])
Методы, аналогичные строковым методам языка Python
Практически для всех встроенных строковых методов Python есть соответ -
ствующий векторизованный строковый метод библиотеки Pandas. Вот список 
методов атрибута str библиотеки Pandas, дублирующий строковые методы 
языка Python:
len() lower() translate() islower()
ljust() upper() startswith() isupper()
rjust() find() endswith() isnumeric()
center() rfind() isalnum() isdecimal()
zfill() index() isalpha() split()
strip() rindex() isdigit() rsplit()
rstrip() capitalize() isspace() partition()
lstrip() swapcase() istitle() rpartition()

\1
In[7]: monte.str.lower()
Out[7]: 0 graham chapman
 1 john cleese
 2 terry gilliam

--- СТРАНИЦА 222 ---

\1ndas 
 3 eric idle
 4 terry jones
 5 michael palin
 dtype: object

\1
In[8]: monte.str.len()
Out[8]: 0 14
 1 11
 2 13
 3 9
 4 11
 5 13
 dtype: int64

\1
In[9]: monte.str.startswith('T')
Out[9]: 0 False
 1 False
 2 True
 3 False
 4 True
 5 False
 dtype: bool

\1
In[10]: monte.str.split()
Out[10]: 0 [Graham, Chapman]
 1 [John, Cleese]
 2 [Terry, Gilliam]
 3 [Eric, Idle]
 4 [Terry, Jones]
 5 [Michael, Palin]
 dtype: object
Мы увидим манипуляции над подобными объектами типа «ряды списков», когда 
продолжим обсуждение.
Методы, использующие регулярные выражения
Помимо этого, существует и несколько методов, принимающих на входе регу -
лярные выражения для проверки содержимого каждого из строковых элементов 
и следующих некоторым соглашениям по API встроенного модуля re языка Python 
(табл. 3.4).

--- СТРАНИЦА 223 ---
Векторизованные операции над строками 223
Таблица 3.4. Соответствие между методами библиотеки Pandas и функциями модуля re языка 
Python
Метод Описание
match() Вызывает функцию re.match() для каждого элемента, возвращая булево 
значение
extract() Вызывает функцию re.match() для каждого элемента, возвращая подходя-
щие группы в виде строк
findall() Вызывает функцию re.findall() для каждого элемента
replace() Заменяет вхождения шаблона какой-либо другой строкой
contains() Вызывает функцию re.search() для каждого элемента, возвращая булево 
значение
count() Подсчитывает вхождения шаблона
split() Эквивалент функции str.split(), но принимающий на входе регулярные 
выражения
rsplit() Эквивалент функции str.rsplit(), но принимающий на входе регулярные 
выражения

\1
In[11]: monte.str.extract('([A-Za-z]+)')
Out[11]: 0 Graham
 1 John
 2 Terry
 3 Eric
 4 Terry
 5 Michael
 dtype: object

\1
In[12]: monte.str.findall(r'^[^AEIOU].*[^aeiou]$')
Out[12]: 0 [Graham Chapman]
 1 []
 2 [Terry Gilliam]
 3 []
 4 [Terry Jones]
 5 [Michael Palin]
 dtype: object
Такой сжатый синтаксис регулярных выражений для запи сей объектов Series 
и DataFrame открывает массу возможностей для анализа и очистки данных.

--- СТРАНИЦА 224 ---

\1ndas 
Прочие методы
Наконец, существуют и прочие методы, пригодные для разных удобных операций 
(табл. 3.5).
Таблица 3.5. Прочие методы для работы со строками библиотеки Pandas
Метод Описание
get() Индексирует все элементы
slice() Вырезает подстроку из каждого элемента
slice_replace() Заменяет в каждом элементе вырезанную подстроку заданным значением
cat() Конкатенация строк
repeat() Повторяет значения (указанное число раз)
normalize() Возвращает версию строки в кодировке Unicode
pad() Добавляет пробелы слева, справа или с обеих сторон строки
wrap() Разбивает длинные строковые значения на строки длины, не превыша-
ющей заданную
join() 1 Объединяет строки из всех элементов с использованием заданного раз-
делителя
get_dummies() Извлекает значения переменных-индикаторов в виде объекта DataFrame
Векторизованный доступ к элементам и вырезание подстрок. Операции get() 
и slice(), в частности, предоставляют возможность векторизованного доступа 
к элементам из каждого массива. Например, можно вырезать первые три сим -
вола из каждого массива посредством выражения str.slice(0, 3) . Обратите 
внимание, что такая возможность доступна и с помощь ю обычного синтак -
сиса индексации языка Python, например, df.str.slice(0, 3) эквивалентно 
df.str[0:3]:
In[13]: monte.str[0:3]
Out[13]: 0 Gra
 1 Joh
 2 Ter
 3 Eri
 4 Ter
 5 Mic
 dtype: object
Индексация посредством df.str.get(i) и df.str[i] происходит аналогично.

\1n(monte)
 где ; — разделитель.

--- СТРАНИЦА 225 ---

\1
In[14]: monte.str.split().str.get(-1)
Out[14]: 0 Chapman
 1 Cleese
 2 Gilliam
 3 Idle
 4 Jones
 5 Palin
 dtype: object

\1
In[15]:
full_monte = pd.DataFrame({'name': monte,
 'info': ['B|C|D', 'B|D', 'A|C', 'B|D', 'B|C',
 'B|C|D']})
full_monte
Out[15]: info name
 0 B|C|D Graham Chapman
 1 B|D John Cleese
 2 A|C Terry Gilliam
 3 B|D Eric Idle
 4 B|C Terry Jones
 5 B|C|D Michael Palin

\1
In[16]: full_monte['info'].str.get_dummies('|')
Out[16]: A B C D
 0 0 1 1 1
 1 0 1 0 1
 2 1 0 1 0
 3 0 1 0 1
 4 0 1 1 0
 5 0 1 1 1
Используя эти операции как «строительные блоки», можно создать бесчисленное 
множество обрабатывающих строки процедур для очистки данных.
Мы не будем углубляться в эти методы, но я рекомендую прочитать раздел Working 
with Text Data («Работа с текстовыми данными») из онлай н-документации 

--- СТРАНИЦА 226 ---

\1ndas 
библиотеки Pandas ( http://pandas.pydata.org/pandas-docs/stable/text.html) или заглянуть 
в раздел «Дополнительные источники информации» данной главы.
Пример: база данных рецептов
Описанные векторизованные строковые операции оказываются наиболее полез -
ными при очистке сильно зашумленных реальных данных. Здесь мы рассмотрим 
пример такой очистки, воспользовавшись полученной из множества различных 
интернет-источников базой данных рецептов. Наша цель — разбор рецептов на спи-
ски ингредиентов, чтобы можно было быстро найти рецепт, исходя из име ющихся 
в распоряжении ингредиентов.
Используемые для компиляции сценарии можно найти по адресу https://github.com/
fictivekin/openrecipes, как и ссылку на актуальную версию базы.

\1
In[17]: # !curl -O 
 # http://openrecipes.s3.amazonaws.com/20131812-recipeitems.json.gz
 # !gunzip 20131812-recipeitems.json.gz
База данных находится в формате JSON, так что можно попробовать воспользо -
ваться функцией pd.read_json для ее чтения:
In[18]: try:
 recipes = pd.read_json('recipeitems-latest.json')
 except ValueError as e:
 print("ValueError:", e)
ValueError: Trailing data

\1
In[19]: with open('recipeitems-latest.json') as f:
 line = f.readline()
 pd.read_json(line).shape
Out[19]: (2, 12)
Да, очевидно, каждая строка — корректный JSON, так что нам нужно соединить их 
все воедино. Один из способов сделать это — фактически сформировать строковое 
представление, содержащее все записи JSON, после чего загрузить все с помощью 
pd.read_json:

--- СТРАНИЦА 227 ---
Векторизованные операции над строками 227
In[20]: # Читаем весь файл в массив Python
 with open('recipeitems-latest.json', 'r') as f:
 # Извлекаем каждую строку
 data = (line.strip() for line in f)
 # Преобразуем так, чтобы каждая строка была элементом списка
 data_json = "[{0}]".format(','.join(data))
 # Читаем результат в виде JSON
 recipes = pd.read_json(data_json)
In[21]: recipes.shape
Out[21]: (173278, 17)

\1
In[22]: recipes.iloc[0]
Out[22]:
_id {'$oid': '5160756b96cc62079cc2db15'}
cookTime PT30M
creator NaN
dateModified NaN
datePublished 2013-03-11
description Late Saturday afternoon, after Marlboro Man ha...
image http://static.thepioneerwoman.com/cooking/file...
ingredients Biscuits\n3 cups All-purpose Flour\n2 Tablespo...
name Drop Biscuits and Sausage Gravy
prepTime PT10M
recipeCategory NaN
recipeInstructions NaN
recipeYield 12
source thepioneerwoman
totalTime NaN
ts {'$date': 1365276011104}
url http://thepioneerwoman.com/cooking/2013/03/dro...
Name: 0, dtype: object

\1
In[23]: recipes.ingredients.str.len().describe()
Out[23]: count 173278.000000
 mean 244.617926
 std 146.705285
 min 0.000000
 25% 147.000000

--- СТРАНИЦА 228 ---

\1ndas 
 50% 221.000000
 75% 314.000000
 max 9067.000000
 Name: ingredients, dtype: float64

\1
In[24]: recipes.name[np.argmax(recipes.ingredients.str.len())]
Out[24]: 'Carrot Pineapple Spice &amp; Brownie Layer Cake with Whipped Cream
&amp; Cream Cheese Frosting and Marzipan Carrots'

\1
In[33]: recipes.description.str.contains('[Bb]reakfast').sum()
Out[33]: 3524
Или сколько рецептов содержат корицу (cinnamon) в списке ингредиентов:
In[34]: recipes.ingredients.str.contains('[Cc]innamon').sum()
Out[34]: 10526
Можно даже посмотреть, есть ли рецепты, в которых название этого ингредиента 
написано с орфографической ошибкой, как cinamon:
In[27]: recipes.ingredients.str.contains('[Cc]inamon').sum()
Out[27]: 11
Такая разновидность обязательного предварительного изучения данных возмож-
на благодаря инструментам по работе со строками библиотеки Pandas. Именно 
в сфере такой очистки данных Python действительно силен.

\1
In[28]: spice_list = ['salt', 'pepper', 'oregano', 'sage', 'parsley',
 'rosemary', 'tarragon', 'thyme', 'paprika', 'cumin']

\1
In[29]:
import re
spice_df = pd.DataFrame(
 dict((spice, recipes.ingredients.str.contains(spice, re.IGNORECASE))
 for spice in spice_list))
spice_df.head()
Out[29]:
 cumin oregano paprika parsley pepper rosemary sage salt tarragon thyme
0 False False False False False False True False False False
1 False False False False False False False False False False
2 True False False False True False False True False False
3 False False False False False False False False False False
4 False False False False False False False False False False
Теперь в качестве примера допустим, что мы хотели бы найти рецепт, в котором ис-
пользуются петрушка (parsley), паприка (paprika) и эстрагон (tarragon). Это мож-
но сделать очень быстро, воспользовавшись методом query() объекта DataFrame, 
который мы обсудим подробнее в разделе «Увеличение производительности би-
блиотеки Pandas: eval() и query()» данной главы:
In[30]: selection = spice_df.query('parsley & paprika & tarragon')
 len(selection)
Out[30]: 10

\1
In[31]: recipes.name[selection.index]
Out[31]: 2069 All cremat with a Little Gem, dandelion and wa...
 74964 Lobster with Thermidor butter
 93768 Burton's Southern Fried Chicken with White Gravy
 113926 Mijo's Slow Cooker Shredded Beef
 137686 Asparagus Soup with Poached Eggs
 140530 Fried Oyster Po'boys
 158475 Lamb shank tagine with herb tabbouleh
 158486 Southern fried chicken in buttermilk
 163175 Fried Chicken Sliders with Pickles + Slaw
 165243 Bar Tartine Cauliflower Salad
 Name: name, dtype: object

--- СТРАНИЦА 230 ---

\1ndas 
Теперь, сократив наш список рецептов почти в 20 000 раз, мы можем принять более 
взвешенное решение: что готовить на обед.
Дальнейшая работа с рецептами
Надеемся, что этот пример позволил вам попробовать на вкус (па-ба-ба-бам!) 
операции по очистке данных, которые хорошо выполняются с помощью строковых 
методов библиотеки Pandas. Создание надежной рекомендательной системы для 
рецептов потребовало бы намного больше труда! Важной частью этой задачи было 
бы извлечение из каждого рецепта полного списка ингредиентов. К сожалению, 
разнообразие используемых форматов делает этот процесс весьма трудоемким. 
Как подсказывает нам Капитан Очевидность, в науке о данных очистка поступив -
ших из реального мира данных часто представляет собой основную часть работы, 
и библиотека Pandas предоставляет инструменты для эффективного выполнения 
этой задачи.
Работа с временными рядами
Библиотека Pandas была разработана в расчете на построение финансовых мо -
делей, так что, как вы могли и ожидать, она содержит весьма широкий набор 
инструментов для работы с датой, временем и индекси рованными по времени 
данными. Данные о дате и времени могут находиться в нескольких видах, которые 
мы сейчас обсудим.
 Метки даты/времени ссылаются на конкретные моменты времени (например, 
4 июля 2015 года в 07:00 утра).
 Временные интервалы и периоды ссылаются на отрезки времени между конкрет-
ными начальной и конечной точками (например, 2015 год). Периоды обычно 
представляют собой особый случай интервалов, с непересекающимися интер-
валами одинаковой длительности (например, 24-часовые пе риоды времени, 
составляющие сутки).
 Временная дельта (она же продолжительность) относится к отрезку времени 
конкретной длительности (например, 22,56 с).
В данном разделе мы расскажем, как работать с каждым из этих типов временных 
данных в библиотеке Pandas. Короткий раздел никоим образом не претендует на 
звание исчерпывающего руководства по имеющимся в Python или библиотеке 
Pandas инструментам работы с временными рядами. Он представляет собой обзор 
работы с временными рядами в общих чертах. Мы начнем с краткого обсуждения 
инструментов для работы с датой и временем в языке Python, прежде чем перейти 
непосредственно к обсуждению инструментов библиотеки Pandas. После перечис -
ления источников углубленной информации мы рассмотрим несколько кратких 
примеров работы с данными временных рядов в библиотеке Pandas.

--- СТРАНИЦА 231 ---
Работа с временными рядами 231
Дата и время в языке Python
В мире языка Python существует немало представлений дат, времени, временных 
дельт и интервалов времени. Хотя для приложений науки о данных наиболее удоб-
ны инструменты работы с временными рядами библиотеки Pandas, не помешает 
посмотреть на другие используемые в Python пакеты.
Нативные даты и время языка Python: пакеты datetime и dateutil
Базовые объекты Python для работы с датами и временем располагаются во встро-
енном пакете datetime. Его, вместе со сторонним модулем dateutil, можно исполь -
зовать для быстрого выполнения множества удобных операций над датами и вре-
менем. Например, можно вручную сформировать дату с помощью типа datetime:
In[1]: from datetime import datetime
 datetime(year=2015, month=7, day=4)
Out[1]: datetime.datetime(2015, 7, 4, 0, 0)

\1
In[2]: from dateutil import parser
 date = parser.parse("4th of July, 2015")
 date
Out[2]: datetime.datetime(2015, 7, 4, 0, 0)

\1
In[3]: date.strftime('%A')
Out[3]: 'Saturday'
В этой команде мы использовали для вывода даты один из стандартных кодов фор-
матирования строк ( "%A"), о котором можно прочитать в разделе strftime ( https://
docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior ) документации по 
пакету datetime ( https://docs.python.org/3/library/datetime.html) языка Python. Докумен -
тацию по другим полезным утилитам для работы с датой и временем можно найти 
в онлайн-документации пакета dateutil ( http://labix.org/python-dateutil). Не помешает 
также быть в курсе связанного с ними пакета pytz ( http://pytz.sourceforge.net), содер -
жащего инструменты для работы с частью данных временных рядов — часовыми 
поясами.
Сила пакетов datetime и dateutil заключается в их гибкости и удобном синтак-
сисе: эти объекты и их встроенные методы можно использовать для выполнения 
практически любой интересующей вас операции. Единственное, в чем они работают 
плохо, это работа с большими массивами дат и времени: подобно спискам числовых 

--- СТРАНИЦА 232 ---

\1ndas 
переменных языка Python, работающим неоптимально по сравнению с типизиро-
ванными числовыми массивами в стиле библиотеки NumPy, списки объектов даты/
времени Python работают с меньшей производительностью, чем типизированные 
массивы кодированных дат.
Типизированные массивы значений времени: тип datetime64 
библиотеки NumPy
Указанная слабая сторона формата даты/времени языка Python побудила команду 
разработчиков библиотеки NumPy добавить набор нативных типов данных вре -
менных рядов. Тип (dtype) datetime64 кодирует даты как 64-битные целые числа, 
так что представление массивов дат оказывается очень компактным. Для типа 
datetime64 требуется очень точно заданный формат входных данных:
In[4]: import numpy as np
 date = np.array('2015-07-04', dtype=np.datetime64)
 date
Out[4]: array(datetime.date(2015, 7, 4), dtype='datetime64[D]')

\1
In[5]: date + np.arange(12)
Out[5]:
array(['2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07',
 '2015-07-08', '2015-07-09', '2015-07-10', '2015-07-11',
 '2015-07-12', '2015-07-13', '2015-07-14', '2015-07-15'],
 dtype='datetime64[D]')
Поскольку datetime64-массивы библиотеки NumPy содержат данные одного тип а, 
подобные операции выполняются намного быстрее, чем если работать непосред -
ственно с объектами datetime языка Python, особенно если речь идет о больших 
массивах (мы рассматривали эту разновидность векторизации в разделе «Выпол -
нение вычислений над массивами библиотеки NumPy: универсальные функции» 
главы 2).
Важный нюанс относительно объектов datetime64 и timedelta64: они основаны 
на базовой единице времени (fundamental time unit). Поскольку объект datetime64 
ограничен точностью 64 бита, кодируемый им диапазон времени составляет эту 
базовую единицу, умноженную на 2 64 . Другими словами, datetime64 навязывает 
компромисс между разрешающей способностью по времени и максимальным про-
межутком времени.

\1
In[6]: np.datetime64('2015-07-04')
Out[6]: numpy.datetime64('2015-07-04')

\1
In[7]: np.datetime64('2015-07-0412:00')
Out[7]: numpy.datetime64('2015-07-04T12:00')

\1
In[8]: np.datetime64('2015-07-0412:59:59.50', 'ns')
Out[8]: numpy.datetime64('2015-07-04T12:59:59.500000000')
В табл. 3.6, взятой из документации по типу datetime64 библиотеки NumPy, пере-
числены доступные для использования коды форматирования, а также относитель-
ные и абсолютные промежутки времени, которые можно кодировать с их помощью.
Таблица 3.6. Описание кодов форматирования даты и времени
Код Значение Промежуток времени 
(относительный)
Промежуток времени (абсолютный)
Y Год ±9.2e18 лет [9.2e18 до н. э., 9.2e18 н. э.]
M Месяц ±7.6e17 лет [7.6e17 до н. э., 7.6e17 н. э.]
W Неделя ±1.7e17 лет [1.7e17 до н. э., 1.7e17 н. э.]
D День ±2.5e16 лет [2.5e16 до н. э., 2.5e16 н. э.]
h Час ±1.0e15 лет [1.0e15 до н. э., 1.0e15 н. э.]
m Минута ±1.7e13 лет [1.7e13 до н. э., 1.7e13 н. э.]
s Секунда ±2.9e12 лет [2.9e9 до н. э., 2.9e9 н. э.]
ms Миллисекунда ±2.9e9 лет [2.9e6 до н. э., 2.9e6 н. э.]
us Микросекунда ±2.9e6 лет [2 90301 до н. э., 2 94241 н. э.]
ns Наносекунда ±292 лет [1678 до н. э., 2262 н. э.]
ps Пикосекунда ±106 дней [1969 до н. э., 1970 н. э.]
fs Фемтосекунда ±2.6 часов [1969 до н. э., 1970 н. э.]
as Аттосекунда ±9.2 секунды [1969 до н. э., 1970 н. э.]

--- СТРАНИЦА 234 ---

\1ndas 
Удобное значение по умолчанию для типов данных, встречающихся в реальном 
мире, — datetime64[ns], позволяющее кодировать достаточный диапазон совре -
менных дат с высокой точностью.
Наконец, отметим, что, хотя тип данных datetime64 лишен некоторых недостатков 
встроенного типа данных datetime языка Python, ему недостает многих предо -
ставляемых datetime и особенно dateutil удобных методов и функций. Больше 
информации можно найти в документации по типу datetime64 библиотеки NumPy 
( http://docs.scipy.org/doc/numpy/reference/arrays.datetime.html).
Даты и время в библиотеке Pandas: избранное из лучшего
Библиотека Pandas предоставляет, основываясь на всех только что обсужда-
вшихся инструментах, объект Timestamp, сочетающий удобство использования 
datetime и dateutil с эффективным хранением и векторизованным интерфей -
сом типа numpy.datetime64 . Библиотека Pandas умеет создавать из нескольких 
таких объектов Timestamp объект класса DatetimeIndex , который можно ис -
пользовать для индексации данных в объектах Series или DataFrame. Можно 
применить инструменты библиотеки Pandas для воспроизведения вышепри -
веденной наглядной демонстрации. Можно выполнить синтаксический разбор 
строки с датой в гибком формате и воспользоваться кодами форматирования, 
чтобы вывести день недели:
In[9]: import pandas as pd
 date = pd.to_datetime("4th of July, 2015")
 date
Out[9]: Timestamp('2015-07-0400:00:00')
In[10]: date.strftime('%A')
Out[10]: 'Saturday'

\1
In[11]: date + pd.to_timedelta(np.arange(12), 'D')
Out[11]: DatetimeIndex(['2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07',
 '2015-07-08', '2015-07-09', '2015-07-10', '2015-07-11',
 '2015-07-12', '2015-07-13', '2015-07-14', '2015-07-15'],
 dtype='datetime64[ns]', freq=None)
В следующем разделе мы подробнее рассмотрим манипуляции над данными вре-
менных рядов с помощью предоставляемых библиотекой Pandas инструментов.

--- СТРАНИЦА 235 ---
Работа с временными рядами 235
Временные ряды библиотеки Pandas: индексация 
по времени
Инструменты для работы с временными рядами библиотеки Pandas особенно удоб -
ны при необходимости индексации данных по меткам даты/времени . Например, 
создадим объект Series с индексированными по времени данными:
In[12]: index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',
 '2015-07-04', '2015-08-04'])
 data = pd.Series([0, 1, 2, 3], index=index)
 data
Out[12]: 2014-07-04 0
 2014-08-04 1
 2015-07-04 2
 2015-08-04 3
 dtype: int64

\1
In[13]: data['2014-07-04':'2015-07-04']
Out[13]: 2014-07-04 0
 2014-08-04 1
 2015-07-04 2
 dtype: int64

\1
In[14]: data['2015']
Out[14]: 2015-07-04 2
 2015-08-04 3
 dtype: int64
Позднее мы рассмотрим еще примеры удобства индексации по датам. Но сначала 
изучим имеющиеся структуры данных для временных рядов.
Структуры данных для временных рядов 
библиотеки Pandas
В этом разделе мы рассмотрим основные структуры данных, предназначенные для 
работы с временными рядами.

--- СТРАНИЦА 236 ---

\1ndas 
 Для меток даты/времени библиотека Pandas предоставляет тип данных Timestamp. 
Этот тип является заменой для нативного типа данных datetime языка Python, он 
основан на более эффективном типе данных numpy.datetime64. Соответствующая 
индексная конструкция — DatetimeIndex.
 Для периодов времени библиотека Pandas предоставляет тип данных Period. 
Этот тип на основе типа данных numpy.datetime64 кодирует интервал времени 
фиксированной периодичности. Соответствующая индексная конструкция — 
PeriodIndex.
 Для временных дельт ( продолжительностей) библиотека Pandas предоставляет 
тип данных Timedelta. Timedelta — основанная на типе numpy.timedelta64 более 
эффективная замена нативного типа данных datetime.timedelta языка Python. 
Соответствующая индексная конструкция — TimedeltaIndex.
Самые базовые из этих объектов даты/времени — объекты Timestamp и Datetime-
Index. Хотя к ним и можно обращаться непосредственно, чаще используют функ -
цию pd.to_datetime() , умеющую выполнять синтаксический разбор широкого 
диапазона форматов. При передаче в функцию pd.to_datetime() отдельной 
даты она возвращает Timestamp, при передаче ряда дат по умолчанию возвращает 
DatetimeIndex:
In[15]: dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015',
 '2015-Jul-6', '07-07-2015', '20150708'])
 dates
Out[15]: DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-06', '2015-07-07',
 '2015-07-08'],
 dtype='datetime64[ns]', freq=None)
Любой объект DatetimeIndex можно с помощью функции to_period() преобра -
зовать в объект PeriodIndex, указав код для периодичности интервала. В данном 
случае мы использовали код 'D', означающий, что периодичность интервала — 
один день:
In[16]: dates.to_period('D')
Out[16]: PeriodIndex(['2015-07-03', '2015-07-04', '2015-07-06', '2015-07-07',
 '2015-07-08'],
 dtype='int64', freq='D')
Объект TimedeltaIndex создается, например, при вычитании одной даты из 
другой:
In[17]: dates - dates[0]
Out[17]:
TimedeltaIndex(['0 days', '1 days', '3 days', '4 days', '5 days'],
 dtype='timedelta64[ns]', freq=None)

--- СТРАНИЦА 237 ---
Работа с временными рядами 237
Регулярные последовательности: функция pd.date_range(). Чтобы облегчить 
создание регулярных последовательностей, библиотека Pandas предоставляет 
несколько функций: pd.date_range() — для меток даты/времени, pd.period_
range() — для периодов времени и pd.timedelta_range() — для временных дельт. 
Мы уже видели, что функции range() языка Python и np.arange() библиотеки 
NumPy преобразуют начальную точку, конечную точку и (необязательную) 
величину шага в последовательность. Аналогично функция pd.date_range() 
создает регулярную последовательность дат, принимая на входе начальную дату, 
конечную дату и необязательный код периодичности. По умолчанию период 
равен одному дню:
In[18]: pd.date_range('2015-07-03', '2015-07-10')
Out[18]: DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06',
 '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'],
 dtype='datetime64[ns]', freq='D')

\1
In[19]: pd.date_range('2015-07-03', periods=8)
Out[19]: DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06',
 '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'],
 dtype='datetime64[ns]', freq='D')

\1
In[20]: pd.date_range('2015-07-03', periods=8, freq='H')
Out[20]: DatetimeIndex(['2015-07-0300:00:00', '2015-07-0301:00:00',
 '2015-07-0302:00:00', '2015-07-0303:00:00',
 '2015-07-0304:00:00', '2015-07-0305:00:00',
 '2015-07-0306:00:00', '2015-07-0307:00:00'],
 dtype='datetime64[ns]', freq='H')
Для создания регулярных последовательностей значений периодов или временных 
дельт можно воспользоваться функциями pd.period_range() и pd.timedelta_
range(), напоминающими функцию date_range(). Вот несколько периодов времени 
длительностью в месяц:
In[21]: pd.period_range('2015-07', periods=8, freq='M')
Out[21]:
PeriodIndex(['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12',
 '2016-01', '2016-02'],
 dtype='int64', freq='M')

--- СТРАНИЦА 238 ---

\1ndas 

\1
In[22]: pd.timedelta_range(0, periods=10, freq='H')
Out[22]:
TimedeltaIndex(['00:00:00', '01:00:00', '02:00:00', '03:00:00', '04:00:00',
 '05:00:00', '06:00:00', '07:00:00', '08:00:00', '09:00:00'],
 dtype='timedelta64[ns]', freq='H')
Все эти операции требуют понимания кодов периодичности, приведенных в сле-
дующем разделе.
Периодичность и смещения дат
Периодичность или смещение даты — базовое понятие для инструментов библио-
теки Pandas, необходимых для работы с временными рядами. Аналогично уже 
продемонстрированным кодам D (день) и H (час) можно использовать коды для 
задания любой требуемой периодичности. В табл. 3.7 описаны основные суще -
ствующие коды.
Таблица 3.7. Список кодов периодичности библиотеки Pandas

\1
In[23]: pd.timedelta_range(0, periods=9, freq="2H30T")
Out[23]:
TimedeltaIndex(['00:00:00', '02:30:00', '05:00:00', '07:30:00', '10:00:00',
 '12:30:00', '15:00:00', '17:30:00', '20:00:00'],
 dtype='timedelta64[ns]', freq='150T')
Все эти короткие коды ссылаются на соответствующие экземпляры смещений 
даты/времени временных рядов библиотеки Pandas, которые можно найти в мо -
дуле pd.tseries.offsets. Например, можно непосредственно создать смещение 
в один рабочий день следующим образом:
In[24]: from pandas.tseries.offsets import BDay
 pd.date_range('2015-07-01', periods=5, freq=BDay())
Out[24]: DatetimeIndex(['2015-07-01', '2015-07-02', '2015-07-03', 
 '2015-07-06','2015-07-07'],
 dtype='datetime64[ns]', freq='B')
Дальнейшее обсуждение периодичности и смещений времени можно найти 
в разделе «Объекты DateOffset» ( http://pandas.pydata.org/pandas-docs/stable/timeseri-
es.html#dateoffset-objects ) онлайн-документации библиотеки Pandas.
Передискретизация, временные сдвиги и окна
Возможность использовать дату/время в качестве индексов для интуитивно понят-
ной организации данных и доступа к ним — немаловажная часть инструментария 

--- СТРАНИЦА 240 ---

\1ndas 
библиотеки Pandas по работе с временными рядами. При этом сохраняются общие 
преимущества использования индексированных данных (автоматическое выравни-
вание во время операций, интуитивно понятные срезы и доступ к данным и т. д.), но 
библиотека Pandas предоставляет еще несколько дополнительных операций специ -
ально для временных рядов.
Мы рассмотрим некоторые из них, воспользовавшись в качестве примера данны-
ми по курсам акций. Библиотека Pandas, будучи разработанной в значительной 
степени для работы с финансовыми данными, имеет для этой цели несколько 
весьма специфических инструментов. Например, сопутствующий Pandas пакет 
pandas-datareader (который можно установить с помощью команды conda install 
pandas-datareader) умеет импортировать финансовые данные из множества ис -
точников, включая Yahoo! Finance, Google Finance и другие. 

\1
In[25]: from pandas_datareader import data
 goog = data.DataReader('GOOG', start='2004', end='2016',
 data_source='google')
 goog.head()
Out[25]: Open High Low Close Volume
 Date
 2004-08-19 49.96 51.98 47.93 50.12 NaN
 2004-08-20 50.69 54.49 50.20 54.10 NaN
 2004-08-23 55.32 56.68 54.47 54.65 NaN
 2004-08-24 55.56 55.74 51.73 52.38 NaN
 2004-08-25 52.43 53.95 51.89 52.95 NaN

\1
In[26]: goog = goog['Close']

\1
In[27]: %matplotlib inline
 import matplotlib.pyplot as plt
 import seaborn; seaborn.set()
In[28]: goog.plot();
Передискретизация и изменение периодичности интервалов
При работе с данными временных рядов часто бывает необходимо переразбить 
их с использованием интервалов другой периодичности. Сделать это можно с по-
мощью метода resample() или гораздо более простого метода asfreq(). Основная 

--- СТРАНИЦА 241 ---
Работа с временными рядами 241
разница между ними заключается в том, что resample() выполняет агрегирование 
данных, а asfreq() — выборку данных.
Рассмотрим, что возвращают эти два метода для данных по ценам закрытия Google 
при понижающей дискретизации данных. Здесь мы выполняем передискретизацию 
данных на конец финансового года (рис. 3.6).
Рис. 3.5. График изменения цен акций Google с течением времени
Рис. 3.6. Передискретизация цен акций Google
In[29]: goog.plot(alpha=0.5, style='-')
 goog.resample('BA').mean().plot(style=':')

--- СТРАНИЦА 242 ---

\1ndas 
 goog.asfreq('BA').plot(style='--');
 plt.legend(['input', 'resample', 'asfreq'],
 loc='upper left');
Обратите внимание на различие: в каждой точке resample выдает среднее значение 
за предыдущий год, а asfreq — значение на конец года.
В случае повышающей дискретизации методы resample() и asfreq() в значительной 
степени идентичны, хотя доступных для использования параме тров у resample() 
гораздо больше. В данном случае оба этих метода по умолчанию оставляют зна -
чения интерполированных точек пустыми, то есть заполненными значениями NA . 
Аналогично обсуждавшейся выше функции pd.fillna() метод asfreq() принимает 
аргумент method, определяющий, откуда будут браться значения для таких точек. 

\1
In[30]: fig, ax = plt.subplots(2, sharex=True)
 data = goog.iloc[:10]
 data.asfreq('D').plot(ax=ax[0], marker='o')
 data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
 data.asfreq('D', method='ffill').plot(ax=ax[1], style='--o')
 ax[1].legend(["back-fill", "forward-fill"]);
Рис. 3.7. Сравнение интерполяции вперед (forward-fill interpolation) 
и назад (backward-fill interpolation)

--- СТРАНИЦА 243 ---
Работа с временными рядами 243
Верхний график представляет поведение по умолчанию: в выходные дни1 значения 
равны NA и отсутствуют на графике. Нижний график демонстрирует различия 
между двумя методиками заполнения пропусков: интерполяцией вперед (forward-
fill interpolation) и интерполяцией назад (back-fill interpolation).
Временные сдвиги
Еще одна распространенная операция с временными рядами — сдвиг данных во 
времени. В библиотеке Pandas есть два родственных метода для подобных вычис-
лений: shift() и tshift(). Разница между ними заключается в том, что shift() 
выполняет сдвиг данных, а tshift() — сдвиг индекса. В обоих случаях сдвиг задается 
кратным периоду.

\1
In[31]: fig, ax = plt.subplots(3, sharey=True)
 # задаем периодичность данных
 goog = goog.asfreq('D', method='pad')
 goog.plot(ax=ax[0])
 goog.shift(900).plot(ax=ax[1])
 goog.tshift(900).plot(ax=ax[2])
 # Легенды и пояснения
 local_max = pd.to_datetime('2007-11-05')
 offset = pd.Timedelta(900, 'D')
 ax[0].legend(['input'], loc=2)
 ax[0].get_xticklabels()[4].set(weight='heavy', color='red')
 ax[0].axvline(local_max, alpha=0.3, color='red')
 ax[1].legend(['shift(900)'], loc=2)
 ax[1].get_xticklabels()[4].set(weight='heavy', color='red')
 ax[1].axvline(local_max + offset, alpha=0.3, color='red')
 ax[2].legend(['tshift(900)'], loc=2)
 ax[2].get_xticklabels()[1].set(weight='heavy', color='red')
 ax[2].axvline(local_max + offset, alpha=0.3, color='red');

\1

\1ndas 
In[32]: ROI = 100 * (goog.tshift(-365) / goog - 1)
 ROI.plot()
 plt.ylabel('% Return on Investment'); # Прибыль от вложений
Рис. 3.8. Сравнение shift и tshift
Рис. 3.9. Прибыль на текущий день от вложений в акции Google

--- СТРАНИЦА 245 ---
Работа с временными рядами 245
Это помогает увидеть общие тренды акций Google: до сих пор наиболее благопри -
ятным для инвестиций в акции Google был (что неудивительно) момент вскоре по -
сле первоначального их размещения на рынке, а также в середине экономической 
рецессии 2009 года.
Скользящие окна
Скользящие статистические показатели — третья из реализованных в библиотеке 
Pandas разновидностей операций, предназначенных для временных рядов. Рабо-
тать с ними можно с помощью атрибута rolling() объектов Series и DataFrame, 
возвращающего представление, подобное тому, с которым мы сталкивались при 
выполнении операции groupby (см. раздел «Агрегирование и группировка» данной 
главы). Это скользящее представление предоставляет по умолчанию несколько 
операций агрегирования.

\1
In[33]: rolling = goog.rolling(365, center=True)
 data = pd.DataFrame({'input': goog,
 'one-year rolling_mean': rolling.mean(),
 'one-year rolling_std': rolling.std()})
 ax = data.plot(style=['-', '--', ':'])
 ax.lines[0].set_alpha(0.3)
Рис. 3.10. Скользящие статистические показатели для цен на акции Google
Как и в случае операций groupby , можно использовать методы aggregate() 
и apply() для вычисления пользовательских скользящих показателей.

--- СТРАНИЦА 246 ---

\1ndas 
Где найти дополнительную информацию
В данном разделе приведена лишь краткая сводка некоторых наиболее важных 
возможностей инструментов для работы с временными рядами библиотеки Pandas. 
Более развернутое обсуждение этой темы можно найти в разделе Time Series/Date 
(«Временные ряды/даты») онлайн-документации библиотеки Pandas ( http://pan-
das.pydata.org/pandas-docs/stable/timeseries.html).
Еще один великолепный источник информации — руководство Python for Data 
Analysis издательства O’Reilly ( http://shop.oreilly.com/product/063 69200 23784.do). Хотя 
ему уже несколько лет, это бесценный источник информации по использованию 
библиотеки Pandas. В частности, в книге сделан особый акцент на применении 
инструментов временных рядов в контексте бизнеса и финансов и уделено боль-
ше внимания конкретным деталям бизнес-календаря, работе с часовыми поясами 
и связанным с этим вопросам.
Вы также можете воспользоваться справочной функциональностью оболочки 
IPython для изучения и экспериментов с другими параметрами, имеющимися 
у обсуждавшихся здесь функций и методов. Я считаю, что это оптимальный способ 
изучения какого-либо нового инструмента языка Python.
Пример: визуализация количества велосипедов в Сиэтле
В качестве более сложного примера работы с данными временных рядов рас -
смотрим подсчет количества велосипедов на Фримонтском мосту в Сиэтле. Эти 
данные поступают из автоматического счетчика велосипедов, установленного 
в конце 2012 года с индуктивными датчиками на восточной и западной боковых 
дорожках моста. Сведения о почасовом количестве велосипедов можно скачать 
по адресу http://data.seattle.gov/; вот прямая ссылка на набор данных: https://data.seat t- 
le.gov/Transportation/Fremont-Bridge-Hourly-Bicycle-Counts-by-Month-Octo/65db-xm6k .

\1
In[34]:
# !curl -o FremontBridge.csv
# https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD
После скачивания набора данных можно воспользоваться библиотекой Pandas для 
чтения CSV-файла в объект DataFrame. Можно указать, что в качестве индекса мы 
хотим видеть объекты Date и чтобы выполнялся автоматический синтаксический 
разбор этих дат:
In[35]:
data = pd.read_csv('FremontBridge.csv', index_col='Date', parse_dates=True)
data.head()
Out[35]: Fremont Bridge West Sidewalk \\

--- СТРАНИЦА 247 ---
Работа с временными рядами 247
 Date
 2012-10-0300:00:00 4.0
 2012-10-0301:00:00 4.0
 2012-10-0302:00:00 1.0
 2012-10-0303:00:00 2.0
 2012-10-0304:00:00 6.0
 Fremont Bridge East Sidewalk
 Date
 2012-10-0300:00:00 9.0
 2012-10-0301:00:00 6.0
 2012-10-0302:00:00 1.0
 2012-10-0303:00:00 3.0
 2012-10-0304:00:00 1.0

\1
In[36]: data.columns = ['West', 'East']
 data['Total'] = data.eval('West + East')

\1
In[37]: data.dropna().describe()
Out[37]: West East Total
 count 33544.000000 33544.000000 33544.000000
 mean 61.726568 53.541706 115.268275
 std 83.210813 76.380678 144.773983
 min 0.000000 0.000000 0.000000
 25% 8.000000 7.000000 16.000000
 50% 33.000000 28.000000 64.000000
 75% 80.000000 66.000000 151.000000
 max 825.000000 717.000000 1186.000000
Визуализация данных
Мы можем почерпнуть полезную информацию из этого набора данных, визуали-
зировав его. Начнем с построения графика исходных данных (рис. 3.11).
In[38]: %matplotlib inline
 import seaborn; seaborn.set()
In[39]: data.plot()
 plt.ylabel('Hourly Bicycle Count'); # Количество велосипедов по часам

\1

--- СТРАНИЦА 248 ---

\1ndas 
In[40]: weekly = data.resample('W').sum()
 weekly.plot(style=[':', '--', '-'])
 plt.ylabel('Weekly bicycle count'); # Количество велосипедов еженедельно
Рис. 3.11. Количество велосипедов за каждый час на Фримонтском мосту в Сиэтле
Рис. 3.12. Количество велосипедов, пересекающих Фримонтский мост в Сиэтле, 
с шагом одна неделя

--- СТРАНИЦА 249 ---
Работа с временными рядами 249
Это демонстрирует нам некоторые интересные сезонные тренды: как и сле -
довало ожидать, летом люди ездят на велосипедах больше, чем зимой, и даже 
в пределах каждого из сезонов велосипеды используются с разной интенсив -
ностью в разные недели (вероятно, в зависимости от погоды; см. раздел «За -
глянем глубже: линейная регрессия» главы 5, в котором будем рассматривать 
этот вопрос).
Еще один удобный способ агрегирования данных — вычисление скользящего 
среднего с помощью функции pd.rolling_mean(). Здесь мы вычисляем для наших 
данных скользящее среднее за 30 дней, центрируя при этом окно (рис. 3.13):
In[41]: daily = data.resample('D').sum()
 daily.rolling(30, center=True).sum().plot(style=[':', '--', '-'])
 plt.ylabel('mean hourly count'); # Среднее количество по часам

\1
In[42]:
daily.rolling(50, center=True,
 win_type='gaussian').sum(std=10).plot(style=[':', '--', '-']);

--- СТРАНИЦА 250 ---

\1ndas 

\1
In[43]: by_time = data.groupby(data.index.time).mean()
 hourly_ticks = 4 * 60 * 60 * np.arange(6)
 by_time.plot(xticks=hourly_ticks, style=[':', '--', '-']);
Почасовое движение транспорта представляет собой строго бимодальное рас -
пределение с максимумами в 08:00 утра и 05:00 вечера. Вероятно, это свидетель -
ствует о существенном вкладе маятниковой миграции1 через мост. В пользу этого 
говорят и различия между значениями с западной боковой дорожки (обычно 
используемой при движении в деловой центр Сиэтла) с более выраженными 
утренними максимумами и значениями с восточной боковой дорожки (обычно 
используемой при движении из делового центра Сиэтла) с более выраженными 
вечерними максимумами.

\1n[44]: by_weekday = data.groupby(data.index.dayofweek).mean()
 by_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 
 'Fri', 'Sat', 'Sun']
 by_weekday.plot(style=[':', '--', '-']);
Рис. 3.16. Среднее количество велосипедов по дням

--- СТРАНИЦА 252 ---

\1ndas 

\1
In[45]: weekend = np.where(data.index.weekday < 5, 'Weekday', 'Weekend')
 by_time = data.groupby([weekend, data.index.time]).mean()

\1
In[46]: import matplotlib.pyplot as plt
 fig, ax = plt.subplots(1, 2, figsize=(14, 5))
 by_time.ix['Weekday'].plot(ax=ax[0], title='Weekdays',
 xticks=hourly_ticks, style=[':', '--', '-'])
 by_time.ix['Weekend'].plot(ax=ax[1], title='Weekends',
 xticks=hourly_ticks, style=[':', '--', '-']);
Результат оказался очень интересным: мы видим бимодальный паттерн, связанный 
с поездками на работу в город на протяжении рабочей недели, и унимодальный 
 паттерн, связанный с досугом/отдыхом во время выходных. Было бы интересно 
дальше покопаться в этих данных и изучить влияние погоды, температуры, времени 
года и других факторов на паттерны поездок в город на велосипедах. Дальнейшее об-
суждение этих вопросов см. в сообщении «Действительно ли в Сиэтле наблюдается 
оживление в сфере поездок на велосипедах?» ( https://jakevdp.github.io/blog/2014/06/10/
is-seattle-really-seeing-an-uptick-in-cycling/) из моего блога, в котором используется под -
множество этих данных. Мы также вернемся к этому набору данных в контексте 
моделирования в разделе «Заглянем глубже: линейная регрессия» главы 5.
Увеличение производительности библиотеки 
Pandas: eval() и query()
Основные возможности стека PyData основываются на умении библиотек NumPy 
и Pandas передавать простые операции на выполнение программам на языке C по -
средством интуитивно понятного синтаксиса: примерами могут послужить век -
торизованные/транслируемые операции в библиотеке NumPy, а также операции 
группировки в библиотеке Pandas. Хотя эти абстракции весьма производительны 
и эффективно работают для многих распространенных сценариев использова -
ния, они зачастую требуют создания вр еменных вспомогательных объектов, что 
приводит к чрезмерным накладным расходам как процессорного времени, так 
и оперативной памяти.

--- СТРАНИЦА 253 ---
Увеличение производительности библиотеки Pandas: eval() и query() 253
Рис. 3.17. Среднее количество велосипедов по часам, в рабочие и выходные дни

--- СТРАНИЦА 254 ---

\1ndas 
По состоянию на версию 0.13 (выпущенную в январе 2014 года) библиотека Pandas 
включает некоторые экспериментальные инструменты, позволяющие обращаться 
к работающим со скоростью написанных на языке C операциям без выделения су-
щественных объемов памяти на промежуточные массивы. Эти утилиты — функции 
eval() и query(), основанные на пакете Numexpr ( https://github.com/pydata/numexpr). 
Мы рассмотрим их использование и приведем некоторые эмпирические правила, 
позволяющие решить, имеет ли смысл их применять.
Основания для использования функций query() и eval(): 
составные выражения
Библиотеки NumPy и Pandas поддерживают выполнение быстрых векторизован-
ных операций; например, при сложении элементов двух массивов:
In[1]: import numpy as np
 rng = np.random.RandomState(42)
 x = rng.rand(1E6)
 y = rng.rand(1E6)
 %timeit x + y 100 loops, best of 3: 3.39 ms per loop
Как уже обсуждалось в разделе «Выполнение вычислени й над массивами библи -
отеки NumPy: универсальные функции» главы 2, такая операция выполняется 
гораздо быстрее, чем сложение с помощью цикла или спискового включения 
языка Python:
In[2]:
%timeit np.fromiter((xi + yi for xi, yi in zip(x, y)),
 dtype=x.dtype, count=len(x))
1 loop, best of 3: 266 ms per loop

\1
In[3]: mask = (x > 0.5) & (y < 0.5)

\1
In[4]: tmp1 = (x > 0.5)
 tmp2 = (y < 0.5)
 mask = tmp1 & tmp2
Другими словами, для каждого промежуточного шага явным образом выделяет -
ся оперативная память . Если массивы x и y очень велики, это может привести 
к значительным накладным расходам оперативной памяти и процессорного 
времени.

--- СТРАНИЦА 255 ---
Увеличение производительности библиотеки Pandas: eval() и query() 255
Библиотека Numexpr позволяет вычислять подобные составные выражения по -
элементно, не требуя выделения памяти под промежуточные массивы целиком. 
В документации библиотеки Numexpr ( https://github.com/pydata/numexpr) приведено 
больше подробностей, но пока достаточно будет сказать, что функции этой библио-
теки принимают на входе строку , содержащую выражение в стиле библиотеки 
NumPy, которое требуется вычислить:
In[5]: import numexpr
 mask_numexpr = numexpr.evaluate('(x > 0.5) & (y < 0.5)')
 np.allclose(mask, mask_numexpr)
Out[5]: True
Преимущество заключается в том, что библиотека Numexpr вычисляет выраже -
ние, не используя полноразмерных вр еменных массивов, а потому оказывается 
намного более эффективной, чем NumPy, особенно в случае больших массивов. 
Инструменты query() и eval(), которые мы будем обсуждать, идеологически схожи 
и используют пакет Numexpr.
Использование функции pandas.eval() для эффективных 
операций
Функция eval() библиотеки Pandas применяет строковые выражения для эффек -
тивных вычислительных операций с объектами DataFrame. Например, рассмотрим 
следующие объекты DataFrame:
In[6]: import pandas as pd
 nrows, ncols = 100000, 100
 rng = np.random.RandomState(42)
 df1, df2, df3, df4 = (pd.DataFrame(rng.rand(nrows, ncols))
 for i in range(4))
Для вычисления суммы всех четырех объектов DataFrame при стандартном подходе 
библиотеки Pandas можно написать сумму:
In[7]: %timeit df1 + df2 + df3 + df4
10 loops, best of 3: 87.1 ms per loop

\1
In[8]: %timeit pd.eval('df1 + df2 + df3 + df4')
10 loops, best of 3: 42.2 ms per loop

\1

--- СТРАНИЦА 256 ---

\1ndas 
In[9]: np.allclose(df1 + df2 + df3 + df4,
 pd.eval('df1 + df2 + df3 + df4'))
Out[9]: True
Поддерживаемые функцией pd.eval() операции. На момент выпуска версии 0.16 
библиотеки Pandas функция pd.eval() поддерживает широкий спектр операций. 

\1
In[10]: df1, df2, df3, df4, df5 = (pd.DataFrame(rng.randint(0, 1000, (100, 
3)))
 for i in range(5))

\1
In[11]: result1 = -df1 * df2 / (df3 + df4) - df5
 result2 = pd.eval('-df1 * df2 / (df3 + df4) - df5')
 np.allclose(result1, result2)
Out[11]: True

\1
In[12]: result1 = (df1 < df2) & (df2 <= df3) & (df3 != df4)
 result2 = pd.eval('df1 < df2 <= df3 != df4')
 np.allclose(result1, result2)
Out[12]: True

\1
In[13]: result1 = (df1 < 0.5) & (df2 < 0.5) | (df3 < df4)
 result2 = pd.eval('(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)')
 np.allclose(result1, result2)
Out[13]: True
Кроме того, она допускает использование литералов and и or в булевых выраже-
ниях:
In[14]: result3 = pd.eval('(df1 < 0.5) and (df2 < 0.5) or (df3 < df4)')
 np.allclose(result1, result3)
Out[14]: True
Атрибуты объектов и индексы. Функция pd.eval() поддерживает доступ к атрибу-
там объектов с помощью синтаксиса obj.attr и к индексам посредством синтаксиса 
obj[index]:

--- СТРАНИЦА 257 ---
Увеличение производительности библиотеки Pandas: eval() и query() 257
In[15]: result1 = df2.T[0] + df3.iloc[1]
 result2 = pd.eval('df2.T[0] + df3.iloc[1]')
 np.allclose(result1, result2)
Out[15]: True
Другие операции. Другие операции, например вызовы функций, условные вы -
ражения, циклы и другие, более сложные конструкции, пока в функции pd.eval() 
не реализованы. При необходимости выполнения подобных сложных видов вы -
ражений можно воспользоваться самой библиотекой Numexpr.
Использование метода DataFrame.eval() для 
выполнения операций по столбцам
У объектов DataFrame существует метод eval(), работающий схожим образом с вы-
сокоуровневой функцией pd.eval() из библиотеки Pandas. Преимущество метода 
eval() заключается в возможности ссылаться на столбцы по имени. Возьмем для 
примера следующий маркированный массив:
In[16]: df = pd.DataFrame(rng.rand(1000, 3), columns=['A', 'B', 'C'])
 df.head()
Out[16]: A B C 
 0 0.375506 0.406939 0.069938
 1 0.069087 0.235615 0.154374
 2 0.677945 0.433839 0.652324
 3 0.264038 0.808055 0.347197
 4 0.589161 0.252418 0.557789

\1
In[17]: result1 = (df['A'] + df['B']) / (df['C'] - 1)
 result2 = pd.eval("(df.A + df.B) / (df.C - 1)")
 np.allclose(result1, result2)
Out[17]: True

\1
In[18]: result3 = df.eval('(A + B) / (C - 1)')
 np.allclose(result1, result3)
Out[18]: True
Обратите внимание, что мы обращаемся с названиями столбцов в вычисляемом 
выражении как с переменными и получаем желаемый результат.

--- СТРАНИЦА 258 ---

\1ndas 

\1
In[19]: df.head()
Out[19]: A B C
 0 0.375506 0.406939 0.069938
 1 0.069087 0.235615 0.154374
 2 0.677945 0.433839 0.652324
 3 0.264038 0.808055 0.347197
 4 0.589161 0.252418 0.557789

\1
In[20]: df.eval('D = (A + B) / C', inplace=True)
 df.head()
Out[20]: A B C D
 0 0.375506 0.406939 0.069938 11.187620
 1 0.069087 0.235615 0.154374 1.973796
 2 0.677945 0.433839 0.652324 1.704344
 3 0.264038 0.808055 0.347197 3.087857
 4 0.589161 0.252418 0.557789 1.508776

\1
In[21]: df.eval('D = (A - B) / C', inplace=True)
 df.head()
Out[21]: A B C D
 0 0.375506 0.406939 0.069938 -0.449425
 1 0.069087 0.235615 0.154374 -1.078728
 2 0.677945 0.433839 0.652324 0.374209
 3 0.264038 0.808055 0.347197 -1.566886
 4 0.589161 0.252418 0.557789 0.603708
Локальные переменные в методе DataFrame.eval()
Метод DataFrame.eval() поддерживает дополнительный синтаксис для работы 
с локальными переменными языка Python. Взгляните на следующий фрагмент кода:
In[22]: column_mean = df.mean(1)
 result1 = df['A'] + column_mean
 result2 = df.eval('A + @column_mean')
 np.allclose(result1, result2)
Out[22]: True

--- СТРАНИЦА 259 ---
Увеличение производительности библиотеки Pandas: eval() и query() 259
Символ @ отмечает имя переменной, а не имя столбца, позволяя тем самым эффек -
тивно вычислять значение выражений с использованием двух пространств имен: 
пространства имен столбцов и пространства имен объектов Python. Обратите вни-
мание, что этот символ @ поддерживается лишь методом DataFrame.eval(), но не 
функцией pandas.eval(), поскольку у функции pandas.eval() есть доступ только 
к одному пространству имен (языка Python).

\1
In[23]: result1 = df[(df.A < 0.5) & (df.B < 0.5)]
 result2 = pd.eval('df[(df.A < 0.5) & (df.B < 0.5)]')
 np.allclose(result1, result2)
Out[23]: True

\1
In[24]: result2 = df.query('A < 0.5 and B < 0.5')
 np.allclose(result1, result2)
Out[24]: True

\1
In[25]: Cmean = df['C'].mean()
 result1 = df[(df.A < Cmean) & (df.B < Cmean)]
 result2 = df.query('A < @Cmean and B < @Cmean')
 np.allclose(result1, result2)
Out[25]: True
Производительность: когда следует использовать эти 
функции
В процессе принятия решения применять ли эти функции обратите внимание на 
два момента: процессорное время и объем используемой памяти . Предсказать объ -
ем используемой памяти намного проще. Как уже упоминалось, все составные 

--- СТРАНИЦА 260 ---

\1ndas 
выражения с применением массивов NumPy или объектов DataFrame библиотеки 
Pandas приводят к неявному созданию вр еменных массивов. Например, вот это:
In[26]: x = df[(df.A < 0.5) & (df.B < 0.5)]
приблизительно соответствует следующему:
In[27]: tmp1 = df.A < 0.5
 tmp2 = df.B < 0.5
 tmp3 = tmp1 & tmp2
 x = df[tmp3]

\1
In[28]: df.values.nbytes
Out[28]: 32000
eval() будет работать быстрее, если вы не используете всю доступную в системе 
оперативную память. Основную роль играет отношение размера вр еменных объ-
ектов DataFrame по сравнению с размером L1 или L2 кэша процессора в системе 
(в 2016 году он составляет несколько мегабайтов). eval() позволяет избежать по-
тенциально медленного перемещения значений между различными кэшами памяти 
в том случае, когда это отношение намного больше 1. Я обнаружил, что на практике 
различие в скорости вычислений между традиционными методами и методом eval/
query обычно довольно незначительно. Напротив, традиционный метод работает 
быстрее для маленьких массивов! Преимущество метода eval/query заключается 
в экономии оперативной памяти и иногда — в более понятном синтаксисе.
Мы рассмотрели большинство нюансов работы с методами eval() и query(), до -
полнительную информацию можно найти в документации по библиотеке Pandas. 
В частности, можно задавать для работы этих запросов различные синтаксические 
анализаторы и механизмы. Подробности — в разделе Enhancing Performance («Повы -
шение производительности», http://pandas.pydata.org/pandas-docs/dev/enhancingperf.html).
Дополнительные источники информации
В этой главе мы охватили большую часть основ эффективного использования би-
блиотеки Pandas для анализа данных. Чтобы изучить библиотеку Pandas глубже, 
я бы рекомендовал обратиться к следующим источникам информации.
 Онлайн-документация библиотеки Pandas ( http://pandas.pydata.org/ ). Это все -
сторонний источник документации по данному пакету. Хотя примеры в до -
кументации обычно представляют собой небольшие сгенерированные наборы 

--- СТРАНИЦА 261 ---
Дополнительные источники информации 261
данных, параметры описываются во всей полноте, что обычно очень удобно для 
понимания того, как использовать различные функции.
 Python for Data Analysis 1 ( http://bit.ly/python-for-data-analysis). В книге, написанной 
Уэсом Маккинни, углубленно рассматриваются инструменты для работы с вре-
менными рядами, обеспечившие его как финансового аналитика средствами 
к существованию. В книге также приведено множество интересных примеров 
применения Python для получения полезной информации из реальных наборов 
данных. Не забывайте, что этой книге уже несколько лет и с того времени в паке-
те Pandas появилось немало новых возможностей, не охваченных ею (впрочем, 
в 2017 году 2 ожидается новое издание).
 Обсуждение библиотеки Pandas на форуме Stack Overflow. У библиотеки Pandas 
столько пользователей, что любой вопрос, который только мож ет у вас возник-
нуть, вероятно, уже был задан на форуме Stack Overflow и на него получен от-
вет. При использовании библиотеки Pandas вашими лучшими друзьями станут 
поисковые системы. Просто введите свой вопрос, сформулируйте проблему 
или ошибку, с которой вы столкнулись, — более чем вероятно, что вы найдете 
решение на одной из страниц сайта Stack Overflow.
 Библиотека Pandas на сайте PyVideo ( http://pyvideo.org/tag/pandas/). Многие фору -
мы, начиная от PyCon, SciPy и до PyData, выпускали руководства от разработ -
чиков и опытных пользователей библиотеки Pandas. Презентаторы, создающие 
руководства PyCon, наиболее опытные и профессиональные.
Надеюсь, что с этими источниками информации в сочетании с полученной в данной 
главе демонстрацией возможностей вы будете готовы справиться с помощью би -
блиотеки Pandas с любой задачей по анализу данных, какая вам только встретится!

\1n и анализ данных. — М.: ДМК-Пресс, 2015. — 482 с.

\1n, предназначенный 
для реализации возможности интерактивного построения с помощью утилиты 
gnuplot графиков в стиле MATLAB из командной строки IPython. Создатель обо-
лочки IPython Фернандо Перес в этот момент был занят завершением написания 
диссертации, он сообщил Джону, что в ближайшие несколько месяцев у него не 
будет времени на анализ патча. Хантер принял это как благословение на самосто-
ятельную разработку — так родился пакет Matplotlib, версия 0.1 которого была 
выпущена в 2003 году. Институт исследований космоса с помощью космического 
телескопа (Space Telescope Science Institute, занимающийся управлением теле -
скопом «Хаббл») финансово поддержал разработку пакета Matplotlib и обеспечил 
расширение его возможностей, избрав в качестве пакета для формирования гра -
фических изображений.
Одна из важнейших возможностей пакета Matplotlib — хорошая совместимость 
с множеством операционных систем и графических прикладных частей. Matplotlib 
поддерживает десятки прикладных частей и типов вывода, а значит, можно пола-
гаться на него независимо от используемой операционной системы или требуемого 
формата вывода. Самая сильная сторона пакета Matplotlib — кросс-платформенный 
подход типа «все для всех», который привел к росту по льзователей, что, в свою 
очередь, стало причиной появления большого числа активных разработчиков, уве-
личения возможностей инструментов пакета Matplotlib и его распространенности 
в мире научных вычислений на языке Python.
В последние годы интерфейс и стиль библиотеки Matplotlib начали несколько уста -
ревать. На фоне новых утилит, таких как ggplot и gg vis в языке R, а также наборов 
веб-инструментов визуализации, основанных на холстах D3js и HTML5, она кажется 

--- СТРАНИЦА 263 ---
Общие советы по библиотеке Matplotlib 263
 неуклюжей и старомодной. Тем не менее, я полагаю, нельзя игнорировать возмож-
ности библиотеки Matplotlib — надежного кросс-платформенного графического ме -
ханизма. Свежие версии Matplotlib упрощают настройку новых глобальных стилей 
вывода графики (см. раздел «Пользовательские настройки Matplotlib: конфигурации 
и таблицы стилей» данной главы). Разрабатываются новые пакеты, предназначенные 
для работы с ней через более современные и «чистые» API, например Seaborn (см. раз -
дел «Визуализация с помощью библиотеки Seaborn» этой главы), ggplot ( http://yhat.
github.io/ggplot), HoloViews ( http://holoviews.org/), Altair ( http://altair-viz.github.io/) и даже 
саму библиотеку Pandas можно использовать в качеств е адаптеров для API Matplotlib. 
Однако даже при наличии подобных адаптеров полезно разобраться в синтаксисе 
Matplotlib для настройки вывода итогового графика. Исходя из этого, я считаю, что 
сама библиотека Matplotlib остается жизненно важной частью стека визуализации 
данных, даже если новые инструменты приводят к тому, что сообщество пользовате-
лей постепенно отходит от непосредственного использования API Matplotlib.
Общие советы по библиотеке Matplotlib
Прежде чем погрузиться в подробности создания визуализаций с помощью 
Matplotlib, расскажу несколько полезных вещей про этот пакет.
Импорт matplotlib
Аналогично тому, как мы использовали сокращение np для библиотеки NumPy 
и сокращение pd для библиотеки Pandas, мы будем применять стандартные сокра -
щения для импортов библиотеки Matplotlib:
In[1]: import matplotlib as mpl
 import matplotlib.pyplot as plt

\1
In[2]: plt.style.use('classic')
В данном разделе мы будем настраивать этот стиль по мере необходимости. Об -
ратите внимание, что таблицы стилей поддерживаются версией 1.5 библиоте -
ки Matplotlib. В более ранних версиях доступен только стиль по умолчанию. 
Дальнейшую информацию о таблицах стилей см. в разделе «Пользовательские 
настройки Matplotlib: конфигурации и таблицы стилей» этой главы.

--- СТРАНИЦА 264 ---

\1n;
 в блокноте IPython.

\1
# ------- файл: myplot.py ------
import matplotlib.pyplot as plt
import numpy as np
x = np.linspace(0, 10, 100)
plt.plot(x, np.sin(x))
plt.plot(x, np.cos(x))
plt.show()

\1
$ python myplot.py
Команда plt.show() выполняет «под капотом» много разной работы, так как ей 
необходимо взаимодействовать с интерактивной графической прикладной частью 
вашей системы. Детали этой операции различаются в зависимости от операцион -
ной системы и конкретной версии, но библиотека Matplotlib делает все возможное, 
чтобы скрыть от вас эти детали.
Одно важное замечание: команду plt.show() следует использовать только один раз 
за сеанс работы с Python, и чаще всего ее можно увидеть в самом конце сценария. 
Выполнение нескольких команд show() может привести к непредсказуемому по-
ведению в зависимости от прикладной части, так что лучше избегать этого.

--- СТРАНИЦА 265 ---
Общие советы по библиотеке Matplotlib 265
Построение графиков из командной оболочки IPython
Очень удобно использовать Matplotlib интерактивно из командной оболочки 
IPython (см. главу 1). Оболочка IPython будет отлично работать с библиотекой 
Matplotlib, если перевести ее в режим Matplotlib. Для активизации этого режима 
после запуска IPython можно воспользоваться «магической» командой %matplotlib:
In [1]: %matplotlib
Using matplotlib backend: TkAgg
In [2]: import matplotlib.pyplot as plt
После любая команда plot приведет к открытию окна графика с возможностью 
выполнения дальнейших команд для его изменения. Некоторые изменения (на -
пример, модификация свойств уже нарисованных линий) не будут отрисовываться 
автоматически. Чтобы добиться этого, воспользуйтесь командой plt.draw(). Вы-
полнять команду plt.show() в режиме Matplotlib не обязательно.
Построение графиков из блокнота IPython
Блокнот IPython — браузерный интерактивный инструмент для анализа данных, 
допускающий совмещение комментариев, кода, графики, элементов HTML и мно -
гого другого в единый исполняемый документ (см. главу 1).
Интерактивное построение графиков в блокноте IPython возможно с помощью 
команды %matplotlib, работает аналогично командной оболочке IPython. В блок-
ноте IPython у вас появляется возможность включения графики непосредственно 
в блокнот с двумя возможными альтернативами:
 использование команды %matplotlib notebook приведет к включению в блокнот 
интерактивных графиков;
 выполнение команды %matplotlib inline приведет к включению в блокнот 
статических изображений графиков.
В книге мы будем использовать команду %matplotlib inline:
In[3]: %matplotlib inline
После выполнения этой команды (которое нужно произвести только один раз за 
сеанс/для одного ядра Python) все создающие графики блоки в блокноте будут 
включать PNG-изображения итогового графика (рис. 4.1):
In[4]: import numpy as np
 x = np.linspace(0, 10, 100)
 fig = plt.figure()
 plt.plot(x, np.sin(x), '-')
 plt.plot(x, np.cos(x), '--');

--- СТРАНИЦА 266 ---

\1n[5]: fig.savefig('my_figure.png')
В текущем рабочем каталоге появился файл с названием my_figure.png:
In[6]: !ls -lh my_figure.png
-rw-r--r-- 1 jakevdp staff 16K Aug 1110:59 my_figure.png
Чтобы убедиться, что содержимое этого файла соответствует нашим ожиданиям, 
воспользуемся объектом Image оболочки IPython для отображения его содержи -
мого (рис. 4.2):
In[7]: from IPython.display import Image
 Image('my_figure.png')
Рис. 4.2. Простой график в виде PNG

--- СТРАНИЦА 267 ---
Два интерфейса по цене одного 267
Команда savefig() определяет формат файла, исходя из расширения заданного 
имени файла. В зависимости от установленной в вашей системе прикладной части 
может поддерживаться множество различных форматов файлов. Вывести список 
поддерживаемых форматов файлов для вашей системы вы можете с помощью 
следующего метода объекта canvas рисунка:
In[8]: fig.canvas.get_supported_filetypes()
Out[8]: {'eps': 'Encapsulated Postscript',
 'jpeg': 'Joint Photographic Experts Group',
 'jpg': 'Joint Photographic Experts Group',
 'pdf': 'Portable Document Format',
 'pgf': 'PGF code for LaTeX',
 'png': 'Portable Network Graphics',
 'ps': 'Postscript',
 'raw': 'Raw RGBA bitmap',
 'rgba': 'Raw RGBA bitmap',
 'svg': 'Scalable Vector Graphics',
 'svgz': 'Scalable Vector Graphics',
 'tif': 'Tagged Image File Format',
 'tiff': 'Tagged Image File Format'}
Обратите внимание, что при сохранении рисунка не обязательно использовать 
команду plt.show() или другие команды, обсуждавшиеся ранее.
Два интерфейса по цене одного
Два интерфейса библиотеки Matplotlib (удобный MATLAB-подобный интерфейс, 
основанный на сохранении состояния, и обладающий б ольшими возможностями 
объектно-ориентированный интерфейс) — свойство, которое потенциально может 
привести к путанице. Рассмотрим вкратце различия между ними.
Интерфейс в стиле MATLAB
Библиотека Matplotlib изначально была написана как альтернативный вариант 
(на языке Python) для пользователей пакета MATLAB, и значительная часть ее 
синтаксиса отражает этот факт. MATLAB-подобные инструменты содержатся 
в интерфейсе pyplot ( plt). Например, следующий код, вероятно, выглядит до -
вольно знакомо пользователям MATLAB (рис. 4.3):
In[9]: plt.figure() # Создаем рисунок для графика
 # Создаем первую из двух областей графика и задаем текущую ось
 plt.subplot(2, 1, 1) # (rows, columns, panel number)
 plt.plot(x, np.sin(x))

--- СТРАНИЦА 268 ---

\1np.cos(x));
Важно отметить, что этот интерфейс сохраняет состояние: он отслеживает текущий 
рисунок и его оси координат и для него выполняет все команды plt. Получить 
на них ссылки можно с помощью команд plt.gcf() (от англ. get current figure — 
«получить текущий рисунок») и plt.gca() (от англ. get current axes — «получить 
текущие оси координат»).

\1
In[10]: # Сначала создаем сетку графиков
 # ax будет массивом из двух объектов Axes 
 fig, ax = plt.subplots(2)
 # Вызываем метод plot() соответствующего объекта
 ax[0].plot(x, np.sin(x))
 ax[1].plot(x, np.cos(x));

--- СТРАНИЦА 269 ---

\1
In[1]: %matplotlib inline
 import matplotlib.pyplot as plt
 plt.style.use('seaborn-whitegrid')
 import numpy as np

\1
In[2]: fig = plt.figure()
 ax = plt.axes()
В библиотеке Matplotlib можно рассматривать рисунок (экземпляр класса plt.Fi-
gure) как единый контейнер, содержащий все объекты, представляющие систему 
координат, графику, текст и метки. Система координат (она же — оси коорди -
нат, экземпляры класса plt.Axes) — то, что вы видите выше: ограничивающий 

--- СТРАНИЦА 270 ---

\1n[3]: fig = plt.figure()
 ax = plt.axes()
 x = np.linspace(0, 10, 1000)
 ax.plot(x, np.sin(x));

\1
In[4]: plt.plot(x, np.sin(x));

--- СТРАНИЦА 271 ---

\1
In[5]: plt.plot(x, np.sin(x))
 plt.plot(x, np.cos(x));
Рис. 4.8. Рисуем несколько линий
Вот и все, что касается построения графиков простых функций в библиотеке 
Matplotlib! Теперь углубимся в некоторые подробности управления внешним 
видом осей и линий.
Настройка графика: цвета и стили линий
Первое, что вы можете захотеть сделать с графиком, — научиться управлять цветами 
и стилями линий. Функция plt.plot принимает дополнительные аргументы, ко -
торыми можно воспользоваться для этой цели. Для настройки цвета используйте 

--- СТРАНИЦА 272 ---

\1n[6]:
plt.plot(x, np.sin(x - 0), color='blue') # Задаем цвет по названию
plt.plot(x, np.sin(x - 1), color='g') # Краткий код цвета (rgbcmyk)
plt.plot(x, np.sin(x - 2), color='0.75') # Шкала оттенков серого цвета,
 # значения в диапазоне от 0 до 1
plt.plot(x, np.sin(x - 3), color='#FFDD44') # 16-ричный код 
 # (RRGGBB от 00 до FF)
plt.plot(x, np.sin(x - 4), color=(1.0,0.2,0.3)) # Кортеж RGB, значения 0 и 1
plt.plot(x, np.sin(x - 5), color='chartreuse'); # Поддерживаются все названия
 # цветов HTML
Если цвет не задан, библиотека Matplotlib будет автоматически перебирать по 
циклу набор цветов по умолчанию при наличии на графике нескольких линий.
Стиль линий можно настраивать и с помощью ключевого слова linestyle 
(рис. 4.10):
Рис. 4.10. Примеры различных стилей линий

--- СТРАНИЦА 273 ---
Простые линейные графики 273
In[7]: plt.plot(x, x + 0, linestyle='solid')
 plt.plot(x, x + 1, linestyle='dashed')
 plt.plot(x, x + 2, linestyle='dashdot')
 plt.plot(x, x + 3, linestyle='dotted');
 # Можно использовать и следующие сокращенные коды:
 plt.plot(x, x + 4, linestyle='-') # сплошная линия
 plt.plot(x, x + 5, linestyle='--') # штриховая линия
 plt.plot(x, x + 6, linestyle='-.') # штрихпунктирная линия
 plt.plot(x, x + 7, linestyle=':'); # пунктирная линия
Если вы предпочитаете максимально сжатый синтаксис, можно объединить задание 
кодов linestyle и color в одном неключевом аргументе функции plt.plot (рис. 4.11):
In[8]: plt.plot(x, x + 0, '-g') # сплошная линия зеленого цвета
 plt.plot(x, x + 1, '--c') # штриховая линия голубого цвета
 plt.plot(x, x + 2, '-.k') # штрихпунктирная линия черного цвета
 plt.plot(x, x + 3, ':r'); # пунктирная линия красного цвета
Рис. 4.11. Сокращенный синтаксис для управления цветами и стилями
Эти односимвольные коды цветов отражают стандартные сокращения, принятые 
в широко используемых для цифровой цветной графики цветовых моделях RGB 
(Red/Green/Blue — «красный/зеленый/синий») и CMYK (Cyan/Magenta/Yellow/
blacK — «голубой/пурпурный/желтый/черный»).
Существует множество других ключевых аргументов, позволяющих выполнять более 
тонкую настройку внешнего вида графика. Чтобы узнать больше, рекомендую посмо-
треть docstring функции plt.plot() с помощью справочных инструментов оболочки 
IPython (см. раздел «Справка и документация в оболочке Python» главы 1).
Настройка графика: пределы осей координат
Библиотека Matplotlib достаточно хорошо подбирает пределы осей коорди -
нат по умолчанию, но иногда требуется более точная настройка. Простейший 

--- СТРАНИЦА 274 ---

\1n[9]: plt.plot(x, np.sin(x))
 plt.xlim(-1, 11)
 plt.ylim(-1.5, 1.5);

\1
In[10]: plt.plot(x, np.sin(x))
 plt.xlim(10, 0)
 plt.ylim(1.2, -1.2);
Рис. 4.13. Пример зеркального отображения оси Y 
Удобный метод для этих действий — plt.axis() (не перепутайте метод plt.axis() 
с методом plt.axes()!). Метод plt.axis() предоставляет возможность задавать 

--- СТРАНИЦА 275 ---
Простые линейные графики 275
пределы осей X и Y с помощью одного вызова путем передачи списка, в котором 
указываются [xmin, xmax, ymin, ymax] (рис. 4.14):
In[11]: plt.plot(x, np.sin(x))
 plt.axis([-1, 11, -1.5, 1.5]);

\1
In[12]: plt.plot(x, np.sin(x))
 plt.axis('tight');

\1
In[13]: plt.plot(x, np.sin(x))
 plt.axis('equal');

--- СТРАНИЦА 276 ---

\1n[14]: plt.plot(x, np.sin(x))
 plt.title("A Sine Curve") # Синусоидальная кривая
 plt.xlabel("x")
 plt.ylabel("sin(x)");
Рис. 4.17. Пример меток осей координат и названия графика

--- СТРАНИЦА 277 ---
Простые линейные графики 277
С помощью необязательных аргументов функций можно настраивать расположе-
ние, размер и стиль этих меток. Подробная информация представлена в докумен-
тации библиотеки Matplotlib и в разделе docstring для каждой из функций.
В случае отображения нескольких линий в одной координатной сетке удобно 
создать легенду для графика, на которой бы отмечался каждый тип линии. В би-
блиотеке Matplotlib для быстрого создания такой легенды имеется встроенный 
метод plt.legend(). Хотя существует несколько возможных способов, проще всего, 
как мне кажется, задать метку каждой линии с помощью ключевого слова label 
функции plot (рис. 4.18):
In[15]: plt.plot(x, np.sin(x), '-g', label='sin(x)')
 plt.plot(x, np.cos(x), ':b', label='cos(x)')
 plt.axis('equal')
 plt.legend();
Рис. 4.18. Пример легенды графика
Функция plt.legend() отслеживает стиль и цвет линии и устанавливает их соот-
ветствие с нужной меткой. Больше информации по заданию и форматированию 
легенд графиков можно найти в docstring метода plt.legend(). Кроме того, мы 
рассмотрим некоторые продвинутые параметры задания легенд в разделе «Поль-
зовательские настройки легенд на графиках» этой главы.
Нюансы использования Matplotlib
Хотя для большинства функций интерфейса plt соответствующие методы интер-
фейса ax носят такое же название (например, plt.plot() → ax.plot() , plt.
legend() → ax.legend() и т. д.), это касается не всех команд. В частности, функции 
для задания пределов, меток и названий графиков называются несколько иначе. Вот 
список соответствий между MATLAB-подобным функциями и объектно-ориенти-
рованным методами:

--- СТРАНИЦА 278 ---

\1n[16]: ax = plt.axes()
 ax.plot(x, np.sin(x))
 ax.set(xlim=(0, 10), ylim=(-2, 2),
 xlabel='x', ylabel='sin(x)',
 title='A Simple Peot'); # Простая диаграмма

\1
In[1]: %matplotlib inline
 import matplotlib.pyplot as plt
 plt.style.use('seaborn-whitegrid')
 import numpy as np

--- СТРАНИЦА 279 ---

\1
In[2]: x = np.linspace(0, 10, 30)
 y = np.sin(x)
 plt.plot(x, y, 'o', color='black');

\1
In[3]: rng = np.random.RandomState(0)
 for marker in ['o', '.', ',', 'x', '+', 'v', '^', '<', '>', 's', 'd']:
 plt.plot(rng.rand(5), rng.rand(5), marker,
 label="marker='{0}'".format(marker))
 plt.legend(numpoints=1)
 plt.xlim(0, 1.8);

\1
In[4]: plt.plot(x, y, '-ok'); # линия (-), маркер круга (o), черный цвет (k)

\1
In[5]: plt.plot(x, y, '-p', color='gray',
 markersize=15, linewidth=4,

--- СТРАНИЦА 280 ---

\1n[6]: plt.scatter(x, y, marker='o');

\1
In[7]: rng = np.random.RandomState(0)
 x = rng.randn(100)
 y = rng.randn(100)
 colors = rng.rand(100)
 sizes = 1000 * rng.rand(100)
 plt.scatter(x, y, c=colors, s=sizes, alpha=0.3,
 cmap='viridis')
 plt.colorbar(); # Отображаем цветовую шкалу

--- СТРАНИЦА 282 ---

\1n, каждая вы -
борка представляет собой один из трех типов цветов с тщательно измеренными 
лепестками и чашелистиками (рис. 4.26):
In[8]: from sklearn.datasets import load_iris
 iris = load_iris()
 features = iris.data.T 
 plt.scatter(features[0], features[1], alpha=0.2,
 s=100*features[3], c=iris.target, cmap='viridis')
 plt.xlabel(iris.feature_names[0])
 plt.ylabel(iris.feature_names[1]);

\1
In[1]: %matplotlib inline
 import matplotlib.pyplot as plt
 plt.style.use('seaborn-whitegrid')

--- СТРАНИЦА 284 ---

\1numpy as np
In[2]: x = np.linspace(0, 10, 50)
 dy = 0.8
 y = np.sin(x) + dy * np.random.randn(50)
 plt.errorbar(x, y, yerr=dy, fmt='.k');

\1
In[3]: plt.errorbar(x, y, yerr=dy, fmt='o', color='black',
 ecolor='lightgray', elinewidth=3, capsize=0);
Рис. 4.28. Пользовательские настройки планок погрешностей

--- СТРАНИЦА 285 ---
Визуализация погрешностей 285
В дополнение к этим опциям можно также создавать горизонтальные планки погреш-
ностей ( xerr), односторонние планки погрешностей и много других вариантов. Чтобы 
узнать больше об имеющихся опциях, обратитесь к docstring функции plt.errorbar.
Непрерывные погрешности
В некоторых случаях желательно отображать планки погрешностей для непрерыв-
ных величин. Хотя в библиотеке Matplotlib отсутствует встроенная удобная ути-
лита для решения данной задачи, не составит особого труда скомбинировать такие 
примитивы, как plt.plot и plt.fill_between, для получения искомого результата.
Выполним с помощью API пакета Scikit-Learn (см. подробности в разделе «Зна-
комство с библиотекой Scikit-Learn» главы 5) простую регрессию на основе Гауссова 
процесса (Gaussian process regression, GPR). Она представляет собой метод подбора 
по имеющимся данным очень гибкой непараметрической функции с непрерывной 
мерой неопределенности измерения. Мы не будем углубляться в детали регрессии 
на основе Гауссова процесса, а сконцентрируемся на визуализации подобной не-
прерывной погрешности измерения:
In[4]: from sklearn.gaussian_process import GaussianProcess
 # Описываем модель и отрисовываем некоторые данные
 model = lambda x: x * np.sin(x)
 xdata = np.array([1, 3, 5, 6, 8])
 ydata = model(xdata)
 # Выполняем подгонку Гауссова процесса
 gp = GaussianProcess1(corr='cubic', theta0=1e-2, thetaL=1e-4,
 thetaU=1E-1, random_start=100)
 gp.fit(xdata[:, np.newaxis], ydata)
 xfit = np.linspace(0, 10, 1000)
 yfit, MSE = gp.predict(xfit[:, np.newaxis], eval_MSE=True)
 dyfit = 2 * np.sqrt(MSE) # 2*сигма ~ область с уровнем доверия 95%
Теперь у нас имеются значения xfit, yfit и dyfit, представляющие выборку непре-
рывных приближений к нашим данным. Мы можем передать их, как и выше, функ-
ции plt.errorbar, но рисовать 1000 точек с помощью 1000 планок погрешностей не 
хотелось бы. Вместо этого можно воспользоваться функцией plt.fill_between и ви-
зуализировать эту непрерывную погрешность с помощью светлого цвета (рис. 4.29):
In[5]: # Визуализируем результат
 plt.plot(xdata, ydata, 'or')
 plt.plot(xfit, yfit, '-', color='gray')

\1n. Вместо него рекомендуется использовать класс Gaussian Process Regressor.

--- СТРАНИЦА 286 ---

\1n(xfit, yfit - dyfit, yfit + dyfit,
 color='gray', alpha=0.2)
 plt.xlim(0, 10);
Рис. 4.29. Представляем непрерывную погрешность 
с помощью закрашенных областей
Обратите внимание на передаваемые функции plt.fill_between параметры: мы 
передали в нее значение x, затем нижнюю границу по y, затем верхнюю границу 
по y, в результате получили заполненную область между ними.
Получившийся рисунок отлично поясняет, что делает алгоритм регрессии на основе 
Гауссова процесса. В областях возле измеренной точки данных модель жестко огра-
ничена, что и отражается в малых ошибках модели. В удаленных же от измеренной 
точки данных областях модель жестко не ограничивается и ошибки модели растут.
Чтобы узнать больше о возможностях функции plt.fill_between() (и родствен -
ной ей функции plt.fill() ), см. ее docstring или документацию библиотеки 
Matplotlib.
Наконец, если описанное вам не по вкусу, загляните в раздел «Визуализация с по-
мощью библиотеки Seaborn» данной главы, в котором мы обсудим пакет Seaborn 
с продвинутым API для визуализации подобных непрерывных погрешностей.

\1
 plt.contour — для контурных графиков;
 plt.contourf — для контурных графиков с заполнением;
 plt.imshow — для отображения изображений. В этом разделе мы рассмотрим 
несколько примеров использования данных функций. Начнем с настройки 
блокнота для построения графиков и нужных нам импортов:

--- СТРАНИЦА 287 ---
Графики плотности и контурные графики 287
In[1]: %matplotlib inline
 import matplotlib.pyplot as plt
 plt.style.use('seaborn-white')
 import numpy as np

\1
In[2]: def f(x, y):
 return np.sin(x) ** 10 + np.cos(10 + * x) * np.cos(x)
Создать контурный график можно с помощью функции plt.contour. У нее имеется 
три аргумента: координатная сетка значений x , координатная сетка значений y и ко-
ординатная сетка значений z . Значения x и y представлены точками на графике, 
а значения z будут представлены контурами уровней. Вероятно, наиболее простой 
способ подготовить такие данные — воспользоваться функцией np.meshgrid, фор -
мирующей двумерные координатные сетки из одномерных массивов:
In[3]: x = np.linspace(0, 5, 50)
 y = np.linspace(0, 5, 40)
 X, Y = np.meshgrid(x, y)
 Z = f(X, Y)

\1
In[4]: plt.contour(X, Y, Z, colors='black');

\1

--- СТРАНИЦА 288 ---

\1n[5]: plt.contour(X, Y, Z, 20, cmap='RdGy');
Рис. 4.31. Визуализация трехмерных данных с помощью разноцветных контуров
Мы выбрали здесь карту цветов RdGy (сокращение для Red — Gray — «красный — 
серый») — отличный выбор для случая центрированных данных. В библиотеке 
Matplotlib доступен широкий диапазон карт цветов, которые можно просмотреть 
в IPython путем TAB-автодополнения названия модуля plt.cm:
plt.cm.<TAB>
Наш график приобрел более приятный глазу вид, но промежутки между линиями 
несколько отвлекают внимание. Изменить это можно, воспользовавшись контур-
ным графиком с заполнением, доступным посредством функции plt.contourf() 
(обратите внимание на букву f в конце ее названия), синтаксис которой не отли-
чается от синтаксиса функции plt.contour().

\1
In[6]: plt.contourf(X, Y, Z, 20, cmap='RdGy')
 plt.colorbar();

\1
In[7]: plt.imshow(Z, extent=[0, 5, 0, 5], origin='lower', cmap='RdGy')
 plt.colorbar()
 plt.axis(aspect='image');
Однако есть несколько потенциальных проблем и с функцией imshow().
 Функция plt.imshow() не принимает в качестве параметров координатные сетки 
x и y, так что вам придется вручную задать размеры изображения на графике: 
extent [xmin, xmax, ymin, ymax].
 По умолчанию функция plt.imshow() следует стандартному определению 
массива для изображения, в котором начало координат находится в верхнем 
левом, а не в нижнем левом углу, как на большинстве контурных графиков. Это 
поведение можно изменить в случае отображения данных с привязкой к сетке.
 Функция plt.imshow() автоматически настраивает соотношение сторон гра -
фика в соответствии с входными данными. Это поведение можно из менить, 
задав, например, plt.axis(aspect='image'), чтобы отрезки по осям X и Y были 
одинаковыми.
Рис. 4.33. Представляем трехмерные данные в виде изображения
Иногда удобно комбинировать контурный график с графиком-изображением. 
Например, для создания показанного на рис. 4.34 эффекта мы воспользуемся 

--- СТРАНИЦА 290 ---

\1n[8]: contours = plt.contour(X, Y, Z, 3, colors='black')
 plt.clabel(contours, inline=True, fontsize=8)
 plt.imshow(Z, extent=[0, 5, 0, 5], origin='lower',
 cmap='RdGy', alpha=0.5)
 plt.colorbar();
Сочетание этих трех функций — plt.contour, plt.contourf и plt.imshow — предо-
ставляет практически неограниченные возможности по отображению подобных 
трехмерных данных на двумерных графиках. Дальнейшую информацию относи-
тельно имеющихся у этих функций параметров вы можете найти в их docstring. 

\1
In[1]: %matplotlib inline
 import numpy as np
 import matplotlib.pyplot as plt
 plt.style.use('seaborn-white')

--- СТРАНИЦА 291 ---
Гистограммы, разбиения по интервалам и плотность 291
 data = np.random.randn(1000)
In[2]: plt.hist(data);

\1
In[3]: plt.hist(data, bins=30, normed=True, alpha=0.5,
 histtype='stepfilled', color='steelblue',
 edgecolor='none');
Рис. 4.36. Гистограмма с пользовательскими настройками
Docstring функции plt.hist содержит более подробную информацию о дру -
гих доступных возможностях пользовательской настройки. Сочетание опции 
histtype='stepfilled' с заданной прозрачностью alpha представляется мне очень 
удобным для сравнения гистограмм нескольких распределений (рис. 4.37):
In[4]: x1 = np.random.normal(0, 0.8, 1000)
 x2 = np.random.normal(-2, 1, 1000)
 x3 = np.random.normal(3, 2, 1000)

--- СТРАНИЦА 292 ---

\1normed=True, bins=40)
 plt.hist(x1, **kwargs)
 plt.hist(x2, **kwargs)
 plt.hist(x3, **kwargs);
Рис. 4.37. Рисуем несколько гистограмм поверх друг друга
Если же вам нужно вычислить гистограмму (то есть по дсчитать количество 
точек в заданном интервале) и не отображать ее, к вашим услугам функция 
np.histogram():
In[5]: counts, bin_edges = np.histogram(data, bins=5)
 print(counts)
[ 12 190 468 301 29]

\1
In[6]: mean = [0, 0]
 cov = [[1, 1], [1, 2]]
 x, y = np.random.multivariate_normal(mean, cov, 10000).T 

\1
In[12]: plt.hist2d(x, y, bins=30, cmap='Blues')
 cb = plt.colorbar()
 cb.set_label('counts in bin') # Количествво в интервале

--- СТРАНИЦА 293 ---
Гистограммы, разбиения по интервалам и плотность 293
Рис. 4.38. Двумерная гистограмма, построенная с помощью функции plt.hist2d
У функции plt.hist2d, как и у функции plt.hist, имеется немало дополнительных 
параметров для тонкой настройки графика и разбиения по интервалам, подробно 
описанных в ее docstring. Аналогично тому, как у функции plt.hist есть эквива-
лент np.histogram, так и у функции plt.hist2d имеется эквивалент np.histogram2d, 
который используется следующим образом:
In[8]: counts, xedges, yedges = np.histogram2d(x, y, bins=30)
Для обобщения разбиения по интервалам для гистограммы на число измерений, 
превышающее 2, см. функцию np.histogramdd.
Функция plt.hexbin: гексагональное разбиение по интервалам
Двумерная гистограмма создает мозаичное представление квадратами вдоль ко -
ординатных осей. Другая геометрическая фигура для подобного мозаичного пред-
ставления — правильный шестиугольник. Для этих целей библиотека Matplotlib 
предоставляет функцию plt.hexbin — двумерный набор данных, разбитых по 
интервалам на сетке из шестиугольников (рис. 4.39):
In[9]: plt.hexbin(x, y, gridsize=30, cmap='Blues')
 cb = plt.colorbar(label='count in bin') # Количество в интервале
Рис. 4.39. Создание двумерной гистограммы с помощью функции plt.hexbin

--- СТРАНИЦА 294 ---

\1n имеется множество интересных параметров, включая воз-
можность задавать вес для каждой точки и менять выводимое значение для каждого 
интервала на любой сводный показатель библиотеки NumPy (среднее значение 
весов, стандартное отклонение весов и т. д.).
Ядерная оценка плотности распределения
Еще один часто используемый метод оценки плотностей в многомерном простран-
стве — ядерная оценка плотности распределения (kernel density estimation, KDE). 

\1
In[10]: from scipy.stats import gaussian_kde
 # Выполняем подбор на массиве размера [Ndim, Nsamples]
 data = np.vstack([x, y])
 kde = gaussian_kde(data)
 # Вычисляем на регулярной координатной сетке
 xgrid = np.linspace(-3.5, 3.5, 40)
 ygrid = np.linspace(-6, 6, 40)
 Xgrid, Ygrid = np.meshgrid(xgrid, ygrid)
 Z = kde.evaluate(np.vstack([Xgrid.ravel(), Ygrid.ravel()]))
 # Выводим график результата в виде изображения
 plt.imshow(Z.reshape(Xgrid.shape),
 origin='lower', aspect='auto',
 extent=[-3.5, 3.5, -6, 6],
 cmap='Blues')
 cb = plt.colorbar()
 cb.set_label("density") # Плотность
Рис. 4.40. Ядерная оценка плотности распределения

--- СТРАНИЦА 295 ---
Пользовательские настройки легенд на графиках 295
Длина сглаживания метода KDE позволяет эффективно в ыбирать компромисс между 
гладкостью и детализацией (один из примеров вездесущих компромиссов между 
смещением и дисперсией). Существует обширная литература, посвященная выбору 
подходящей длины сглаживания: в функции gaussian_kde используется эмпириче-
ское правило для поиска квазиоптимальной длины сглаживания для входных данных.
В экосистеме SciPy имеются и другие реализации метода KDE, каждая со своими 
сильными и слабыми сторонами, например методы sklearn.neighbors.KernelDensity 
и statsmodels.nonparametric.kernel_density.KDEMultivariate . Использование 
библиотеки Matplotlib для основанных на методе KDE визуализаций требует на-
писания излишнего кода. Библиотека Seaborn, которую мы будем обсуждать в раз -
деле «Визуализация с помощью библиотеки Seaborn» данной главы, предлагает для 
создания таких визуализаций API с намного более сжатым синтаксисом.
Пользовательские настройки легенд на графиках
Большая понятность графика обеспечивается заданием меток для различных эле-
ментов графика. Мы ранее уже рассматривали создание простой легенды, здесь 
продемонстрируем возможности пользовательской настройки расположения 
и внешнего вида легенд в Matplotlib.
С помощью команды plt.legend() можно автоматически создать простейшую 
легенду для любых маркированных элементов графика (рис. 4.41):
In[1]: import matplotlib.pyplot as plt
 plt.style.use('classic')
In[2]: %matplotlib inline
 import numpy as np
In[3]: x = np.linspace(0, 10, 1000)
 fig, ax = plt.subplots()
 ax.plot(x, np.sin(x), '-b', label='Sine') # Синус
 ax.plot(x, np.cos(x), '--r', label='Cosine') # Косинус
 ax.axis('equal')
 leg = ax.legend();
Рис. 4.41. Легенда графика по умолчанию

--- СТРАНИЦА 296 ---

\1n[4]: ax.legend(loc='upper left', frameon=False)
 fig
Рис. 4.42. Легенда графика с пользовательскими настройками
Можно также воспользоваться командой ncol, чтобы задать количество столбцов 
в легенде (рис. 4.43):
In[5]: ax.legend(frameon=False, loc='lower center', ncol=2)
 fig
Рис. 4.43. Легенда графика в два столбца
Можно использовать для легенды скругленную прямоугольную рамку ( fancybox) 
или добавить тень, поменять прозрачность (альфа-фактор) рамки или поля около 
текста (рис. 4.44):
In[6]: ax.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1)
 fig

--- СТРАНИЦА 297 ---
Пользовательские настройки легенд на графиках 297
Рис. 4.44. Легенда графика со скругленной прямоугольной рамкой
Дополнительную информацию об имеющихся настройках для легенд можно полу-
чить в docstring функции plt.legend.
Выбор элементов для легенды
По умолчанию легенда включает все маркированные элементы. Если нам этого не 
нужно, можно указать, какие элементы и метки должны присутствовать в легенде, 
воспользовавшись объектами, возвращаемыми командами построения графика. 
Команда plt.plot() умеет рисовать за один вызов несколько линий и возвращать 
список созданных экземпляров линий. Для указания, какие элементы использовать, 
достаточно передать какие-либо из них функции plt.legend() вместе с задаваемы-
ми метками (рис. 4.45):
In[7]: y = np.sin(x[:, np.newaxis] + np.pi * np.arange(0, 2, 0.5))
 lines = plt.plot(x, y)
 # lines представляет собой список экземпляров класса plt.Line2D
 plt.legend(lines[:2], ['first', 'second']); # Первый, второй
Рис. 4.45. Пользовательские настройки элементов легенды

--- СТРАНИЦА 298 ---

\1n[8]: plt.plot(x, y[:, 0], label='first')
 plt.plot(x, y[:, 1], label='second')
 plt.plot(x, y[:, 2:])
 plt.legend(framealpha=1, frameon=True);

\1
In[9]: import pandas as pd
 cities = pd.read_csv('data/california_cities.csv')
 # Извлекаем интересующие нас данные
 lat, lon = cities['latd'], cities['longd']
 population, area = cities['population_total'], cities['area_total_km2']
 # Распределяем точки по нужным местам,
 # с использованием размера и цвета, но без меток
 plt.scatter(lon, lat, label=None,
 c=np.log10(population), cmap='viridis',
 s=area, linewidth=0, alpha=0.5)
 plt.axis(aspect='equal')

--- СТРАНИЦА 299 ---
Пользовательские настройки легенд на графиках 299
 plt.xlabel('longitude')
 plt.ylabel('latitude')
 plt.colorbar(label='log$_{10}$(population)')
 plt.clim(3, 7)
 # Создаем легенду:
 # выводим на график пустые списки с нужным размером и меткой
 for area in [100, 300, 500]:
 plt.scatter([], [], c='k', alpha=0.3, s=area,
 label=str(area) + ' km$^2$')
 plt.legend(scatterpoints=1, frameon=False,
 labelspacing=1, title='City Area') # Города
 plt.title('California Cities: Area and Population');
 # Города Калифорнии: местоположение и население
Рис. 4.47. Местоположение, размер и население городов штата Калифорния
Легенда всегда относится к какому-либо находящемуся на графике объекту, по -
этому, если нам нужно отобразить объект конкретного вида, необходимо сначала 
его нарисовать на графике. В данном случае нужных нам объектов (кругов серого 
цвета) на графике нет, поэтому идем на хитрость и выводим на график пустые спи-
ски. Обратите внимание, что в легенде перечислены только те элементы графика, 
для которых задана метка.
Мы создали посредством вывода на график пустых списков маркированные объекты, 
которые затем собираются в легенде. Теперь легенда дает нам полезную информацию. 
Эту стратегию можно использовать для создания и более сложных визуализаций.
Обратите внимание, что в случае подобных географических данных график стал 
бы понятнее при отображении на нем границ штата и других картографических 
элементов. Отличный инструмент для этой цели — дополнительный набор утилит 
Basemap для библиотеки Matplotlib, который мы рассмотрим в разделе «Отобра-
жение географических данных с помощью Basemap» данной главы.

--- СТРАНИЦА 300 ---

\1nd, можно 
создавать только одну легенду для всего графика. Если попытаться создать вторую 
легенду с помощью функций plt.legend() и ax.legend(), она просто перекроет 
первую. Решить эту проблему можно, создав изначально для легенды новый ри -
сователь (artist), после чего добавить вручную второй рисователь на график с по-
мощью низкоуровневого метода ax.add_artist() (рис. 4.48):
In[10]: fig, ax = plt.subplots()
 lines = []
 styles = ['-', '--', '-.', ':']
 x = np.linspace(0, 10, 1000)
 for i in range(4):
 lines += ax.plot(x, np.sin(x - i * np.pi / 2),
 styles[i], color='black')
 ax.axis('equal')
 # Задаем линии и метки первой легенды
 ax.legend(lines[:2], ['line A', 'line B'], # Линия А, линия B
 loc='upper right', frameon=False)
 # Создаем вторую легенду и добавляем рисователь вручную
 from matplotlib.legend import Legend
 leg = Legend(ax, lines[2:], ['line C', 'line D'], # Линия С, линия D
 loc='lower right', frameon=False)
 ax.add_artist(leg);
Рис. 4.48. Разделенная на части легенда
Мы мельком рассмотрели низкоуровневые объекты рисования, из которых состоит 
любой график библиотеки Matplotlib. Если вы заглянете в исходный код метода 

--- СТРАНИЦА 301 ---
Пользовательские настройки шкал цветов 301
ax.legend() (напомню, что сделать это можно в блокноте оболочки IPython с по-
мощью команды legend??), то увидите, что эта функция состоит просто из логики 
создания подходящего рисователя Legend, сохраняемого затем в атрибуте legend_ 
и добавляемого к рисунку при отрисовке графика.
Пользовательские настройки шкал цветов
Легенды графика отображают соответствие дискретных меток дискретным точкам. 
В случае непрерывных меток, базирующихся на цвете точек, линий или областей, 
отлично подойдет такой инструмент, как шкала цветов. В библиотеке Matplotlib 
шкала цветов — отдельная система координат, предоставляющая ключ к значению 
цветов на графике. Поскольку эта книга напечатана в черно-белом исполнении, 
для данного раздела имеется дополнительное онлайн-приложение, в котором вы 
можете посмотреть на оригинальные графики в цвете ( https://github.com/jakevdp/
PythonDataScienceHandbook). Начнем с настройки блокнота для построения графиков 
и импорта нужных функций:
In[1]: import matplotlib.pyplot as plt
 plt.style.use('classic')
In[2]: %matplotlib inline
 import numpy as np

\1
In[3]: x = np.linspace(0, 10, 1000)
 I = np.sin(x) * np.cos(x[:, np.newaxis])
 plt.imshow(I)
 plt.colorbar();
Рис. 4.49. Простая легенда-шкала цветов
Далее мы рассмотрим несколько идей по пользовательской настройке шкалы цве-
тов и эффективному их использованию в разных ситуациях.

--- СТРАНИЦА 302 ---

\1n[4]: plt.imshow(I, cmap='gray');
Все доступные для использования карты цветов содержатся в пространстве имен 
plt.cm. Вы можете получить полный список встроенных опций с помощью TAB-
автодополнения в оболочке IPython:
plt.cm.<TAB>
Но возможность выбора карты цветов — лишь первый шаг, гораздо важнее выбрать 
среди имеющихся вариантов! Выбор оказывается гораздо более тонким, чем вы 
могли бы ожидать.
Рис. 4.50. Карта цветов на основе оттенков серого
Выбор карты цветов
Всестороннее рассмотрение вопроса выбора цветов в визуализации выходит за 
пределы данной книги, но по этому вопросу вы можете почитать статью Ten Simple 
Rules for Better Figures («Десять простых правил для улучшения рисунков», http://
bit.ly/2fDJn9J). Онлайн-документация библиотеки Matplotlib также содержит ин -
тересную информацию по вопросу выбора карты цветов ( http://matplotlib.org/1.4.1/
users/colormaps.html).

\1
 последовательные карты цветов. Состоят из одной непрерывной последователь-
ности цветов (например, binary или viridis);
 дивергентные карты цветов. Обычно содержат два хорошо различимых цвета, 
отражающих положительные и отрицательные отклонения от среднего значе -
ния (например, RdBu или PuOr);
 качественные карты цветов. В них цвета смешиваются без какого-либо четкого 
порядка (например, rainbow или jet).

--- СТРАНИЦА 303 ---

\1
In[5]:
from matplotlib.colors import LinearSegmentedColormap
def grayscale_cmap(cmap):
 """Возвращает версию в оттенках серого заданной карты цветов"""
 cmap = plt.cm.get_cmap(cmap)
 colors = cmap(np.arange(cmap.N))
 # Преобразуем RGBA в воспринимаемую глазом светимость серого цвета
 # ср. http://alienryderflex.com/hsp.html
 RGB_weight = [0.299, 0.587, 0.114]
 luminance = np.sqrt(np.dot(colors[:, :3] ** 2, RGB_weight))
 colors[:, :3] = luminance[:, np.newaxis]
 return LinearSegmentedColormap.from_list(cmap.name + "_gray", 
 colors, cmap.N)
def view_colormap(cmap):
 """Рисует карту цветов в эквивалентных оттенках серого"""
 cmap = plt.cm.get_cmap(cmap)
 colors = cmap(np.arange(cmap.N))
 cmap = grayscale_cmap(cmap)
 grayscale = cmap(np.arange(cmap.N))
 fig, ax = plt.subplots(2, figsize=(6, 2),
 subplot_kw=dict(xticks=[], yticks=[]))
 ax[0].imshow([colors], extent=[0, 10, 0, 1])
 ax[1].imshow([grayscale], extent=[0, 10, 0, 1])
In[6]: view_colormap('jet')
Рис. 4.51. Карта цветов jet и ее неравномерная шкала светимости
Отметим яркие полосы в ахроматическом изображении. Даже в полном цвете 
эта неравномерная яркость означает, что определенные части диапазона цветов 
будут притягивать внимание, что потенциально привед ет к акцентированию 

--- СТРАНИЦА 304 ---

\1n[7]: view_colormap('viridis')

\1
In[8]: view_colormap('cubehelix')
Рис. 4.53. Карта цветов cubehelix и ее светимость
В других случаях, например для отображения положительных и отрицательных 
отклонений от среднего значения, могут оказаться удобны такие двуцветные карты 
шкалы цветов, как RdBu (сокращение от Red — Blue — «красный — синий»). Однако, 
как вы можете видеть на рис. 4.54, такая информация будет потеряна при переходе 
к оттенкам серого!
In[9]: view_colormap('RdBu')
Рис. 4.54. RdBu (красно-синяя) карта цветов и ее светимость
Далее мы увидим примеры использования некоторых из этих карт цветов.
В библиотеке Matplotlib существует множество карт цветов, для просмотра их 
списка вы можете воспользоваться оболочкой IPython для просмотра содержимого 

--- СТРАНИЦА 305 ---
Пользовательские настройки шкал цветов 305
подмодуля plt.cm. Более принципиальный подход к использованию цветов в язы-
ке Python можно найти в инструментах и документации по библиотеке Seaborn 
(см. раздел «Визуализация с помощью библиотеки Seaborn» этой главы).
Ограничения и расширенные возможности 
по использованию цветов
Библиотека Matplotlib предоставляет возможность разнообразных пользователь -
ских настроек шкал цветов. Сами по себе шкалы цветов — просто экземпляры клас-
са plt.Axes, поэтому для них можно использовать все уже изученные нами трюки, 
связанные с форматированием осей координат и делений на них. Шкалы цветов об-
ладают достаточной гибкостью: например, можно сузить границы диапазона цветов, 
обозначив выходящие за пределы этого диапазона значения с помощью тре угольных 
стрелок вверху и внизу путем задания значения свойства extend. Это может оказать-
ся удобно, например, при выводе зашумленного изображения (рис. 4.55):
In[10]: # создаем шум размером 1% от пикселов изображения
 speckles = (np.random.random(I.shape) < 0.01)
 I[speckles] = np.random.normal(0, 3, np.count_nonzero(speckles))
 plt.figure(figsize=(10, 3.5))
 plt.subplot(1, 2, 1)
 plt.imshow(I, cmap='RdBu')
 plt.colorbar()
 plt.subplot(1, 2, 2)
 plt.imshow(I, cmap='RdBu')
 plt.colorbar(extend='both')
 plt.clim(-1, 1);
Рис. 4.55. Задаем расширение карты цветов
Обратите внимание, что на левом рисунке зашумленные пикселы влияют на 
пределы диапазона цветов, по этой причине диапазон шума делает совершенно 

--- СТРАНИЦА 306 ---

\1n[11]: plt.imshow(I, cmap=plt.cm.get_cmap('Blues', 6))
 plt.colorbar()
 plt.clim(-1, 1);
Рис. 4.56. Дискретизированная карта цветов
Использовать дискретный вариант карты цветов можно совершенно так же, как 
и любую другую карту цветов.
Пример: рукописные цифры
В качестве примера рассмотрим интересную визуализацию данных с рукописными 
цифрами. Они включены в библиотеку Scikit-Learn и состоят почти из 2000 миниа-
тюр размером 8 × 8 с рукописными цифрами.

\1
In[12]: # Загружаем изображения цифр от 0 до 5 и визуализируем некоторые из них
 from sklearn.datasets import load_digits
 digits = load_digits(n_class=6)

--- СТРАНИЦА 307 ---
Пользовательские настройки шкал цветов 307
 fig, ax = plt.subplots(8, 8, figsize=(6, 6))
 for i, axi in enumerate(ax.flat):
 axi.imshow(digits.images[i], cmap='binary')
 axi.set(xticks=[], yticks=[])
Рис. 4.57. Пример данных с рукописными цифрами
В силу того что каждая цифра определяется оттенком ее 64 пи кселов, можно 
считать ее точкой в 64-мерном пространстве: каждое измерение отражает яркость 
одного пиксела. Однако визуализация зависимостей в настолько многомерном про-
странстве представляет собой исключительно непростую задачу. Один из способов 
ее решения — воспользоваться каким-либо из методов понижения размерности 
(dimensionality reduction), например обучением на базе многообразий (manifold 
learning) с целью снижения размерности данных с сохранением интересующих нас 
зависимостей. Понижение размерности — пример машинного обучения без учителя 
(unsupervised machine learning). Мы обсудим его подробнее в разделе «Что такое 
машинное обучение» главы 5.

\1
In[13]: # Отображаем цифры на двумерное пространство с помощью функции IsoMap
 from sklearn.manifold import Isomap
 iso = Isomap(n_components=2)
 projection = iso.fit_transform(digits.data)

\1

--- СТРАНИЦА 308 ---

\1n[14]: # Выводим результаты на график
 plt.scatter(projection[:, 0], projection[:, 1], lw=0.1,
 c=digits.target, cmap=plt.cm.get_cmap('cubehelix', 6))
 plt.colorbar(ticks=range(6), label='digit value')
 # Цифровые значения
 plt.clim(-0.5, 5.5)

\1
In[1]: %matplotlib inline
 import matplotlib.pyplot as plt
 plt.style.use('seaborn-white')
 import numpy as np

--- СТРАНИЦА 309 ---

\1
In[2]: ax1 = plt.axes() # обычные оси координат
 ax2 = plt.axes([0.65, 0.65, 0.2, 0.2])

\1
In[3]: fig = plt.figure()
 ax1 = fig.add_axes([0.1, 0.5, 0.8, 0.4],
 xticklabels=[], ylim=(-1.2, 1.2))
 ax2 = fig.add_axes([0.1, 0.1, 0.8, 0.4],
 ylim=(-1.2, 1.2))
 x = np.linspace(0, 10)
 ax1.plot(np.sin(x))
 ax2.plot(np.cos(x));

--- СТРАНИЦА 310 ---

\1n[4]: for i in range(1, 7):
 plt.subplot(2, 3, i)
 plt.text(0.5, 0.5, str((2, 3, i)),
 fontsize=18, ha='center')

\1
In[5]: fig = plt.figure()
 fig.subplots_adjust(hspace=0.4, wspace=0.4)
 for i in range(1, 7):
 ax = fig.add_subplot(2, 3, i)
 ax.text(0.5, 0.5, str((2, 3, i)),
 fontsize=18, ha='center')

\1

--- СТРАНИЦА 312 ---

\1n[6]: fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')

\1
In[7]: # Системы координат располагаются в двумерном массиве, 
 # индексируемом по [строка, столбец]
 for i in range(2):
 for j in range(3):
 ax[i, j].text(0.5, 0.5, str((i, j)),
 fontsize=18, ha='center')
 fig
Рис. 4.64. Нумерация графиков в сетке субграфиков
По сравнению с plt.subplot() функция plt.subplots() намного лучше согласуется 
с принятой в языке Python индексацией, начинающейся с 0.

--- СТРАНИЦА 313 ---

\1
In[8]: grid = plt.GridSpec(2, 3, wspace=0.4, hspace=0.3)
Затем мы можем задать местоположение и размеры субграфиков с помощью обыч-
ного синтаксиса срезов языка Python (рис. 4.65):
In[9]: plt.subplot(grid[0, 0])
 plt.subplot(grid[0, 1:])
 plt.subplot(grid[1, :2])
 plt.subplot(grid[1, 2]);

\1
In[10]: # Создаем нормально распределенные данные 
 mean = [0, 0]
 cov = [[1, 1], [1, 2]]
 x, y = np.random.multivariate_normal(mean, cov, 3000).T 
 # Задаем системы координат с помощью функции GridSpec
 fig = plt.figure(figsize=(6, 6))
 grid = plt.GridSpec(4, 4, hspace=0.2, wspace=0.2)
 main_ax = fig.add_subplot(grid[:-1, 1:])
 y_hist = fig.add_subplot(grid[:-1, 0], xticklabels=[], sharey=main_ax)
 x_hist = fig.add_subplot(grid[-1, 1:], yticklabels=[], sharex=main_ax)
 # Распределяем точки по основной системе координат

--- СТРАНИЦА 314 ---

\1n_ax.plot(x, y, 'ok', markersize=3, alpha=0.2)
 # Рисуем гистограммы на дополнительных системах координат 
 x_hist.hist(x, 40, histtype='stepfilled',
 orientation='vertical', color='gray')
 x_hist.invert_yaxis()
 y_hist.hist(y, 40, histtype='stepfilled',
 orientation='horizontal', color='gray')
 y_hist.invert_xaxis()
Рис. 4.66. Визуализируем многомерные распределения с помощью функции plt.GridSpec
Такое распределение, выводимое на отдельных графиках по бокам, настолько рас -
пространено, что в пакете Seaborn для построения ег о графиков предусмотрено 
отдельное API. Подробную информацию см. в разделе «Визуализация с помощью 
библиотеки Seaborn» данной главы.

\1
In[1]: %matplotlib inline
 import matplotlib.pyplot as plt

--- СТРАНИЦА 315 ---
Текст и поясняющие надписи 315
 import matplotlib as mpl
 plt.style.use('seaborn-whitegrid')
 import numpy as np
 import pandas as pd
Пример: влияние выходных дней 
на рождение детей в США
Вернемся к данным, с которыми мы работали ранее в разделе «Пример: данные 
о рождаемости» главы 3, где мы сгенерировали график среднего количества рож-
дений детей в зависимости от дня календарного года. Эти данные можно скачать 
по адресу https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/births.csv.

\1
In[2]:
births = pd.read_csv('births.csv')
quartiles = np.percentile(births['births'], [25, 50, 75])
mu, sig = quartiles[1], 0.74 * (quartiles[2] - quartiles[0])
births = births.query('(births > @mu - 5 * @sig) & (births < @mu + 5 * @sig)')
births['day'] = births['day'].astype(int)
births.index = pd.to_datetime(10000 * births.year +
 100 * births.month +
 births.day, format='%Y%m%d')
births_by_date = births.pivot_table('births',
 [births.index.month, births.index.day])
births_by_date.index = [pd.datetime(2012, month, day)
 for (month, day) in births_by_date.index]
In[3]: fig, ax = plt.subplots(figsize=(12, 4))
 births_by_date.plot(ax=ax);
Рис. 4.67. Среднее ежедневное количество новорожденных в зависимости от даты

--- СТРАНИЦА 316 ---

\1n[4]: fig, ax = plt.subplots(figsize=(12, 4))
 births_by_date.plot(ax=ax)
 # Добавляем метки на график
 style = dict(size=10, color='gray')
 ax.text('2012-1-1', 3950, "New Year's Day", **style)
 ax.text('2012-7-4', 4250, "Independence Day", ha='center', **style)
 ax.text('2012-9-4', 4850, "Labor Day", ha='center', **style)
 ax.text('2012-10-31', 4600, "Halloween", ha='right', **style)
 ax.text('2012-11-25', 4450, "Thanksgiving", ha='center', **style)
 ax.text('2012-12-25', 3850, "Christmas ", ha='right', **style)
 # Добавляем метки для осей координат
 ax.set(title='USA births by day of year (1969-1988)',
 ylabel='average daily births')
 # Размечаем ось X центрированными метками для месяцев
 ax.xaxis.set_major_locator(mpl.dates.MonthLocator())
 ax.xaxis.set_minor_locator(mpl.dates.MonthLocator(bymonthday=15))
 ax.xaxis.set_major_formatter(plt.NullFormatter())
 ax.xaxis.set_minor_formatter(mpl.dates.DateFormatter('%h'));
Рис. 4.68. Ежедневное количество рождаемых детей в зависимости от даты, с комментариями
Метод ax.text принимает на входе координату x , координату y, строковое значение 
и необязательные ключевые слова, задающие цвет, размер, стиль, выравнивание 
и другие свойства текста. В данном случае мы использовали значения ha='right' 
и ha='center', где ha — сокращение от horizontal alignment («выравнивание по го-
ризонтали»). См. дальнейшую информацию об имеющихся настройках в docstring 
функций plt.text() и mpl.text.Text().

--- СТРАНИЦА 317 ---
Текст и поясняющие надписи 317
Преобразования и координаты текста
В предыдущем примере мы привязали наши текстовые пояснения к конкретным 
значениям данных. Иногда бывает удобнее привязать текст к координатам на осях 
рисунка, независимо от данных. В библиотеке Matplot lib это осуществляется путем 
модификации преобразования (transform).
Всем фреймворкам отображения графики необходимы схемы преобразования 
между разными системами координат. Например, точку данных с координа -
тами (x, y) = (1, 1) следует представить в виде точки в определенном месте на 
рисунке, который, в свою очередь, необходимо представить в виде пикселов на 
экране. С математической точки зрения подобные преобразования несложны, 
и сама библиотека Matplotlib внутри использует для их выполнения имеющийся 
в ней набор инструментов (эти инструменты можно найти в подмодуле matplot-
lib.transforms ).

\1
 ax.transData — преобразование из системы координат данных;
 ax.transAxes — преобразование из системы координат объекта Axes (в единицах 
размеров рисунка);
 fig.transFigure — преобразование из системы координат объекта Figure (в еди -
ницах размеров рисунка)

\1
In[5]: fig, ax = plt.subplots(facecolor='lightgray')
 ax.axis([0, 10, 0, 10])
 # transform=ax.transData – значение по умолчанию, 
 # но мы все равно указываем его
 ax.text(1, 5, ". Data: (1, 5)", transform=ax.transData)
 ax.text(0.5, 0.1, ". Axes: (0.5, 0.1)", transform=ax.transAxes)
 ax.text(0.2, 0.2, ". Figure: (0.2, 0.2)", transform=fig.transFigure);
Обратите внимание, что по умолчанию текст выравнивается по базовой линии 
и левому краю указанных координат, поэтому "." в начале каждой строки при -
близительно отмечает здесь заданные координаты.
Координаты transData задают обычные координаты данных, соответствующие 
меткам на осях X и Y . Координаты transAxes задают местоположение, считая от 
нижнего левого угла системы координат (здесь — белый прямоугольник), в виде 

--- СТРАНИЦА 318 ---

\1nsFigure схожи с transAxes, 
но задают местоположение, считая от нижнего левого угла рисунка (здесь — серый 
прямоугольник) в виде доли от размера рисунка.
Рис. 4.69. Сравнение различных систем координат библиотеки Matplotlib
Отмечу, что, если поменять пределы осей координат, это повлияет только на коор -
динаты transData, а другие останутся неизменными (рис. 4.70):
In[6]: ax.set_xlim(0, 2)
 ax.set_ylim(-6, 6)
 fig
Рис. 4.70. Сравнение различных систем координат библиотеки Matplotlib
Наблюдать это поведение более наглядно можно путем интерактивного изменения 
пределов осей координат. При выполнении кода в блокноте этого можно добиться, 
заменив %matplotlib inline на %matplotlib notebook и воспользовавшись меню 
каждого из графиков для работы с ним.

--- СТРАНИЦА 319 ---
Текст и поясняющие надписи 319
Стрелки и поясняющие надписи
Наряду с отметками делений и текстом удобной поясняющей меткой является 
простая стрелка.
Рисование стрелок в Matplotlib зачастую оказывается более сложной задачей, чем 
вы могли бы предполагать. Несмотря на существование функ ции plt.arrow() , 
использовать ее я бы не советовал: создаваемые ею стрелки представляют собой 
SVG-объекты, подверженные изменениям в зависимости от соотношения сторон 
графиков, поэтому результат редко оказывается соответствующим ожиданиям. 
Вместо этого я предложил бы воспользоваться функцией plt.annotate() . Она 
создает текст и стрелку, причем позволяет очень гибко задавать настройки для 
стрелки.
В следующем фрагменте кода мы используем функцию annotate с несколькими 
параметрами (рис. 4.71):
In[7]: %matplotlib inline
 fig, ax = plt.subplots()
 x = np.linspace(0, 20, 1000)
 ax.plot(x, np.cos(x))
 ax.axis('equal')
 ax.annotate('local maximum', xy=(6.28, 1), xytext=(10, 4),
 arrowprops=dict(facecolor='black', shrink=0.05))
 ax.annotate('local minimum', xy=(5 * np.pi, -1), xytext=(2, -6),
 arrowprops=dict(arrowstyle="->", 
 connectionstyle="angle3,angleA=0,angleB=-90"));
Рис. 4.71. Примеры поясняющих надписей
Стилем стрелки можно управлять с помощью словаря arrowprops с множеством 
параметров. Эти параметры отлично описаны в онлайн-документации библиотеки 

--- СТРАНИЦА 320 ---

\1n[8]:
fig, ax = plt.subplots(figsize=(12, 4))
births_by_date.plot(ax=ax)
# Добавляем на график метки
ax.annotate("New Year's Day", xy=('2012-1-1', 4100), xycoords='data',
 xytext=(50, -30), textcoords='offset points',
 arrowprops=dict(arrowstyle="->",
 connectionstyle="arc3,rad=-0.2"))
ax.annotate("Independence Day", xy=('2012-7-4', 4250), xycoords='data',
 bbox=dict(boxstyle="round", fc="none", ec="gray"),
 xytext=(10, -40), textcoords='offset points', ha='center',
 arrowprops=dict(arrowstyle="->"))
ax.annotate('Labor Day', xy=('2012-9-4', 4850), xycoords='data', ha='center',
 xytext=(0, -20), textcoords='offset points')
ax.annotate('', xy=('2012-9-1', 4850), xytext=('2012-9-7', 4850),
 xycoords='data', textcoords='data',
 arrowprops={'arrowstyle': '|-|,widthA=0.2,widthB=0.2', })
ax.annotate('Halloween', xy=('2012-10-31', 4600), xycoords='data',
 xytext=(-80, -40), textcoords='offset points',
 arrowprops=dict(arrowstyle="fancy",
 fc="0.6", ec="none",
 connectionstyle="angle3,angleA=0,angleB=-90"))
ax.annotate('Thanksgiving', xy=('2012-11-25', 4500), xycoords='data',
 xytext=(-120, -60), textcoords='offset points',
 bbox=dict(boxstyle="round4,pad=.5", fc="0.9"),
 arrowprops=dict(arrowstyle="->",
 connectionstyle="angle,angleA=0,angleB=80,rad=20"))
ax.annotate('Christmas', xy=('2012-12-25', 3850), xycoords='data',
 xytext=(-30, 0), textcoords='offset points',
 size=13, ha='right', va="center",
 bbox=dict(boxstyle="round", alpha=0.1),
 arrowprops=dict(arrowstyle="wedge, tail_width=0.5",
 alpha=0.1));
# Задаем метки для осей координат
ax.set(title='USA births by day of year (1969-1988)',
 ylabel='average daily births')
# Размечаем ось X центрированными метками для месяцев
ax.xaxis.set_major_locator(mpl.dates.MonthLocator())
ax.xaxis.set_minor_locator(mpl.dates.MonthLocator(bymonthday=15))
ax.xaxis.set_major_formatter(plt.NullFormatter())

--- СТРАНИЦА 321 ---
Пользовательские настройки делений на осях координат 321
ax.xaxis.set_minor_formatter(mpl.dates.DateFormatter('%h'));
ax.set_ylim(3600, 5400);
Вы видите, что спецификации стрелок и текстовых полей очень подробны. Благо-
даря этому мы можем создавать стрелки практически любого нужного нам вида. 
К сожалению, это также означает, что подобные элементы требуют отладки вруч-
ную — процесс, занимающий немало времени, если речь идет о создании графики 
типографского уровня качества! Наконец, отмечу, что использовать для представ -
ления данных продемонстрированную выше смесь стилей я отнюдь не рекомендую, 
она дана в качестве примера возможностей.
Рис. 4.72. Средняя рождаемость по дням с пояснениями
Дальнейшее обсуждение и примеры стилей стрелок и поясняющих надписей 
можно найти в галерее библиотеки Matplotlib по адресу http://matplotlib.org/examples/
pylab_examples/annotation_demo2.html.
Пользовательские настройки делений 
на осях координат
Локаторы и форматеры делений на осях, используемые по умолчанию в библиотеке 
Matplotlib, спроектированы так, что в большинстве обычных ситуаций их вполне 
достаточно, хотя они отнюдь не оптимальны для всех графиков. В этом разделе мы 
рассмотрим несколько примеров настройки расположения делений и их формати-
рования для конкретных интересующих нас видов графиков.
Прежде чем перейти к примерам, следует разобраться в объектной иерархии гра-
фиков библиотеки Matplotlib. Matplotlib старается делать объектами языка Python 
все элементы на графике, например объект figure — ограничивающий снаружи 
все элементы графика прямоугольник. Каждый объект библ иотеки Matplotlib 
также служит контейнером подобъектов. Например, любой объект figure может 

--- СТРАНИЦА 322 ---

\1n[1]: %matplotlib inline
 import matplotlib.pyplot as plt
 plt.style.use('seaborn-whitegrid')
 import numpy as np
In[2]: ax = plt.axes(xscale='log', yscale='log')

\1
In[3]: print(ax.xaxis.get_major_locator())
 print(ax.xaxis.get_minor_locator())
<matplotlib.ticker.LogLocator object at 0x107530cc0>
<matplotlib.ticker.LogLocator object at 0x107530198>

--- СТРАНИЦА 323 ---
Пользовательские настройки делений на осях координат 323
In[4]: print(ax.xaxis.get_major_formatter())
 print(ax.xaxis.get_minor_formatter())
<matplotlib.ticker.LogFormatterMathtext object at 0x107512780>
<matplotlib.ticker.NullFormatter object at 0x10752dc18>

\1
In[5]: ax = plt.axes()
 ax.plot(np.random.rand(50))
 ax.yaxis.set_major_locator(plt.NullLocator())
 ax.xaxis.set_major_formatter(plt.NullFormatter())
Рис. 4.74. График со скрытыми метками делений (ось X) 
и скрытыми делениями (ось Y)
Обратите внимание, что мы убрали метки (но оставили деления/линии коорди -
натной сетки) с оси X , и убрали деления (а следовательно, и метки) с оси Y . От-
сутствие делений может быть полезно во многих случаях, например, если нужно 
отобразить сетку изображений. Например, рассмотрим рис. 4.75, содержащий 

--- СТРАНИЦА 324 ---

\1n[6]: fig, ax = plt.subplots(5, 5, figsize=(5, 5))
 fig.subplots_adjust(hspace=0, wspace=0)
 # Получаем данные по лицам людей из библиотеки scikit-learn
 from sklearn.datasets import fetch_olivetti_faces
 faces = fetch_olivetti_faces().images
 for i in range(5):
 for j in range(5):
 ax[i, j].xaxis.set_major_locator(plt.NullLocator())
 ax[i, j].yaxis.set_major_locator(plt.NullLocator())
 ax[i, j].imshow(faces[10 * i + j], cmap="bone")

\1
In[7]: fig, ax = plt.subplots(4, 4, sharex=True, sharey=True)

--- СТРАНИЦА 325 ---

\1
In[8]: # Задаем, для всех систем координат, локаторы основных делений осей X и Y 
 for axi in ax.flat:
 axi.xaxis.set_major_locator(plt.MaxNLocator(3))
 axi.yaxis.set_major_locator(plt.MaxNLocator(3))
 fig
Рис. 4.77. Пользовательские настройки количества делений
Благодаря этому рисунок становится гораздо понятнее. При необходимости еще более 
точного контроля расположения делений с равными интервалами можно воспользо-
ваться классом plt.MultipleLocator, который мы обсудим в следующем разделе.
Более экзотические форматы делений
Форматирование делений, используемое по умолчанию в библиотеке Matplotlib, 
оставляет желать лучшего. В качестве варианта по умолчанию, который бы 

--- СТРАНИЦА 326 ---

\1n[9]: # Строим графики синуса и косинуса
 fig, ax = plt.subplots()
 x = np.linspace(0, 3 * np.pi, 1000)
 ax.plot(x, np.sin(x), lw=3, label='Sine')
 ax.plot(x, np.cos(x), lw=3, label='Cosine')
 # Настраиваем сетку, легенду и задаем пределы осей координат
 ax.grid(True)
 ax.legend(frameon=False)
 ax.axis('equal')
 ax.set_xlim(0, 3 * np.pi);

\1
In[10]: ax.xaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))
 ax.xaxis.set_minor_locator(plt.MultipleLocator(np.pi / 4))
 fig
Рис. 4.79. Деления в точках, кратных π/2

--- СТРАНИЦА 327 ---
Пользовательские настройки делений на осях координат 327
Но теперь эти метки делений выглядят несколько по-дурацки: можно догадаться, 
что они кратны π, но из десятичного представления сразу это непонятно. Необхо-
димо модифицировать форматер деления. Встроенного форматера для нашей зада-
чи нет, поэтому мы воспользуемся форматером plt.FuncFormatter, принимающим 
на входе пользовательскую функцию, обеспечивающую более точный контроль за 
форматом вывода делений (рис. 4.80):
In[11]: def format_func(value, tick_number):
 # Определяем количество кратных пи/2 значений 
 # [в требуемом диапазоне]
 N = int(np.round(2 * value / np.pi))
 if N == 0:
 return "0"
 elif N == 1:
 return r"$\pi/2$"
 elif N == 2:
 return r"$\pi$"
 elif N % 2 > 0:
 return r"${0}\pi/2$".format(N)
 else:
 return r"${0}\pi$".format(N // 2)
 ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))
 fig
Рис. 4.80. Деления с пользовательскими метками
Намного лучше! Обратите внимание: мы воспользовались тем, что библиотека 
Matplotlib поддерживает систему верстки LaTeX. Для ее испол ьзования необ -
ходимо заключить нужную строку в знаки доллара с двух сторон. Это облегчает 
отображение математических символов и формул. В нашем случае "$\pi$" визуа-
лизируется в виде греческой буквы π.
Форматер plt.FuncFormatter обеспечивает возможность чрезвычайно тонкого 
контроля внешнего вида делений графика и очень удобен при подготовке графиков 
для презентаций или публикации.

--- СТРАНИЦА 328 ---

\1ng или в онлайн-документации 
библиотеки Matplotlib. Все форматеры и локаторы доступны в пространстве имен 
plt (табл. 4.1, 4.2).
Таблица 4.1. Локаторы
Класс локатора Описание
NullLocator Без делений
FixedLocator Положения делений фиксированы
IndexLocator Локатор для графика индексированной переменной (например, 
для x = range(len(y)))
LinearLocator Равномерно распределенные деления от min до max
LogLocator Логарифмически распределенные деления от min до max
MultipleLocator Деления и диапазон значений кратны заданному основанию
MaxNLocator Находит удачные местоположения для делений в количестве, не превы-
шающем заданного максимального числа
AutoLocator (По умолчанию.) MaxNLocator с простейшими значениями по умолчанию
AutoMinorLocator Локатор для промежуточных делений
Таблица 4.2. Форматеры
Класс форматера Описание
NullFormatter Деления без меток
IndexFormatter Задает строковые значения для меток на основе списка меток
FixedFormatter Позволяет задавать строковые значения для меток вручную
FuncFormatter Значения меток задаются с помощью пользовательской функции
FormatStrFormatter Для всех значений используется строка формата
ScalarFormatter (По умолчанию.) Форматер для скалярных значений
LogFormatter Форматер по умолчанию для логарифмических систем координат

\1
In[1]: import matplotlib.pyplot as plt
 plt.style.use('classic')
 import numpy as np
 %matplotlib inline
In[2]: x = np.random.randn(1000)
 plt.hist(x);

\1
In[3]: # Используем серый фон
 ax = plt.axes(axisbg='#E6E6E6')
 ax.set_axisbelow(True)
 # Рисуем сплошные белые линии сетки
 plt.grid(color='w', linestyle='solid')
 # Скрываем основные линии осей координат
 for spine in ax.spines.values():
 spine.set_visible(False)
 # Скрываем деления сверху и справа

--- СТРАНИЦА 330 ---

\1n='out')
 for tick in ax.get_xticklabels():
 tick.set_color('gray')
 for tick in ax.get_yticklabels():
 tick.set_color('gray')
 # Задаем цвет заливки и границ гистограммы
 ax.hist(x, edgecolor='#E6E6E6', color='#EE6666');

\1
In[4]: IPython_default = plt.rcParams.copy()

\1

--- СТРАНИЦА 331 ---
Пользовательские настройки Matplotlib: конфигурации и таблицы стилей 331
In[5]: from matplotlib import cycler
 colors = cycler('color',
 ['#EE6666', '#3388BB', '#9988DD',
 '#EECC55', '#88BB44', '#FFBBBB'])
 plt.rc('axes', facecolor='#E6E6E6', edgecolor='none',
 axisbelow=True, grid=True, prop_cycle=colors)
 plt.rc('grid', color='w', linestyle='solid')
 plt.rc('xtick', direction='out', color='gray')
 plt.rc('ytick', direction='out', color='gray')
 plt.rc('patch', edgecolor='#E6E6E6')
 plt.rc('lines', linewidth=2)

\1
In[6]: plt.hist(x);

\1
In[7]: for i in range(4):
 plt.plot(np.random.rand(10))
Рис. 4.84. Линейный график с пользовательскими стилями

--- СТРАНИЦА 332 ---

\1n[8]: plt.style.available[:5]
Out[8]: ['fivethirtyeight',
 'seaborn-pastel',
 'seaborn-whitegrid',
 'ggplot',
 'grayscale']

\1
plt.style.use('stylename')

\1
with plt.style.context('stylename'):
 make_a_plot()

\1
In[9]: def hist_and_lines():
 np.random.seed(0)
 fig, ax = plt.subplots(1, 2, figsize=(11, 4))
 ax[0].hist(np.random.randn(1000))
 for i in range(3):
 ax[1].plot(np.random.rand(10))
 ax[1].legend(['a', 'b', 'c'], loc='lower left')

\1
In[10]: # Восстанавливаем rcParams
 plt.rcParams.update(IPython_default);

\1
In[11]: hist_and_lines()
Рис. 4.85. Стиль библиотеки Matplotlib по умолчанию
Стиль FiveThirtyEight
Стиль FiveThirtyEight подражает оформлению популярного сайта FiveThirtyEight 
( http://fivethirtyeight.com/). Как вы можете видеть на рис. 4.86, он включает жирные 
шрифты, толстые линии и прозрачные оси координат.
In[12]: with plt.style.context('fivethirtyeight'):
 hist_and_lines()
Рис. 4.86. Стиль FiveThirtyEight

--- СТРАНИЦА 334 ---

\1n[13]: with plt.style.context('ggplot'):
 hist_and_lines()
Рис. 4.87. Стиль ggplot
Стиль «байесовские методы для хакеров»
Существует замечательная онлайн-книга «Вероятностное программирование и байе-
совские методы для хакеров» (Probabilistic Programming and Bayesian Methods for 
Hackers , http://bit.ly/2fDJsKC). Она содержит рисунки, созданные с помощью библио-
теки Matplotlib, и использует в книге для создания единообразного и приятного 
внешне стиля набор параметров rc. Этот стиль воспроизведен в таблице стилей 
bmh (рис. 4.88):
In[14]: with plt.style.context('bmh'):
 hist_and_lines()
Рис. 4.88. Стиль bmh

--- СТРАНИЦА 335 ---
Пользовательские настройки Matplotlib: конфигурации и таблицы стилей 335
Темный фон
Для используемых в презентациях рисунков часто удобнее темный, а не светлый 
фон. Эту возможность предоставляет стиль dark_background (рис. 4.89):
In[15]: with plt.style.context('dark_background'):
 hist_and_lines()
Рис. 4.89. Стиль dark_background

\1
In[16]: with plt.style.context('grayscale'):
 hist_and_lines()
Рис. 4.90. Стиль grayscale
Стиль Seaborn
В библиотеке Matplotlib есть также стили, источником вдохновения для которых 
послужила библиотека Seaborn (обсуждаемая подробнее в разделе «Визуализация 

--- СТРАНИЦА 336 ---

\1n» данной главы). Как мы увидим, эти стили загру-
жаются автоматически при импорте пакета Seaborn в блокнот. Мне эти настройки 
очень нравятся, я склонен использовать их как настр ойки по умолчанию в моих 
собственных исследованиях данных (рис. 4.91):
In[17]: import seaborn
 hist_and_lines()
Рис. 4.91. Стили построения графиков 
библиотеки Seaborn

\1
In[1]: from mpl_toolkits import mplot3d
После импорта этого подмодуля появляется возможность создавать трехмерные 
системы координат путем передачи ключевого слова projection='3d' любой из 
обычных функций создания систем координат:

--- СТРАНИЦА 337 ---
Построение трехмерных графиков в библиотеке Matplotlib 337
In[2]: %matplotlib inline
 import numpy as np
 import matplotlib.pyplot as plt
In[3]: fig = plt.figure()
 ax = plt.axes(projection='3d')
Рис. 4.92. Пустая трехмерная система координат
С помощью такой трехмерной системы координат можно строить различные 
виды трехмерных графиков. Построение трехмерных графиков — один из видов 
функциональности, для которого очень полезен интерактивный, а не статический 
просмотр рисунков в блокноте. Напомню, что для работы с интерактивными ри-
сунками необходимо вместо команды %matplotlib inline использовать команду 
%matplotlib notebook.

\1
In[4]: ax = plt.axes(projection='3d')
 # Данные для трехмерной кривой 
 zline = np.linspace(0, 15, 1000)
 xline = np.sin(zline)

--- СТРАНИЦА 338 ---

\1ne = np.cos(zline)
 ax.plot3D(xline, yline, zline, 'gray')
 # Данные для трехмерных точек
 zdata = 15 * np.random.random(100)
 xdata = np.sin(zdata) + 0.1 * np.random.randn(100)
 ydata = np.cos(zdata) + 0.1 * np.random.randn(100)
 ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Greens');
Рис. 4.93. Точки и линии в трех измерениях
Обратите внимание, что по умолчанию степень прозрачности рассеянных по 
графику точек настраивается таким образом, чтобы придать графику эффект 
глубины. Хотя в статическом изображении этот трехмерный эффект иногда 
незаметен, динамическое представление может дать информацию о топологии 
точек.
Трехмерные контурные графики
Аналогично контурным графикам, рассмотренным нами в разделе «Графики 
плотности и контурные графики» этой главы, mplot3d содержит инструменты для 
создания трехмерных рельефных графиков на основе тех же входных данных. По-
добно двумерным графикам, создаваемым с помощью функции ax.contour, для 
функции ax.contour3D необходимо, чтобы все входные данные находились в фор-
ме двумерных регулярных сеток, с вычислением данных по оси Z в каждой точке. 

\1
In[5]: def f(x, y):
 return np.sin(np.sqrt(x ** 2 + y ** 2))
 x = np.linspace(-6, 6, 30)
 y = np.linspace(-6, 6, 30)
 X, Y = np.meshgrid(x, y)

--- СТРАНИЦА 339 ---
Построение трехмерных графиков в библиотеке Matplotlib 339
 Z = f(X, Y)
In[6]: fig = plt.figure()
 ax = plt.axes(projection='3d')
 ax.contour3D(X, Y, Z, 50, cmap='binary')
 ax.set_xlabel('x')
 ax.set_ylabel('y')
 ax.set_zlabel('z');
Рис. 4.94. Трехмерный контурный график
Иногда используемый по умолчанию угол зрения неидеален. В этом случае можно 
воспользоваться методом view_init для задания угла возвышения и азимутального 
угла. В нашем примере (результат которого показан на рис. 4.95) используется угол 
возвышения 60 градусов (то есть 60 градусов над плоскостью X-Y ) и азимут 35 гра -
дусов (то есть график повернут на 35 градусов против часовой стрелки вокруг оси Z ):
In[7]: ax.view_init(60, 35)
 fig
Рис. 4.95. Настройка угла зрения для трехмерного графика
Выполнить такое вращение можно и интерактивно, щелчком кнопкой мыши и пе-
ретаскиванием, при использовании одной из интерактивных прикладных частей 
библиотеки Matplotlib.

--- СТРАНИЦА 340 ---

\1n[8]: fig = plt.figure()
 ax = plt.axes(projection='3d')
 ax.plot_wireframe(X, Y, Z, color='black')
 ax.set_title('wireframe'); # Каркас

\1
In[9]: ax = plt.axes(projection='3d')
 ax.plot_surface(X, Y, Z, rstride=1, cstride=1,
 cmap='viridis', edgecolor='none')
 ax.set_title('surface'); # Поверхность

\1
In[10]: r = np.linspace(0, 6, 20)
 theta = np.linspace(-0.9 * np.pi, 0.8 * np.pi, 40)
 r, theta = np.meshgrid(r, theta)
 X = r * np.sin(theta)
 Y = r * np.cos(theta)
 Z = f(X, Y)
 ax = plt.axes(projection='3d')
 ax.plot_surface(X, Y, Z, rstride=1, cstride=1,
 cmap='viridis', edgecolor='none');
Рис. 4.98. Поверхностный график в полярной системе координат
Триангуляция поверхностей
В некоторых приложениях необходимые для предыдущих операций равномерно 
дискретизированные координатные сетки неудобны и накладывают слишком 
много ограничений. В таких случаях могут быть удобны графики, основанные на 
координатной сетке из треугольников. Что, если вместо равномерно выбранных 
значений из декартовой или полярной систем координат, мы имеем дело с набором 
случайно выбранных значений?
In[11]: theta = 2 * np.pi * np.random.random(1000)
 r = 6 * np.random.random(1000)
 x = np.ravel(r * np.sin(theta))
 y = np.ravel(r * np.cos(theta))
 z = f(x, y)

--- СТРАНИЦА 342 ---

\1n[12]: ax = plt.axes(projection='3d')
 ax.scatter(x, y, z, c=z, cmap='viridis', linewidth=0.5);

\1
In[13]: ax = plt.axes(projection='3d')
 ax.plot_trisurf(x, y, z, cmap='viridis', edgecolor='none');

\1
In[14]: theta = np.linspace(0, 2 * np.pi, 30)
 w = np.linspace(-0.25, 0.25, 8)
 w, theta = np.meshgrid(w, theta)
Теперь нам нужно на основе этой параметризации вычислить координаты (x, y, z) 
ленты.
Размышляя, можно понять, что в данном случае происходят два вращательных 
движения: одно — изменение расположения кольца относительно его центра (ко-
ордината, которую мы назвали θ), а второе — скручивание полоски относительно 
ее оси координат (назовем эту координату ϕ). Чтобы получилась лента Мебиуса, 
полоска должна выполнить половину скручивания за время полного сворачивания 
в кольцо, то есть Δϕ = Δθ / 2 .
In[15]: phi = 0.5 * theta

\1
In[16]: # радиус в плоскости X-Y
 r = 1 + w * np.cos(phi)
 x = np.ravel(r * np.cos(theta))
 y = np.ravel(r * np.sin(theta))
 z = np.ravel(w * np.sin(phi))

\1
In[17]: # Выполняем триангуляцию в координатах базовой параметризации
 from matplotlib.tri import Triangulation
 tri = Triangulation(np.ravel(w), np.ravel(theta))
 ax = plt.axes(projection='3d')
 ax.plot_trisurf(x, y, z, triangles=tri.triangles,
 cmap='viridis', linewidths=0.2);
 ax.set_xlim(-1, 1); ax.set_ylim(-1, 1); ax.set_zlim(-1, 1);

--- СТРАНИЦА 344 ---

\1n. В этом разделе 
мы продемонстрируем несколько примеров, возможных благодаря этому набору 
инструментов визуализаций карт.
Установка набора программ Basemap не представляет сл ожности. Если вы ис -
пользуете conda, для скачивания и установки пакета вам достаточно набрать 
команду:
$ conda install basemap1

\1

\1n 2 или 
же Python 3 с младшей версией не выше 3.3. При необходимости вы можете установить на 
свою машину неофициальный выпуск пакета Basemap с помощью следующей команды:
 conda install -c conda-forge basemap=1.0.8.dev0.

--- СТРАНИЦА 345 ---
Отображение географических данных с помощью Basemap 345
In[1]: %matplotlib inline
 import numpy as np
 import matplotlib.pyplot as plt
 from mpl_toolkits.basemap import Basemap
Лишь несколько строк кода отделяет установку и импорт набора инструмен -
тов Basemap от построения географических графиков (построение графика 
на рис. 4.102 требует также наличия пакета PIL в Python 2 или пакета pillow 
в Python 3):
In[2]: plt.figure(figsize=(8, 8))
 m = Basemap(projection='ortho', resolution=None, lat_0=50, lon_0=-100)
 m.bluemarble(scale=0.5);

\1

--- СТРАНИЦА 346 ---

\1n[3]: fig = plt.figure(figsize=(8, 8))
 m = Basemap(projection='lcc', resolution=None,
 width=8E6, height=8E6,
 lat_0=45, lon_0=-100,)
 m.etopo(scale=0.5, alpha=0.5)
 # Проецируем кортеж (долгота, широта) на координаты (x, y) 
 # для построения графика
 x, y = m(-122.3, 47.6)
 plt.plot(x, y, 'ok', markersize=5)
 plt.text(x, y, ' Seattle', fontsize=12);
Рис. 4.103. Наносим на карту данные и метки
Приведенный пример позволил нам взглянуть на то, какие географические ви -
зуализации возможны с помощью всего нескольких строк кода на языке Python. 

\1
In[4]: from itertools import chain
 def draw_map(m, scale=0.2):
 # Отрисовываем изображение с оттененным рельефом
 m.shadedrelief(scale=scale)
 # Значения широты и долготы возвращаются в виде словаря
 lats = m.drawparallels(np.linspace(-90, 90, 13))
 lons = m.drawmeridians(np.linspace(-180, 180, 13))
 # Ключи, содержащие экземпляры класса plt.Line2D 
 lat_lines = chain(*(tup[1][0] for tup in lats.items()))
 lon_lines = chain(*(tup[1][0] for tup in lons.items()))
 all_lines = chain(lat_lines, lon_lines)
 # Выполняем цикл по этим линиям, устанавливая нужный стиль
 for line in all_lines:
 line.set(linestyle='-', alpha=0.3, color='w')
Цилиндрические проекции
Простейшие из картографических проекций — цилиндрические, в которых линии 
одинаковой широты и долготы проецируются на горизонтальные и вертикальные 
линии соответственно. Такой тип проекции хорошо подходит для отображения об-
ластей у экватора, но приводит к сильной дисторсии возле полюсов. В различных 
цилиндрических проекциях расстояния между линиями широты различаются, что 
приводит к различной степени сохранения пропорций и дисторсии в районе полю-
сов. На рис. 4.104 показан пример равнопромежуточной цилиндрической проекции 
(equidistant cylindrical projection), характеризующейся выбором такого масштабиро -
вания широты, при котором расстояния вдоль меридианов остаются неизменными. 
Среди других цилиндрических проекций — проекция Меркатора (projection='merc') 
и равновеликая цилиндрическая проекция (equal-area cylindrical projection).
In[5]: fig = plt.figure(figsize=(8, 6), edgecolor='w')
 m = Basemap(projection='cyl', resolution=None,
 llcrnrlat=-90, urcrnrlat=90,
 llcrnrlon=-180, urcrnrlon=180, )
 draw_map(m)

--- СТРАНИЦА 348 ---

\1n) нижнего левого ( llcrnr) и верхнего правого 
( urcrnr) углов карты в градусах.
Псевдоцилиндрические проекции
В псевдоцилиндрических проекциях отсутствует требов ание вертикальности 
меридианов (линии одинаковой долготы), это позволяет улучшить показатели 
возле полюсов проекции. Проекция Мольвейде ( projection='moll' ) — распро -
страненный пример псевдоцилиндрической проекции, в которой вс е меридианы 
представляют собой эллиптические дуги (рис. 4.105). Она разработана с целью 
сохранения пропорций на всей площади карты: хотя имеются некоторые дистор-
сии около полюсов, мелкие участки отображаются правдоподобно. В числе других 
псевдоцилиндрических проекций — синусоидальная проекция (projection='sinu') 
и проекция Робинсона ( projection='robin').
In[6]: fig = plt.figure(figsize=(8, 6), edgecolor='w')
 m = Basemap(projection='moll', resolution=None,
 lat_0=0, lon_0=0)
 draw_map(m)
Рис. 4.105. Проекция Мольвейде

--- СТРАНИЦА 349 ---
Отображение географических данных с помощью Basemap 349
Передаваемые в Basemap дополнительные аргументы относятся к широте ( lat_0) 
и долготе ( lon_0) центра карты.
Перспективные проекции
Перспективные проекции создаются путем выбора конкретной главной точ -
ки, аналогично фотографированию Земли из конкретной точки пространства 
(в некоторых проекциях эта точка формально находится внутри Земли!). Рас -
пространенный пример — ортографическая проекция ( projection='ortho' ), 
показывающая одну сторону земного шара так, как его бы видел наблюдатель 
с очень большого расстояния. Таким образом, при ней можно видеть одновре -
менно только половину земного шара. Среди других перспективных проекций — 
гномоническая ( projection='gnom') и стереографическая ( projection='stere') 
проекции. Они лучше всего подходят для отображения небольших участков 
карты.

\1
In[7]: fig = plt.figure(figsize=(8, 8))
 m = Basemap(projection='ortho', resolution=None,
 lat_0=50, lon_0=0)
 draw_map(m);
Рис. 4.106. Ортографическая проекция

--- СТРАНИЦА 350 ---

\1n='lcc'), которую мы уже видели в карте Северной Америки. При 
ее использовании карта проецируется на конус, устроенный таким образом, чтобы 
сохранялись расстояния на двух стандартных параллелях (задаваемых в Basemap 
с помощью аргументов lat_1 и lat_2), в то время как между ними масштаб был 
меньше реального, а за их пределами — больше реального. Другие удобные кониче-
ские проекции — равнопромежуточная коническая проекция ( projection='eqdc') 
и равновеликая коническая проекция Альберса ( projection='aea') (рис. 4.107). 
Конические проекции, как и перспективные проекции, хорошо подходят для от -
ражения маленьких и средних кусков земного шара.
In[8]: fig = plt.figure(figsize=(8, 8))
 m = Basemap(projection='lcc', resolution=None,
 lon_0=0, lat_0=50, lat_1=45, lat_2=55,
 width=1.6E7, height=1.2E7)
 draw_map(m)
Рис. 4.107. Равновеликая коническая проекция Альберса
Другие проекции
Если вы собираетесь часто иметь дело с картографическими визуализациями, 
рекомендую почитать и о других проекциях, их свойствах, преимуществах и недо -
статках. Скорее всего, они имеются в пакете Basemap (http://matplotlib.org/basemap/

--- СТРАНИЦА 351 ---
Отображение географических данных с помощью Basemap 351
users/mapsetup.html) . Окунувшись в этот вопрос, вы обнаружите поразительную 
субкультуру фанатов геовизуализаций, яростно отстаивающих преимущество их 
излюбленной проекции для любого приложения!
Отрисовка фона карты
Ранее мы рассмотрели методы bluemarble() и shadedrelief() , предназначен -
ные для проекций изображений всего земного шара на карту, а также методы 
drawparallels() и drawmeridians()для рисования линий с постоянной широтой 
или долготой. Пакет Basemap содержит множество удобн ых функций для рисо -
вания границ физических объектов, например континентов, океанов, озер и рек, 
а также политических границ — границ стран или штатов/округов США. Далее 
приведены некоторые из имеющихся функций рисования, возможно, вы захотите 
изучить их подробнее с помощью справочных средств оболочки IPython.
 Физические границы и водоемы:
 y drawcoastlines() — рисует континентальные береговые линии;
 y drawlsmask() — рисует маску «земля/море» с целью проекции изображений 
на то или другое;
 y drawmapboundary() — рисует границы на карте, включая заливку цветом 
океанов;
 y drawrivers() — рисует реки на карте;
 y fillcontinents() — заливает пространство континентов заданным цветом; 
в качестве дополнительной настройки может залить озера другим цветом.
 Политические границы:
 y drawcountries() — рисует границы стран;
 y drawstates() — рисует границы штатов США;
 y drawcounties() — рисует границы округов США.
 Свойства карты:
 y drawgreatcircle() — рисует большой круг между двумя точками;
 y drawparallels() — рисует линии с постоянной широтой (меридианы);
 y drawmeridians() — рисует линии с постоянной долготой (параллели);
 y drawmapscale() — рисует на карте линейную шкалу масштаба.
 Изображения всего земного шара:
 y bluemarble() — проецирует сделанную NASA фотографию «голубого ша -
рика» на карту;
 y shadedrelief() — проецирует на карту изображение с оттененным рельефом;
 y etopo() — рисует на карте изображение рельефа на основе набора данных etopo;
 y warpimage() — проецирует на карту пользовательское изображение.

--- СТРАНИЦА 352 ---

\1n класса Basemap задает уровень 
детализации границ 1 ( 'c' (от англ. crude) — грубая детализация, 'l' (от англ. 
low) — низкая детализация, 'i' (от англ. intermediate) — средняя детализация, 'h' 
(от англ. high) — высокая детализация, 'f' (от англ. full) — полная детализация, 
или None — если границы не используются). Выбор значения этого параметра очень 
важен. Например, отрисовка границ с высоким разрешением на карте земного шара 
может происходить очень медленно.

\1
Рис. 4.108. Границы на карте при низком и высоком разрешении
In[9]: fig, ax = plt.subplots(1, 2, figsize=(12, 8))
 for i, res in enumerate(['l', 'h']):
 m = Basemap(projection='gnom', lat_0=57.3, lon_0=-6.2,
 width=90000, height=120000, resolution=res, ax=ax[i])
 m.fillcontinents(color="#FFDDCC", lake_color='#DDEEFF')
 m.drawmapboundary(fill_color="#DDEEFF")

\1nda install -c conda-forge basemap-data-hires

--- СТРАНИЦА 353 ---
Отображение географических данных с помощью Basemap 353
 m.drawcoastlines()
 ax[i].set_title("resolution='{0}'".format(res));
Обратите внимание, что при низком разрешении береговые линии на этом уровне 
масштабирования отображаются некорректно, а при высоком — достаточно хоро-
шо. Однако низкое разрешение отлично подходит для глобального представления 
и работает гораздо быстрее, чем загрузка данных по границам в высоком разреше-
нии для всего земного шара. Может потребоваться немного поэкспериментировать 
с разрешением для конкретного представления, чтобы найти нужное, лучше всего 
начать с быстро отрисовываемого графика с низким разрешением и наращивать 
разрешение по мере необходимости.
Нанесение данных на карты
Вероятно, самая полезная на практике возможность набора инструментов Basemap — 
умение наносить разнообразные данные поверх фонового изображения карты. Для 
построения простых диаграмм и текста на картах подойдет любая из функций plt. 
Чтобы нанести данные на карту с помощью plt, можно воспользоваться для проек-
ции координат широты и долготы на координаты (X , Y ) экземпляром класса Basemap, 
как мы уже делали ранее в примере с Сиэтлом.
Помимо этого, среди методов экземпляра Basemap имеется множество функций, 
специально предназначенных для работы с картами. Они работают очень схоже 
со своими аналогами из библиотеки Matplotlib, но принимают дополнительный 
булев аргумент latlon, позволяющий (при равном True значении) передавать им 
исходные значения широты и долготы, а не их проекции на координаты ( X , Y ) .

\1
 contour()/contourf() — рисует контурные линии или заполненные контуры;
 imshow() — отображает изображение;
 pcolor() / pcolormesh() — рисует псевдоцветной график для нерегулярных 
и регулярных сеток;
 plot() — рисует линии и/или маркеры;
 scatter() — рисует точки с маркерами;
 quiver() — рисует вектора;
 barbs() — рисует стрелки ветра;
 drawgreatcircle() — рисует большой круг1 .
Мы рассмотрим примеры некоторых из этих функций далее. Дальнейшую инфор-
мацию о них, включая примеры графиков, можно найти в онлайн-документации 
Basemap.

\1n[10]: import pandas as pd
 cities = pd.read_csv('data/california_cities.csv')
 # Извлекаем интересующие нас данные
 lat = cities['latd'].values
 lon = cities['longd'].values
 population = cities['population_total'].values
 area = cities['area_total_km2'].values

\1
In[11]: # 1. Рисуем фон карты
 fig = plt.figure(figsize=(8, 8))
 m = Basemap(projection='lcc', resolution='h',
 lat_0=37.5, lon_0=-119,
 width=1E6, height=1.2E6)
 m.shadedrelief()
 m.drawcoastlines(color='gray')
 m.drawcountries(color='gray')
 m.drawstates(color='gray')
 # 2. Наносим данные по городам, отражая население разными цветами,
 # а площадь – разными размерами точек
 m.scatter(lon, lat, latlon=True,
 c=np.log10(population), s=area,
 cmap='Reds', alpha=0.5)
 # 3. Создаем шкалу цветов и легенду
 plt.colorbar(label=r'$\log_{10}({\rm population})$')
 plt.clim(3, 7)
 # Делаем легенду с фиктивными точками
 for a in [100, 300, 500]:
 plt.scatter([], [], c='k', alpha=0.5, s=a,
 label=str(a) + ' km$^2$')
 plt.legend(scatterpoints=1, frameon=False,
 labelspacing=1, loc='lower left');

--- СТРАНИЦА 355 ---
Отображение географических данных с помощью Basemap 355
Рис. 4.109. Диаграмма рассеяния поверх карты в качестве фона
Этот график демонстрирует приблизительно, в каких местах Калифорнии жи -
вет большое количество людей: они сосредоточены на побережье в районе Лос-
Анджелеса и Сан-Франциско, по бокам шоссе в Центральной долине, избегая 
практически полностью гористых районов на границах штата.
Пример: данные о температуре 
на поверхности Земли
В качестве примера визуализации непрерывных географических данных рассмо -
трим «полярный вихрь», охвативший западную половину США в январе 2014 года. 
Замечательный источник разнообразных метеорологических данных — подразде-
ление NASA Институт изучения космоса имени Годдарда (https://data.giss.na sa.gov/). 
В этом случае мы воспользуемся температурными данными GIS 250, которые 
можно скачать с помощью командной оболочки (возможно, на работающих под 
управлением операционной системы Windows эти команды придется несколько 
изменить). Используемые здесь данные скачивались 12 июня 2016 года, и их раз-
мер тогда составлял примерно 9 Мбайт:
In[12]: # !curl -O https://data.giss.nasa.gov/pub/gistemp/gistemp250.nc.gz
 # !gunzip gistemp250.nc.gz

--- СТРАНИЦА 356 ---

\1n можно прочитать 
с помощью библиотеки netCDF4. Установить эту библиотеку можно следующим 
образом:
$ conda install netcdf4

\1
In[13]: from netCDF4 import Dataset
 data = Dataset('gistemp250.nc')

\1
In[14]: from netCDF4 import date2index
 from datetime import datetime
 timeindex = date2index(datetime(2014, 1, 15),
 data.variables['time'])

\1
In[15]: lat = data.variables['lat'][:]
 lon = data.variables['lon'][:]
 lon, lat = np.meshgrid(lon, lat)
 temp_anomaly = data.variables['tempanomaly'][timeindex]
Воспользуемся методом pcolormesh() для отрисовки цветовой сетки наших 
данных. Нас интересует Северная Америка, и в качестве фона мы будем исполь -
зовать карту с оттененным рельефом. Обратите внимание, что для этих данных 
мы специально выбрали дивергентную карту цветов, с нейтральным цветом для 0 
и двумя контрастными цветами для отрицательных и положительных значений 
(рис. 4.110). 
В справочных целях мы также нарисуем светлым цветом поверх цветов берего -
вые линии: 
In[16]: fig = plt.figure(figsize=(10, 8))
 m = Basemap(projection='lcc', resolution='c',
 width=8E6, height=8E6,
 lat_0=45, lon_0=-100,)
 m.shadedrelief(scale=0.5)
 m.pcolormesh(lon, lat, temp_anomaly,
 latlon=True, cmap='RdBu_r')
 plt.clim(-8, 8)
 m.drawcoastlines(color='lightgray')
 plt.title('January 2014 Temperature Anomaly')
 plt.colorbar(label='temperature anomaly (°C)');
 # Температурные аномалии

--- СТРАНИЦА 357 ---
Визуализация с помощью библиотеки Seaborn 357
Рис. 4.110. Температурная аномалия в январе 2014 года
Диаграмма демонстрирует картину экстремальных температурных аномалий, 
имевших место на протяжении этого месяца. В восточной части США температу-
ра была гораздо ниже обычного, в то время как на западе и Аляске было намного 
теплее. В местах, где температура не регистрировалась, мы видим фон карты.
Визуализация с помощью библиотеки Seaborn
Библиотека Matplotlib зарекомендовала себя как невероятно удобный и популяр -
ный инструмент визуализации, но даже заядлые ее пользователи признают, что она 
зачастую оставляет желать лучшего. Существует несколько часто возникающих 
жалоб на Matplotlib.
 До версии 2.0 параметры по умолчанию библиотеки Matplotlib были далеко 
не идеальными. Они вели свое происхождение от MATLAB-версии примерно 
1999 года, и это часто ощущалось.
 API библиотеки Matplotlib — относительно низкоуровневый. С его помощью 
можно создавать сложные статистические визуализации, но это требует немало 
шаблонного кода.
 Matplotlib была выпущена на десятилетие раньше, чем библиотека Pandas, и по -
тому не ориентирована на работу с объектами DataFrame библиотеки Pandas. 

--- СТРАНИЦА 358 ---

\1ndas приходится 
извлекать все объекты Series и зачастую конкатенировать их в нужный формат. 
Хорошо было бы иметь библиотеку для построения графиков, в которой присут-
ствовали бы возможности по интеллектуальному использованию меток DataFrame 
на графиках.
Библиотека Seaborn — решение этих проблем. Seaborn предоставляет API поверх 
библиотеки Matplotlib, обеспечивающий разумные варианты стилей графиков 
и цветов по умолчанию, определяющий простые высокоу ровневые функции для 
часто встречающихся типов графиков и хорошо интегрирующийся с функциональ-
ностью, предоставляемой объектами DataFrame библиотеки Pandas.
Справедливости ради упомяну, что команда разработчиков библиотеки Matplotlib 
тоже пытается решить эти проблемы: они добавили утилиты plt.style (которые 
мы обсуждали в разделе «Пользовательские настройки Matplotlib: конфигурации 
и таблицы стилей» этой главы) и принимают меры к более органичной обработке 
данных Pandas. Версия 2.0 библиотеки Matplotlib включает новую таблицу стилей 
по умолчанию, исправляющую ситуацию. Но по вышеизложенным причинам би-
блиотека Seaborn остается исключительно удобным дополнением.
Seaborn по сравнению с Matplotlib

\1
In[1]: import matplotlib.pyplot as plt
 plt.style.use('classic')
 %matplotlib inline
 import numpy as np
 import pandas as pd

\1
In[2]: # Создаем данные
 rng = np.random.RandomState(0)
 x = np.linspace(0, 10, 500)
 y = np.cumsum(rng.randn(500, 6), 0)

\1
In[3]: # Рисуем график, используя параметры Matplotlib по умолчанию
 plt.plot(x, y)
 plt.legend('ABCDEF', ncol=2, loc='upper left');
Хотя результат содержит всю информацию, которую нам требуется донести до 
читателя, это происходит не слишком приятным глазу образом, и даже выглядит 
слегка старомодным в свете современных визуализаций данных.

--- СТРАНИЦА 359 ---
Визуализация с помощью библиотеки Seaborn 359
Рис. 4.111. Данные в стиле библиотеки Matplotlib по умолчанию
Теперь посмотрим, как можно сделать это с помощью Seaborn. Помимо множества 
собственных высокоуровневых функций построения графиков библиотеки Seaborn, 
она может также перекрывать параметры по умолчанию библиотеки Matplotlib, 
благодаря чему применение даже более простых сценариев Matplotlib приводит к на -
много лучшему результату. Задать стиль можно с помощью метода set() библиотеки 
Seaborn. По принятым соглашениям Seaborn импортируется под именем sns:
In[4]: import seaborn as sns
 sns.set()

\1
In[5]: # Тот же самый код для построения графика, что и выше!
 plt.plot(x, y)
 plt.legend('ABCDEF', ncol=2, loc='upper left');
Рис. 4.112. Данные в используемом по умолчанию стиле библиотеки Seaborn
О, намного лучше!

--- СТРАНИЦА 360 ---

\1n
Основная идея библиотеки Seaborn — предоставление высокоуровневых команд 
для создания множества различных типов графиков, удобных для исследования 
статистических данных и даже подгонки статистических моделей.
Рассмотрим некоторые из имеющихся в Seaяborn наборов данных и типов гра -
фиков. Обратите внимание, что все изложенное далее можно выполнить и с по -
мощью обычных команд библиотеки Matplotlib, но API Seaborn намного более 
удобен.

\1
In[6]: data = np.random.multivariate_normal([0, 0], [[5, 2], [2, 2]], 
 size=2000)
 data = pd.DataFrame(data, columns=['x', 'y'])
 for col in 'xy':
 plt.hist(data[col], normed=True, alpha=0.5)
 
Рис. 4.113. Гистограммы для визуализации распределений
Вместо гистограммы можно получить гладкую оценку распределения путем ядер-
ной оценки плотности распределения, которую Seaborn выполняет с помощью 
функции sns.kdeplot (рис. 4.114):

--- СТРАНИЦА 361 ---
Визуализация с помощью библиотеки Seaborn 361
In[7]: for col in 'xy':
 sns.kdeplot(data[col], shade=True)

\1
In[8]: sns.distplot(data['x'])
 sns.distplot(data['y']);

\1
In[9]: sns.kdeplot(data);

--- СТРАНИЦА 362 ---

\1ns.jointplot . Для этого графика мы зададим стиль 
с белым фоном (рис. 4.117):
In[10]: with sns.axes_style('white'):
 sns.jointplot("x", "y", data, kind='kde');
Рис. 4.117. График совместного распределения с двумерным графиком 
ядерной оценки плотности

--- СТРАНИЦА 363 ---
Визуализация с помощью библиотеки Seaborn 363
Функции jointplot можно передавать и другие параметры, например можно вос-
пользоваться гистограммой на базе шестиугольников (рис. 4.118):
In[11]: with sns.axes_style('white'):
 sns.jointplot("x", "y", data, kind='hex')

\1
In[12]: iris = sns.load_dataset("iris")
 iris.head()
Out[12]: sepal_length sepal_width petal_length petal_width species
 0 5.1 3.5 1.4 0.2 setosa
 1 4.9 3.0 1.4 0.2 setosa

\1ndows и ваша версия Python — 3.6, при 
выполнении этих команд вы можете столкнуться с известной ошибкой, связанной с из -
менением кодировки имен файлов по умолчанию в Python 3.6. Простейшим решением 
проблемы будет изменение кодировки на использовавшуюся в предыдущих версиях:
 import sys
 sys._enablelegacywindowsfsencoding().

--- СТРАНИЦА 364 ---

\1ns.pairplot (рис. 4.119):
In[13]: sns.pairplot(iris, hue='species', size=2.5);
Рис. 4.119. График пар, демонстрирующий зависимости между четырьмя переменными
Фасетные гистограммы
Иногда оптимальный способ представления данных — гистограммы подмножеств. 
Функция FacetGrid библиотеки Seaborn делает эту задачу элементарной. Рас -
смотрим данные, отображающие суммы, которые персона л ресторана получает 
в качестве чаевых, в зависимости от данных различных индикаторов (рис. 4.120):
In[14]: tips = sns.load_dataset('tips')
 tips.head()
Out[14]: total_bill tip sex smoker day time size
 0 16.99 1.01 Female No Sun Dinner 2

--- СТРАНИЦА 365 ---
Визуализация с помощью библиотеки Seaborn 365
 1 10.34 1.66 Male No Sun Dinner 3
 2 21.01 3.50 Male No Sun Dinner 3
 3 23.68 3.31 Male No Sun Dinner 2
 4 24.59 3.61 Female No Sun Dinner 4
In[15]: tips['tip_pct'] = 100 * tips['tip'] / tips['total_bill']
 grid = sns.FacetGrid(tips, row="sex", col="time", margin_titles=True)
 grid.map(plt.hist, "tip_pct", bins=np.linspace(0, 40, 15));

\1
In[16]: with sns.axes_style(style='ticks'):
 g = sns.factorplot("day", "total_bill", "sex", data=tips,
 kind="box")
 g.set_axis_labels("Day", "Total Bill"); # День; Итого
Совместные распределения
Аналогично графикам пар, которые мы рассматривали ранее, мы можем восполь-
зоваться функцией sns.jointplot для отображения совместного распределения 
между различными наборами данных, а также соответствующих частных распре-
делений (рис. 4.122):

--- СТРАНИЦА 366 ---

\1n[17]: with sns.axes_style('white'):
 sns.jointplot("total_bill", "tip", data=tips, kind='hex')

\1
In[18]: sns.jointplot("total_bill", "tip", data=tips, kind='reg');

--- СТРАНИЦА 367 ---
Визуализация с помощью библиотеки Seaborn 367
Рис. 4.123. График совместного распределения 
с подбором регрессии
Столбчатые диаграммы
Графики временных рядов можно строить с помощью функции sns.factorplot. 
В следующем примере, показанном на рис. 4.124, мы воспользуемся данными из 
набора Planets («Планеты»), которые мы уже видели в разделе «Агрегирование 
и группировка» главы 2:
In[19]: planets = sns.load_dataset('planets')
 planets.head()
Out[19]: method number orbital_period mass distance year
 0 Radial Velocity 1 269.300 7.10 77.40 2006
 1 Radial Velocity 1 874.774 2.21 56.95 2008
 2 Radial Velocity 1 763.000 2.60 19.84 2011
 3 Radial Velocity 1 326.030 19.40 110.62 2007
 4 Radial Velocity 1 516.220 10.50 119.47 2009
In[20]: with sns.axes_style('white'):
 g = sns.factorplot("year", data=planets, aspect=2, # Год
 kind="count", color='steelblue') # Количество
 g.set_xticklabels(step=5)

--- СТРАНИЦА 368 ---

\1n[21]: with sns.axes_style('white'):
 g = sns.factorplot("year", data=planets, aspect=4.0, kind='count',
 hue='method', order=range(2001, 2015))
 g.set_ylabels('Number of Planets Discovered')
 # Количество обнаруженных планет
Дополнительную информацию о построении графиков с помощью библиотеки 
Seaborn можно найти в документации, справочном руководстве и галерее Seaborn.
Пример: время прохождения марафона
В этом разделе мы рассмотрим использование библиотеки Seaborn для визуали-
зации и анализа данных по времени прохождения марафонской дистанции. Эти 
данные я собрал из различных интернет-источников, агрегировал, убрал все иден -
тифицирующие данные и поместил на GitHub, откуда их можно скачать (если вас 
интересует использование языка Python для веб-скрапинга, рекомендую книгу 
Web Scraping with Python 1 ( http://shop.oreilly.com/product/063 69200 34391.do ) Райана 
Митчелла. Начнем со скачивания данных из Интернета и загрузки их в Pandas:
In[22]: # !curl -O https://raw.githubusercontent.com/jakevdp/marathon-data/
# master/marathon-data.csv
In[23]: data = pd.read_csv('marathon-data.csv')
 data.head()
Out[23]: age gender split final
 0 33 M 01:05:38 02:08:51
 1 32 M 01:06:26 02:09:28
 2 31 M 01:06:49 02:10:42
 3 38 M 01:06:16 02:13:45
 4 31 M 01:06:32 02:13:59

\1n. — М.: ДМК-Пресс, 2016.

--- СТРАНИЦА 369 ---
Визуализация с помощью библиотеки Seaborn 369
Рис. 4.125. Количесство открытых планет по типу и году открытия (см. полноцветный график 
в онлайн-режиме (https://github.com/jakevdp/PythonDataScienceHandbook))

--- СТРАНИЦА 370 ---

\1ndas загружает столбцы с временем как строки 
Python (тип object), убедиться в этом можно, посмотрев значение атрибута dtypes 
объекта DataFrame:
In[24]: data.dtypes
Out[24]: age int64
 gender object
 split object
 final object
 dtype: object

\1
In[25]: def convert_time(s):
 h, m, s = map(int, s.split(':'))
 return pd.datetools.timedelta(hours=h, minutes=m, seconds=s)
 data = pd.read_csv('marathon-data.csv',
 converters={'split':convert_time, 
 'final':convert_time})
 data.head()
Out[25]: age gender split final
 0 33 M 01:05:3802:08:51
 1 32 M 01:06:2602:09:28
 2 31 M 01:06:4902:10:42
 3 38 M 01:06:1602:13:45
 4 31 M 01:06:3202:13:59
In[26]: data.dtypes
Out[26]: age int64
 gender object
 split timedelta64[ns]
 final timedelta64[ns]
 dtype: object

\1
In[27]: data['split_sec'] = data['split'].astype(int) / 1E9
 data['final_sec'] = data['final'].astype(int) / 1E9
 data.head()
Out[27]: age gender split final split_sec final_sec
 0 33 M 01:05:3802:08:51 3938.0 7731.0
 1 32 M 01:06:2602:09:28 3986.0 7768.0
 2 31 M 01:06:4902:10:42 4009.0 7842.0
 3 38 M 01:06:1602:13:45 3976.0 8025.0
 4 31 M 01:06:3202:13:59 3992.0 8039.0

\1n в 64-битной операционной системе Windows 
может возникнуть ошибка преобразования типа. Один из путей решения этой проблемы — 
использовать в следующем коде тип np.int64 вместо int.

--- СТРАНИЦА 371 ---
Визуализация с помощью библиотеки Seaborn 371
Чтобы понять, что представляют собой данные, можно нарисовать для них график 
jointplot (рис. 4.126):
In[28]: with sns.axes_style('white'):
 g = sns.jointplot("split_sec", "final_sec", data, kind='hex')
 g.ax_joint.plot(np.linspace(4000, 16000),
 np.linspace(8000, 32000), ':k')

\1
In[29]: data['split_frac'] = 1 - 2 * data['split_sec'] / data['final_sec']
 data.head()
Out[29]: age gender split final split_sec final_sec split_frac
 0 33 M 01:05:3802:08:51 3938.0 7731.0 -0.018756
 1 32 M 01:06:2602:09:28 3986.0 7768.0 -0.026262
 2 31 M 01:06:4902:10:42 4009.0 7842.0 -0.022443
 3 38 M 01:06:1602:13:45 3976.0 8025.0 0.009097
 4 31 M 01:06:3202:13:59 3992.0 8039.0 0.006842

--- СТРАНИЦА 372 ---

\1n[30]: sns.distplot(data['split_frac'], kde=False);
 plt.axvline(0, color="k", linestyle="--");
In[31]: sum(data.split_frac < 0)
Out[31]: 251

\1
In[32]:
g = sns.PairGrid(data, vars=['age', 'split_sec', 'final_sec', 'split_frac'],
 hue='gender', palette='RdBu_r')
g.map(plt.scatter, alpha=0.8)
g.add_legend();
Похоже, что коэффициент распределения сил никак не коррелирует с возрастом, но 
коррелирует с итоговым временем забега: более быстрые бегуны склонны распреде-
лять свои силы поровну. Как мы видим на этом графике, библиотека Seaborn — не 
панацея от «недугов» библиотеки Matplotlib, если речь идет о стилях графиков: 

--- СТРАНИЦА 373 ---
Визуализация с помощью библиотеки Seaborn 373
в частности, метки на оси X перекрываются. Однако, поскольку результат — про-
стой график Matplotlib, можно воспользоваться методами из раздела «Пользо -
вательские настройки делений на осях координат» данной главы для настройки 
подобных вещей.

\1
In[33]: sns.kdeplot(data.split_frac[data.gender=='M'],
 label='men', shade=True)
 sns.kdeplot(data.split_frac[data.gender=='W'],
 label='women', shade=True)
 plt.xlabel('split_frac');

--- СТРАНИЦА 374 ---

\1n[34]:
sns.violinplot("gender", "split_frac", data=data,
 palette=["lightblue", "lightpink"]);

\1
In[35]: data['age_dec'] = data.age.map(lambda age: 10 * (age // 10))
data.head()
Out[35]:
 age gender split final split_sec final_sec split_frac age_dec
0 33 M 01:05:3802:08:51 3938.0 7731.0 -0.018756 30
1 32 M 01:06:2602:09:28 3986.0 7768.0 -0.026262 30
2 31 M 01:06:4902:10:42 4009.0 7842.0 -0.022443 30
3 38 M 01:06:1602:13:45 3976.0 8025.0 0.009097 30
4 31 M 01:06:3202:13:59 3992.0 8039.0 0.006842 30
In[36]:
men = (data.gender == 'M')

--- СТРАНИЦА 375 ---
Визуализация с помощью библиотеки Seaborn 375
women = (data.gender == 'W')
with sns.axes_style(style=None):
 sns.violinplot("age_dec", "split_frac", hue="gender", data=data,
 split=True, inner="quartile",
 palette=["lightblue", "lightpink"]);
Рис. 4.130. «Скрипичная» диаграмма, показывающая зависимость 
коэффициента распределения от пола
Рис. 4.131. «Скрипичная» диаграмма, отображающая зависимость 
коэффициента распределения от пола и возраста

--- СТРАНИЦА 376 ---

\1n[38]: (data.age > 80).sum()
Out[38]: 7

\1
In[37]: g = sns.lmplot('final_sec', 'split_frac', col='gender', data=data,
 markers=".", scatter_kws=dict(color='c'))
 g.map(plt.axhline, y=0.1, color="k", ls=":");
Рис. 4.132. Коэффициенты распределения сил по полу 
в зависимости от времени пробега
Как видим, люди с низким значением коэффициента распределения сил — элит -
ные бегуны, финиширующие в пределах 15 000 секунд (примерно 4 часов). Ве -
роятность подобного распределения сил для более медленных бегунов невелика.

--- СТРАНИЦА 377 ---
Дополнительные источники информации 377
Дополнительные источники информации
Источники информации о библиотеке Matplotlib
Бессмысленно надеяться охватить в одной главе все имеющиеся в Matplotlib воз-
можности и типы графиков. Как и для других рассмотренных нами пакетов, раз-
умное использование TAB-автодополнения и справочных возможностей оболочки 
IPython (см. раздел «Справка и документация в оболочке Python» главы 1) может 
принести немалую пользу при изучении API библиотеки Matplotlib. Кроме того, 
полезным источником информации может стать онлайн-документация Matplotlib 
( http://matplotlib.org/), в частности галерея Matplotlib ( http://matplotlib.org/gallery.html). 
В ней представлены миниатюры сотен различных типов гра фиков, каждая из 
которых представляет собой ссылку на страницу с фраг ментом кода на языке 
Python, используемым для его генерации. Таким образом, вы можете визуально 
изучить широкий диапазон различных стилей построения графиков и методик 
визуализации.
В качестве более обширного обзора библиотеки Matplotlib я бы рекомендовал вам об -
ратить внимание на книгу Interactive Applications Using Matplotlib ( http://bit.ly/2fSqswQ), 
написанную разработчиком ядра Matplotlib Беном Рутом.
Другие графические библиотеки языка Python
Хотя Matplotlib — наиболее значительная из предназначенных для визуализации 
библиотек языка Python, существуют и другие, более современные инструменты, 
заслуживающие пристального внимания. Я перечислю некоторые из них.
 Bokeh ( http://bokeh.pydata.org/ ) — JavaScript-библиотека визуализации с кли -
ентской частью для языка Python, предназначенная дл я создания высокоин -
терактивных визуализаций с возможностью обработки очень больших и/или 
потоковых наборов данных. Клиентская часть Python возвращает структуры 
данных в формате JSON, интерпретируемые затем JavaScript-движком библио-
теки Bokeh.
 Plotly ( http://plot.ly/) — продукт с открытым исходным кодом одноименной ком -
пании, аналогичный по духу библиотеке Bokeh. Поскольку Plotly — основной 
продукт этого стартапа, разработчики прилагают максимум усилий к его раз-
работке. Использовать эту библиотеку можно совершенно бесплатно.
 Vispy ( http://vispy.org/) — активно разрабатываемый программный продукт, ори -
ентированный на динамические визуализации очень больших наборов данных. 
В силу его ориентации на OpenGL и эффективное использование графических 
процессоров он способен формировать очень большие и впечатляющие визуа-
лизации.

--- СТРАНИЦА 378 ---

\1n находится в процессе разработки, в пакете Al tair ( http://altair-viz.github.io/). 
Хотя он еще не вполне готов, меня радует сама возможность, что этот проект 
послужит общей точкой отсчета для визуализаций на языке Python и других 
языках программирования.
Сфера визуализации в Python-сообществе меняется очень динамично, и я уверен, 
что этот список устареет сразу после публикации. Внимательно следите за ново-
стями в данной области!

--- СТРАНИЦА 379 ---

\1nce), пере-
обучение (overfitting) и недообучение (underfitting) и т. д.
В этой главе мы рассмотрим практические аспекты машинного обучения с помо-
щью пакета Scikit-Learn ( http://scikit-learn.org/) языка Python. Здесь не планируется 
всестороннее введение в сферу машинного обучения, поскольку это обширная 
тема, требующая более формализованного подхода. Не планируется и всестороннее 
руководство по пакету Scikit-Learn (такие руководства вы можете найти в разделе 
«Дополнительные источники информации по машинному обучению» этой главы).

\1
 с базовой терминологией и понятиями машинного обучения;
 с API библиотеки Scikit-Learn и некоторыми примерами его использования;
 с подробностями нескольких наиболее важных методов машинного обучения, 
помочь разобраться в том, как они работают, а также где и когда применимы.
Большая часть материала взята из учебных курсов по Scikit-Learn, а также семи-
наров, проводившихся мной на PyCon, SciPy, PyData и других конференциях. 
Многолетние отзывы участников и других докладчиков семинаров позволили 
сделать изложение материала более доходчивым!

--- СТРАНИЦА 380 ---

\1ning) — включает моделирование 
признаков данных и соответствующих данным меток. После выбора модели ее 
можно использовать для присвоения меток новым, неизвестным ранее данным. 
Оно разделяется далее на задачи классификации и задачи регрессии. При класси-
фикации метки представляют собой дискретные категории, а при регрессии они 
являются непрерывными величинами. Мы рассмотрим примеры обоих типов 
машинного обучения с учителем в следующем разделе.
 Машинное обучение без учителя (unsupervised learning) — включает модели -
рование признаков набора данных без каких-либо меток и описывается фра -
зой «Пусть набор данных говорит сам за себя». Эти модели включают такие 
 з адачи, как кластеризация (clustering) и понижение размерности (dimensionality 

--- СТРАНИЦА 381 ---
Что такое машинное обучение 381
reduction). Алгоритмы кластеризации служат для выделения отдельных групп 
данных, в то время как алгоритмы понижения размерности предназначены для 
поиска более сжатых представлений данных. Мы рассмотрим примеры обоих 
типов машинного обучения без учителя в следующем разделе.
Кроме того, существуют так называемые методы частичного обучения (semi-
supervised learning), располагающиеся примерно посередине между машинным 
обучением с учителем и машинным обучением без учителя. Методы частичного 
обучения бывают полезны в случае наличия лишь неполных меток.
Качественные примеры прикладных задач машинного 
обучения
Чтобы конкретизировать вышеописанные понятия, рассмотрим несколько про -
стых примеров задач машинного обучения. Цель этих примеров — дать интуи -
тивно понятный обзор тех разновидностей машинного обучения, с которыми мы 
столкнемся в этой главе. В следующих разделах мы рассмотрим подробнее соот -
ветствующие модели и их использование. Чтобы получить представление о более 
технических аспектах, вы можете заглянуть в онлайн-приложение ( https://github.
com/jakevdp/PythonDataScienceHandbook ), а также в генерирующие соответствующие 
рисунки исходные коды на языке Python.
Классификация: предсказание дискретных меток
Рассмотрим простую задачу классификации: имеется набор точек с метками и тре-
буется классифицировать некоторое количество точек без меток.
Допустим, у нас есть показанные на рис. 5.1 данные (код, использовавшийся для 
генерации этого, как и всех остальных в этом разделе, рисунка, приведен в онлайн-
приложении).
Наши данные двумерны, то есть для каждой точки имеются два признака (feature), 
которым соответствуют координаты (x, y) точки на плоскости. В дополнение каж-
дой точке поставлена в соответствие одна из двух меток класса (class label), пред-
ставленная определенным цветом точки. Нам требуется на основе этих признаков 
и меток создать модель, с помощью которой мы смогли бы определить, должна ли 
новая точка быть «синей» или «красной».
Существует множество возможных моделей для решения подобной задачи класси-
фикации, но мы воспользуемся исключительно простой моделью. Будем исходить 
из допущения, что наши две группы можно разделить прямой линией на плоско -
сти, так что точки с одной стороны прямой будут принадлежать к одной группе. 
Данная модель представляет собой количественное выражение утверждения 
«прямая линия разделяет классы», в то время как параметры модели представ -
ляют собой конкретные числа, описывающие местоположение и направленность 

--- СТРАНИЦА 382 ---

\1ning 
the model). Рисунок 5.2 демонстрирует вид обученной модели для наших данных.
Рис. 5.1. Простой набор данных для классификации
Рис. 5.2. Простая модель классификации

--- СТРАНИЦА 383 ---
Что такое машинное обучение 383
После обучения модели ее можно обобщить на новые, немаркированные данные. 
Другими словами, можно взять другой набор данных, провести прямую модели 
через них, после чего на основе этой прямой присвоить метки новым точкам. Этот 
этап обычно называют предсказанием (prediction) (рис. 5.3).

\1
 признак 1 , признак 2 и т. д. → нормированные количества ключевых слов или 
фраз («Виагра», «Нигерийский принц» и т. д.);
 метка → «спам» или «не спам».
Для обучающей последовательности эти метки определяются путем индивиду -
ального осмотра небольшой репрезентативной выборки сообщений электронной 
почты, для остальных сообщений электронной почты метка будет определяться 
с помощью модели. При обученном соответствующим образом алгоритме клас -
сификации с достаточно хорошо сконструированными признаками (обычно 
тысячи или миллионы слов/фраз) этот тип классификации может оказаться 
весьма эффективным. Мы рассмотрим пример подобной текстовой класси -
фикации в разделе «Заглянем глубже: наивная байесовская классификация» 
данной главы.

--- СТРАНИЦА 384 ---

\1n, http://scikit-learn.org/stable/modules/clustering.html).
Рис. 5.9. Данные, маркированные с помощью модели кластеризации 
методом k-средних
Понижение размерности
Понижение размерности — еще один пример алгоритма обучения без учителя, 
в котором метки или другая информация определяются исходя из структуры 
самого набора данных. Алгоритм понижения размерности несколько труднее для 
понимания, чем рассмотренные нами ранее примеры, но он заключается в попытке 
получения представления низкой размерности, которое бы в какой-то мере со -
храняло существенные качества полного набора данных. Различные алгоритмы 
понижения размерности оценивают существенность этих качеств по-разному, как 
мы увидим далее в разделе «Заглянем глубже: обучение на базе многообразий» 
этой главы.
В качестве примера рассмотрим данные, показанные на рис. 5.10. Зритель -
но очевидно, что у таких данных есть внутренняя структура: они получены из 
одномерной прямой, расположенной в двумерном пространстве в виде спирали. 
В некотором смысле можно сказать, что эти данные по своей внутренней сути 
одномерны, но вложены в пространство большей размерности. Подходящая мо -
дель понижения размерности в таком случае должна учитывать эту нелинейную 

--- СТРАНИЦА 389 ---
Что такое машинное обучение 389
вложенную структуру, чтобы суметь получить из нее представление более низкой 
размерности.
Рис. 5.10. Пример данных для понижения размерности
На рис. 5.11 представлена визуализация результатов работы алгоритма обучения 
на базе многообразий Isomap, который именно это и делает.
Рис. 5.11. Данные с метками, полученными с помощью 
алгоритма понижения размерности

--- СТРАНИЦА 390 ---

\1nDataScienceHandbook).

--- СТРАНИЦА 391 ---
Знакомство с библиотекой Scikit-Learn 391
Знакомство с библиотекой Scikit-Learn
Существует несколько библиотек языка Python с надежными реализациями ши-
рокого диапазона алгоритмов машинного обучения. Одна из самых известных — 
Scikit-Learn, пакет, предоставляющий эффективные версии множества распро -
страненных алгоритмов. Пакет Scikit-Learn отличает аккуратный, единообразный 
и продвинутый API, а также удобная и всеохватывающая онлайн-документация. 
Преимущество этого единообразия в том, что, разобравшись в основах использова-
ния и синтаксисе Scikit-Learn для одного типа моделей, вы сможете легко перейти 
к другой модели или алгоритму.
В этом разделе вы найдете обзор API библиотеки Scikit-Learn. Ясное понимание 
элементов API — основа для более углубленного практического обсуждения алго -
ритмов и методов машинного обучения в следующих разделах.
Начнем с представления данных (data representation) в библиотеке Scikit-Learn, 
затем рассмотрим API Estimator (API статистического оценивания) и, наконец, 
взглянем на интересный пример использования этих инструментов для исследо-
вания набора изображений рукописных цифр.
Представление данных в Scikit-Learn
Машинное обучение связано с созданием моделей на основе данных, поэтому 
начнем с обсуждения понятного компьютеру представле ния данных. Лучше все -
го представлять используемые в библиотеке Scikit-Learn данные в виде таблиц.
Данные как таблица
Простейшая таблица — двумерная сетка данных, в которой строки представляют 
отдельные элементы набора данных, а столбцы — атрибуты, связанные с каждым 
из этих элементов. Например, рассмотрим набор данных Iris ( https://en.wikipedia.
org/wiki/Iris_flower_data_set), проанализированный Рональдом Фишером в 1936 году. 
Скачаем его в виде объекта DataFrame библиотеки Pandas с помощью библиотеки 
Seaborn:
In[1]: import seaborn as sns
 iris = sns.load_dataset('iris')
 iris.head()
Out[1]: sepal_length sepal_width petal_length petal_width species
 0 5.1 3.5 1.4 0.2 setosa
 1 4.9 3.0 1.4 0.2 setosa
 2 4.7 3.2 1.3 0.2 setosa
 3 4.6 3.1 1.5 0.2 setosa
 4 5.0 3.6 1.4 0.2 setosa

--- СТРАНИЦА 392 ---

\1n_samples.
Каждый столбец данных относится к конкретному количественному показателю, 
описывающему данную выборку. Мы будем называть столбцы матрицы призна-
ками (features), а количество столбцов полагать равным n_features.
Матрица признаков
Из устройства таблицы очевидно, что информацию можно рассматривать как 
двумерный числовой массив или матрицу, которую мы будем назыв ать матри-
цей признаков (features matrix). По традиции матрицу признаков часто хранят 
в переменной X. Предполагается, что матрица признаков — двумерная , с формой 
[n_samples, n_features] , и хранят ее чаще всего в массиве NumPy или объекте 
DataFrame библиотеки Pandas, хотя некоторые модели библиотеки Scikit-Learn 
допускают использование также разреженных матриц из библиотеки SciPy.
Выборки (то есть строки) всегда соответствуют отдельн ым объектам, описы -
ваемым набором данных. Например, выборка может быть цветком, человеком, 
документом, изображением, звуковым файлом, видеофайлом, астрономическим 
объектом или чем-то еще, что можно описать с помощью набора количественных 
измерений.
Признаки (то есть столбцы) всегда соответствуют конкретным наблюдениям, 
описывающим каждую из выборок количественным образом. Значения призна -
ков обычно представляют собой вещественные числа, но в некоторых случаях они 
могут быть булевыми или иметь дискретные1 значения.
Целевой массив
Помимо матрицы признаков x, обычно мы имеем дело с целевым массивом (мас -
сивом меток), который принято обозначать y. Целевой массив обычно одномерен, 
длиной n_samples. Его хранят в массиве NumPy или объекте Series библиотеки 
Pandas. Значения целевого массива могут быть непрерывными числовыми или 
дискретными классами/метками. Хотя некоторые оцениватели библиотеки Scikit-
Learn умеют работать с несколькими целевыми величинами в виде двумерного 
целевого массива [n_samples, n_targets], мы в основном будем работать с более 
простым случаем одномерного целевого массива.
Отличительная черта целевого массива от остальных столбцов признаков в том, 
что он представляет собой величину, значения которой мы хотим предсказать на 

\1n 393
основе имеющихся данных. Говоря статистическим языком, это зависимая перемен-
ная (dependent variable). Например, для предыдущих данных это могла оказаться 
модель для предсказания вида цветка на основе остальных измерений. В таком 
случае столбец species рассматривался бы как целевой массив.
С учетом вышесказанного можно воспользоваться библиотекой Seaborn (которую 
мы рассматривали в разделе «Визуализация с помощью библиотеки Seaborn» гла -
вы 4), чтобы без труда визуализировать данные (рис. 5.12):
In[2]: %matplotlib inline
 import seaborn as sns; sns.set()
 sns.pairplot(iris, hue='species', size=1.5);
Рис. 5.12. Визуализация набора данных Iris
Для использования набора данных Iris в Scikit-Learn мы извлечем матрицу при-
знаков и целевой массив из объекта DataFrame. Сделать это можно с помощью 
обсуждавшихся в главе 3 операций объекта DataFrame библиотеки Pandas:

--- СТРАНИЦА 394 ---

\1n[3]: X_iris = iris.drop('species', axis=1)
 X_iris.shape
Out[3]: (150, 4)
In[4]: y_iris = iris['species']
 y_iris.shape
Out[4]: (150,)

\1
Рис. 5.13. Структура данных Scikit-Learn
Теперь, отформатировав наши данные нужным образом, мы можем перейти к рас -
смотрению API статистических оценок библиотеки Scikit-Learn.
API статистического оценивания библиотеки Scikit-Learn
В документации по API Scikit-Learn говорится, что он основывается на следующих 
принципах:
 единообразие — интерфейс всех объектов идентичен и основан на ограниченном 
наборе методов, причем документация тоже единообразна;
 контроль — видимость всех задаваемых значений параметров как открытых 
атрибутов;
 ограниченная иерархия объектов — классы языка Python используются только 
для алгоритмов; наборы данных представлены в стандартных форматах (массивы 
NumPy, объекты DataFrame библиотеки Pandas, разреженные матрицы библиотеки 
SciPy), а для имен параметров используются стандартные строки языка Python;
 объединение — многие из задач машинного обучения можно выразить в виде по-
следовательностей алгоритмов более низкого уровня, и библиотека Scikit-Learn 
пользуется этим фактом при любой возможности;

--- СТРАНИЦА 395 ---
Знакомство с библиотекой Scikit-Learn 395
 разумные значения по умолчанию — библиотека задает для необходимых 
моделей пользовательских параметров соответствующие значения по умол -
чанию.
На практике эти принципы очень облегчают изучение библиотеки Scikit-Learn. 
Все алгоритмы машинного обучения в библиотеке Scikit-Learn реализуются через 
API статистического оценивания, предоставляющий единообразный интерфейс 
для широкого диапазона прикладных задач машинного обучения.
Основы API статистического оценивания
Чаще всего использование API статистического оценивания библиотеки Scikit-
Learn включает следующие шаги (далее мы рассмотрим несколько подробных 
примеров).

\1n.

\1nsform() 
или predict().

\1
In[5]: import matplotlib.pyplot as plt
 import numpy as np
 rng = np.random.RandomState(42)
 x = 10 * rng.rand(50)
 y = 2 * x - 1 + rng.randn(50)
 plt.scatter(x, y);

--- СТРАНИЦА 396 ---

\1n представлен соответствующим 
классом языка Python. Так, например, для расчета модели простой линейной 
регрессии можно импортировать класс линейной регрессии:
In[6]: from sklearn.linear_model import LinearRegression
Обратите внимание, что существуют и другие, более общие модели линейной 
регрессии, прочитать о них подробнее вы можете в документации модуля skle-
arn.li near_model ( http://scikit-learn.org/stable/modules/linear_model.html).

\1n 397
Это примеры тех важных решений, которые нам придется принять после выбора 
класса модели . Результаты этих решений часто называют гиперпараметрами , 
то есть параметрами, задаваемыми до обучения модели на данных. Выбор 
гиперпараметров в библиотеке Scikit-Learn осуществляется путем передачи 
значений при создании экземпляра модели. Мы рассмотрим количественные 
обоснования выбора гиперпараметров в разделе «Гиперпараметры и проверка 
модели» данной главы.
Создадим экземпляр класса LinearRegression и укажем с помощью гиперпара-
метра fit_intercept, что нам бы хотелось выполнить подбор точки пересечения 
с осью координат:
In[7]: model = LinearRegression(fit_intercept=True)
 model
Out[7]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1,
 normalize=False)
Помните, что при создании экземпляра модели выполняется только сохранение 
значений этих гиперпараметров. В частности, мы все еще не применили модель 
ни к каким данным: API библиотеки Scikit-Learn очень четко разделяет выбор 
модели и применение модели к данным.

\1n, для которого необходимы двумерная матрица признаков и одномерный 
целевой вектор. Наша целевая переменная y уже имеет нужный вид (массив 
длиной n_samples), но нам придется проделать небольшие манипуляции с дан -
ными x, чтобы сделать из них матрицу размера [n_samples, n_features]. В дан-
ном случае манипуляции сводятся просто к изменению фор мы одномерного 
массива:
In[8]: X = x[:, np.newaxis]
 X.shape
Out[8]: (50, 1)

\1n[9]: model.fit(X, y)
Out[9]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1,
 normalize=False)
Команда fit() вызывает выполнение «под капотом» множества вычислений, 
в зависимости от модели, и сохранение результатов этих вычислений в атрибутах 
модели, доступных для просмотра пользователем. В библиотеке Scikit-Learn по 

--- СТРАНИЦА 398 ---

\1n[10]: model.coef_
Out[10]: array([ 1.97 76566])
In[11]: model.intercept_
Out[11]: -0.90 33107 25531 11635
Эти два параметра представляют собой угловой коэффициент и точку пересе-
чения с осью координат для простой линейной аппроксимации наших данных. 
Сравнивая с описанием данных, видим, что они очень близки к исходному 
угловому коэффициенту, равному 2, и точке пересечения, равной –1.
Часто возникает вопрос относительно погрешностей в подобных внутрен -
них параметрах модели. В целом библиотека Scikit-Learn не предоставляет 
инструментов, позволяющих делать выводы непосредственно из внутренних 
параметров модели: интерпретация параметров скорее вопрос статистического 
моделирования, а не машинного обучения. Машинное обучение концентрируется 
в основном на том, что предсказывает модель. Для тех, кто хочет узнать больше 
о смысле подбираемых параметров модели, существуют другие инструменты, 
включая пакет StatsModels языка Python ( http://statsmodels.sourceforge.net/).

\1n 
можно посредством метода predict(). В этом примере наши новые данные будут 
сеткой x -значений и нас будет интересовать, какие y-значения предсказывает 
модель:
In[12]: xfit = np.linspace(-1, 11)
Как и ранее, эти x -значения требуется преобразовать в матрицу признаков 
[n_samples, n_features], после чего можно подать их на вход модели:
In[13]: Xfit = xfit[:, np.newaxis]
 yfit = model.predict(Xfit)

\1
In[14]: plt.scatter(x, y)
 plt.plot(xfit, yfit);
Обычно эффективность модели оценивают, сравнивая ее результаты с эталоном, 
как мы увидим в следующем примере.

--- СТРАНИЦА 399 ---
Знакомство с библиотекой Scikit-Learn 399
Рис. 5.15. Простая линейная регрессионная аппроксимация наших данных
Пример обучения с учителем: классификация набора данных Iris
Рассмотрим другой пример того же процесса, воспользовавшись обсуждавшимся 
ранее набором данных Iris. Зададимся вопросом: насколько хорошо мы сможем 
предсказать метки остальных данных с помощью модели, обученной на некоторой 
части данных набора Iris?
Для этой задачи мы воспользуемся чрезвычайно простой обобщенной моделью, из-
вестной под названием «Гауссов наивный байесовский классификатор», исходящей 
из допущения, что все классы взяты из выровненного по осям координат Гауссова 
распределения (см. раздел «Заглянем глубже: наивная байесовская классификация» 
данной главы). Гауссов наивный байесовский классификатор в силу отсутствия гипер-
параметров и высокой производительности — хороший кандидат на роль эталонной 
классификации. Имеет смысл поэкспериментировать с ним, прежде чем выяснять, 
можно ли получить лучшие результаты с помощью более сложных моделей.
Мы собираемся проверить работу модели на неизвестных ей данных, так что 
необходимо разделить данные на обучающую последовательность (training set) 
и контрольную последовательность (testing set). Это можно сделать вручную, но 
удобнее воспользоваться вспомогательной функцией train_test_split:
In[15]: from sklearn.cross_validation import train_test_split
 Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris,
 random_state=1)

\1
In[16]: from sklearn.naive_bayes import GaussianNB # 1. Выбираем класс модели
 model = GaussianNB() # 2. Создаем экземпляр модели

--- СТРАНИЦА 400 ---

\1n, ytrain) # 3. Обучаем модель на данных
 y_model = model.predict(Xtest) # 4. Предсказываем значения
 # для новых данных

\1
In[17]: from sklearn.metrics import accuracy_score
 accuracy_score(ytest, y_model)
Out[17]: 0.97368421052631582

\1
In[18]:
from sklearn.decomposition import PCA # 1. Выбираем класс модели
model = PCA(n_components=2) # 2. Создаем экземпляр модели 
 # с гиперпараметрами
model.fit(X_iris) # 3. Обучаем модель на данных. Обратите 
 # внимание, что y мы не указываем!
X_2D = model.transform(X_iris) # 4. Преобразуем данные в двумерные

\1
In[19]: iris['PCA1'] = X_2D[:, 0]
 iris['PCA2'] = X_2D[:, 1]
 sns.lmplot("PCA1", "PCA2", hue='species', data=iris, fit_reg=False);

--- СТРАНИЦА 401 ---
Знакомство с библиотекой Scikit-Learn 401
Мы видим, что в двумерном представлении виды цветов четко разделены, хотя 
алгоритм PCA ничего не знает о метках видов цветов! Это показывает, что, как мы 
и видели ранее, даже относительно простая классификация на этом наборе данных, 
вероятно, будет работать достаточно хорошо (рис. 5.16).
Рис. 5.16. Проекция данных набора Iris на двумерное пространство
Обучение без учителя: кластеризация набора данных Iris
Теперь рассмотрим кластеризацию набора данных Iris. Алгоритм кластеризации 
пытается выделить группы данных безотносительно к каким-либо меткам. Здесь 
мы собираемся использовать мощный алгоритм кластеризации под названием 
смесь Гауссовых распределений (Gaussian mixture model, GMM), которую из -
учим подробнее в разделе «Заглянем глубже: смеси Гауссовых распределений» 
этой главы. Метод GMM состоит в попытке моделирования данных в виде набора 

\1
In[20]:
from sklearn.mixture import GMM # 1. Выбираем класс модели
model = GMM(n_components=3, covariance_type='full') # 2. Создаем экземпляр 
 # модели 
с гиперпараметрами 
model.fit(X_iris) # 3. Обучаем модель на данных. Обратите 
 # внимание, что y мы не указываем! 
y_gmm = model.predict(X_iris) # 4. Определяем метки кластеров
Как и ранее, добавим столбец cluster в DataFrame Iris и воспользуемся библиотекой 
Seaborn для построения графика результатов (рис. 5.17):

--- СТРАНИЦА 402 ---

\1n 403
In[21]:
iris['cluster'] = y_gmm
sns.lmplot("PCA1", "PCA2", data=iris, hue='species',
 col='cluster', fit_reg=False);
Разбив данные в соответствии с номерами кластеров, мы видим, насколько хоро-
шо алгоритм GMM восстановил требуемые метки: вид setosa идеально выделен 
в кластер 0, правда, небольшое количество экземпляров видов versicolor и virginica 
смешались между собой. Следовательно, даже если у нас нет эксперта, который мог 
бы сообщить нам, к каким видам относятся отдельные цветки, одних измерений 
вполне достаточно для автоматического распознания этих различных разновид-
ностей цветков с помощью простого алгоритма кластеризации! Подобный алгоритм 
может в дальнейшем помочь специалистам по предметной области выяснить связи 
между исследуемыми образцами.
Прикладная задача: анализ рукописных цифр
Продемонстрируем эти принципы на более интересной задаче, рассмотрев один из 
аспектов задачи оптического распознавания символов — распознавание рукописных 
цифр. Традиционно эта задача включает как определение местоположения на рисун-
ке, так и распознание символов. Мы пойдем самым коротким путем и воспользуемся 
встроенным в библиотеку Scikit-Learn набором преформатированных цифр.
Загрузка и визуализация цифр
Воспользуемся интерфейсом доступа к данным библиотеки Scikit-Learn и посмо-
трим на эти данные:
In[22]: from sklearn.datasets import load_digits
 digits = load_digits()
 digits.images.shape
Out[22]: (1797, 8, 8)

\1
In[23]: import matplotlib.pyplot as plt
 fig, axes = plt.subplots(10, 10, figsize=(8, 8),
 subplot_kw={'xticks':[], 'yticks':[]},
 gridspec_kw=dict(hspace=0.1, wspace=0.1))
 for i, ax in enumerate(axes.flat):
 ax.imshow(digits.images[i], cmap='binary',
 interpolation='nearest')

--- СТРАНИЦА 404 ---

\1nsform=ax.transAxes, color='green')
Рис. 5.18. Данные рукописных цифр; каждая выборка представлена 
сеткой пикселов размером 8 × 8
Для работы с этими данными в библиотеке Scikit-Learn нам нужно получить их 
двумерное [n_samples, n_features] представление. Для этого мы будем тракто -
вать каждый пиксел в изображении как признак, то есть «расплющим» массивы 
пикселов так, чтобы каждую цифру представлял массив пикселов длиной 64 эле-
мента. Кроме этого, нам понадобится целевой массив, задающий для каждой ци-
фры предопределенную метку. Эти два параметра встроены в набор данных цифр 
в виде атрибутов data и target, соответственно:
In[24]: X = digits.data
 X.shape
Out[24]: (1797, 64)
In[25]: y = digits.target
 y.shape
Out[25]: (1797,)
Итого получаем 1797 выборок и 64 признака.

--- СТРАНИЦА 405 ---
Знакомство с библиотекой Scikit-Learn 405

\1
In[26]: from sklearn.manifold import Isomap
 iso = Isomap(n_components=2)
 iso.fit(digits.data)
 data_projected = iso.transform(digits.data)
 data_projected.shape
Out[26]: (1797, 2)

\1
In[27]: plt.scatter(data_projected[:, 0], data_projected[:, 1], 
 c=digits.target, edgecolor='none', alpha=0.5,
 cmap=plt.cm.get_cmap('spectral', 10))
 plt.colorbar(label='digit label', ticks=range(10))
 plt.clim(-0.5, 9.5);
Рис. 5.19. Isomap-вложение набора данных цифр
Этот график дает нам представление о разделении различных цифр в 64-мер -
ном пространстве. Например, нули (отображаемые черным цветом) и единицы 

--- СТРАНИЦА 406 ---

\1n[28]: Xtrain, Xtest, ytrain, ytest = 
 train_test_split(X, y, random_state=0)
In[29]: from sklearn.naive_bayes import GaussianNB
 model = GaussianNB()
 model.fit(Xtrain, ytrain)
 y_model = model.predict(Xtest)

\1
In[30]: from sklearn.metrics import accuracy_score
 accuracy_score(ytest, y_model)
Out[30]: 0.83333333333333337
Даже при такой исключительно простой модели мы получили более чем 80%-ную 
точность классификации цифр! Однако из одного числа сложно понять, где наша 
модель ошиблась. Для этой цели удобна так называемая матрица различий (confusion 
matrix), вычислить которую можно с помощью библиотеки Scikit-Learn, а нарисовать 
посредством Seaborn (рис. 5.20):
In[31]: from sklearn.metrics import confusion_matrix
 mat = confusion_matrix(ytest, y_model)
 sns.heatmap(mat, square=True, annot=True, cbar=False)
 plt.xlabel('predicted value') # Прогнозируемое значение
 plt.ylabel('true value'); # Настоящее значение

--- СТРАНИЦА 407 ---
Знакомство с библиотекой Scikit-Learn 407

\1
In[32]: fig, axes = plt.subplots(10, 10, figsize=(8, 8),
 subplot_kw={'xticks':[], 'yticks':[]},
 gridspec_kw=dict(hspace=0.1, wspace=0.1))
 for i, ax in enumerate(axes.flat):
 ax.imshow(digits.images[i], cmap='binary', 
 interpolation='nearest')
 ax.text(0.05, 0.05, str(y_model[i]),
 transform=ax.transAxes,
 color='green' if (ytest[i] == y_model[i]) else 'red')
Из этого подмножества данных можно почерпнуть полезную информацию от -
носительно мест, в которых алгоритм работает неопти мально. Чтобы поднять 
нашу точность выше 80 %, можно воспользоваться более сложным алгоритмом, 
таким как метод опорных векторов (см. раздел «Загля нем глубже: метод опорных 
векторов» этой главы), случайные леса (см. раздел «Заглянем глубже: деревья 
принятия решений и случайные леса» данной главы) или другим методом клас -
сификации.

--- СТРАНИЦА 408 ---

\1nDataScienceHandbook)
Резюме
В этом разделе мы рассмотрели основные возможности представления данных 
библиотеки Scikit-Learn, а также API статистического оценивания. Независимо от 
типа оценивателя применяется одна и та же схема: импорт/создание экземпляра/
обучение/предсказание. Вооружившись этой информацией по API статистического 
оценивания, вы можете, изучив документацию библиотеки Scikit-Learn, начать 
экспериментировать, используя различные модели для своих данных.
В следующем разделе мы рассмотрим, вероятно, самый важный вопрос машинного 
обучения: выбор и проверку модели.
Гиперпараметры и проверка модели
В предыдущем разделе описана основная схема использования моделей машинного 
обучения с учителем.

\1n[1]: from sklearn.datasets import load_iris
 iris = load_iris()
 X = iris.data
 y = iris.target
Выберем теперь модель и гиперпараметры. Мы будем использовать в этом при -
мере классификатор на основе метода k -средних с n_neighbors=1 . Это очень 
простая и интуитивно понятная модель, которую можно описать фразой «Мет -
ка для неизвестной точки такая же, как и метка ближайшей к ней обучающей 
точки»:
In[2]: from sklearn.neighbors import KNeighborsClassifier
 model = KNeighborsClassifier(n_neighbors=1)

\1
In[3]: model.fit(X, y)
 y_model = model.predict(X)

\1

--- СТРАНИЦА 410 ---

\1n[4]: from sklearn.metrics import accuracy_score
 accuracy_score(y, y_model)
Out[4]: 1.0
Как видим, показатель точности равен 1.0, то есть наша модель правильно поме -
тила 100 % точек! Но действительно ли это правильная оценка ожидаемой точно -
сти? Действительно ли нам попалась модель, которая будет работать правильно 
в 100 % случаев?
Как вы могли догадаться, ответ на этот вопрос — нет. На самом деле этот подход 
имеет фундаментальный изъян: обучение и оценка модели выполняются на одних 
и тех же данных ! Более того, модель ближайшего соседа — оцениватель, рабо -
тающий путем обучения на примерах (instance-based estimator), попросту сохра -
няющий обучающие данные и предсказывающий метки путем сравнения новых 
данных с этими сохраненными точками. За исключением некоторых специально 
сконструированных случаев его точность будет всегда равна 100 %!
Хороший способ проверки модели: отложенные данные
Для более точного выяснения рабочих характеристик модели воспользуемся так 
называемыми отложенными наборами данных (holdout sets), то есть отложим не-
которое подмножество данных из обучающей последовательности модели, после 
чего используем его для проверки качества работы модели. Это разделение в Scikit-
Learn можно произвести с помощью утилиты train_test_split:
In[5]: from sklearn.cross_validation import train_test_split
 # Разделяем данные: по 50% в каждом из наборов
 X1, X2, y1, y2 = train_test_split(X, y, random_state=0,
 train_size=0.5)
 # Обучаем модель на одном из наборов данных
 model.fit(X1, y1)
 # Оцениваем работу модели на другом наборе
 y2_model = model.predict(X2)
 accuracy_score(y2, y2_model)
Out[5]: 0.90666666666666662
Мы получили более логичный результат: классификатор на основе метода бли -
жайшего соседа демонстрирует точность около 90 % на этом отложенном наборе 
данных. Отложенный набор данных схож с неизвестными данными, поскольку 
модель «не видела» их ранее.

--- СТРАНИЦА 411 ---
Гиперпараметры и проверка модели 411
Перекрестная проверка модели
Потеря части наших данных для обучения модели — один из недостатков ис -
пользования отложенного набора данных для проверки модели. В предыдущем 
случае половина набора данных не участвует в обучении модели! Это неопти -
мально и может стать причиной проблем, особенно если исходный набор данных 
невелик.
Один из способов решения этой проблемы — перекрестная проверка (cross-
validation), то есть выполнение последовательности аппроксимаций, в которых 
каждое подмножество данных используется как в качестве обучающей последо -
вательности, так и проверочного набора. Наглядно его можно представить в виде 
рис. 5.22.

\1
In[6]: y2_model = model.fit(X1, y1).predict(X2)
 y1_model = model.fit(X2, y2).predict(X1)
 accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)
Out[6]: (0.95999999999999996, 0.90666666666666662)
Полученные числа — две оценки точности, которые можно обобщить (допу -
стим, взяв от них среднее значение) для получения лучшей меры общей работы 
модели. Этот конкретный вид перекрестной проверки, в которой мы разбили 
данные на два набора и по очереди использовали каждый из них в качестве про -
верочного набора, называется двухблочной перекрестной проверкой (two-fold 
cross-validation).
Можно распространить эту идею на случай большего числа попыток и большего 
количества блоков данных, например, на рис. 5.23 представлена пятиблочная пере -
крестная проверка.

--- СТРАНИЦА 412 ---

\1n для большей краткости синтаксиса:
In[7]: from sklearn.cross_validation import cross_val_score
 cross_val_score(model, X, y, cv=5)
Out[7]: array([ 0.96666667, 0.96666667, 0.93333333, 0.93333333, 1. ])
Повторение проверки по различным подмножествам данных дает нам лучшее пред-
ставление о качестве работы алгоритма.
Библиотека Scikit-Learn реализует множество схем перекрестной проверки, удоб -
ных в определенных конкретных ситуациях. Они реализованы с помощью итера-
торов в модуле cross_validation. Например, мы взяли предельный случай, при 
котором количество блоков равно количеству точек данных, и обучаем модель 
в каждой попытке на всех точках, кроме одной. Такой тип перекрестной проверки 
известен под названием перекрестной проверки по отдельным объектам (leave-
one-out cross-validation — дословно «перекрестная проверка по всем без одного»), 
ее можно использовать следующим образом:

--- СТРАНИЦА 413 ---
Гиперпараметры и проверка модели 413
In[8]: from sklearn.cross_validation import LeaveOneOut
 scores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))
 scores
Out[8]: array([ 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
 1., 1., 1., 1., 1., 1., 1.])

\1
In[9]: scores.mean()
Out[9]: 0.95999999999999996
Аналогичным образом можно выполнять и другие схемы перекрестной проверки. 
Чтобы узнать, какие еще их варианты есть в библиотеке Scikit-Learn, воспользуйтесь 
оболочкой IPython для просмотра подмодуля sklearn.cross_validation или обра-
титесь к онлайн-документации по перекрестной проверке библиотеки Scikit-Learn.

\1
 использовать более сложную/гибкую модель;
 применять менее сложную/гибкую модель;

--- СТРАНИЦА 414 ---

\1nce). Рассмотрим 
рис. 5.24, на котором представлены два случая регрессионной аппроксимации 
одного набора данных.
Рис. 5.24. Модели регрессии со значительной 
систематической ошибкой и высокой дисперсией
Очевидно, что обе модели не слишком хорошо аппроксимируют наши данные, но 
проблемы с ними различны.
Приведенная слева модель пытается найти прямолинейное приближение к данным. 
Но в силу того, что внутренняя структура данных сложнее прямой линии, с помо -
щью прямолинейной модели невозможно описать этот набор данных достаточно 
хорошо. О подобной модели говорят, что она недообучена (underfit), то есть гибкость 
модели недостаточна для удовлетворительного учета всех признаков в данных. 
Другими словами, у этой модели имеется значительная систематическая ошибка.

--- СТРАНИЦА 415 ---
Гиперпараметры и проверка модели 415
Приведенная справа модель пытается подобрать для наших данных многочлен 
высокой степени. В этом случае модель достаточно гибка, чтобы практически 
идеально соответствовать всем нюансам данных, но хотя она и описывает очень 
точно обучающую последовательность, конкретная ее форма отражает скорее 
характеристики шума в данных, чем внутренние свойства процесса, сгенериро -
вавшего данные. О подобной модели говорят, что она переобучена (overfit), то есть 
гибкость модели такова, что в конце концов модель учитывает не только исходное 
распределение данных, но и случайные ошибки в них. Другими словами, у этой 
модели имеется высокая дисперсия.
Чтобы взглянуть с другой стороны, посмотрим, что получится, если восполь -
зоваться этими двумя моделями для предсказания y-значения для каких-либо 
новых данных. В диаграммах на рис. 5.25 красные (более светлые в печатном 
варианте книги) точки обозначают исключенные из обучающей последователь -
ности данные.
Рис. 5.25. Оценки эффективности модели для обучения и проверки для моделей 
со значительной систематической ошибкой и высокой дисперсией
В качестве оценки эффективности здесь используется R 2 — коэффициент де -
терминации или коэффициент смешанной корреляции (подробнее о нем можно 
прочитать по адресу https://ru.wikipedia.org/wiki/Коэффициент_детерминации). Он пред -
ставляет собой меру того, насколько хорошо модель работает по сравнению с про-
стым средним значением целевых величин. R 2 = 1 означает идеальное совпадение 
предсказаний, а R 2 = 0 показывает, что модель оказалась ничем не лучше простого 
среднего значения данных, а отрицательные значения указывают на модели, ко -
торые работают еще хуже. 
На основе оценок эффективности вышеприведенных двух моделей мы можем 
сделать следующее обобщенное наблюдение.

--- СТРАНИЦА 416 ---

\1n curve). 
На нем можно наблюдать следующие важные особенности.
 Оценка эффективности для обучения всегда превышает оценку эффективно -
сти для проверки. Это логично: модель лучше подходит для уже виденных ею 
данных, чем для тех, которые она еще не видела.
 Модели с очень низкой сложностью (со значительной систематической ошиб-
кой) являются недообученными, то есть эти модели будут плохо предсказывать 
как данные обучающей последовательности, так и любые ранее не виденные 
ими данные.
 Модели с очень высокой сложностью (с высокой дисперсией) являются переобу-
ченными, то есть будут очень хорошо предсказывать данные обучающей после-
довательности, но на любых ранее не виденных данных работать очень плохо.

--- СТРАНИЦА 417 ---
Гиперпараметры и проверка модели 417
 Кривая проверки достигает максимума в какой-то промежуточной точке. Этот 
уровень сложности означает приемлемый компромисс между систематической 
ошибкой и дисперсией.
Средства регулирования сложности модели различаются в зависимости от 
модели. Как осуществлять подобную регулировку для каждой из моделей, мы 
увидим в следующих разделах, когда будем обсуждать конкретные модели под -
робнее.
Кривые проверки в библиотеке Scikit-Learn
Рассмотрим пример перекрестной проверки для расчета кривой проверки для класса 
моделей. Мы будем использовать модель полиномиальной регрессии (polynomial 
regression model): это обобщенная линейная модель с параметризованной степе -
нью многочлена. Например, многочлен 1-й степени аппроксимирует наши данные 
прямой линией; при параметрах модели a и b :
y = ax + b.

\1
y = ax 3 + bx 2 + cx + d.
Это можно обобщить на любое количество полиномиальных признаков. В библио-
теке Scikit-Learn реализовать это можно с помощью простой линейной регрессии 
в сочетании с полиномиальным препроцессором. Мы воспользуемся конвейером 
(pipeline) для соединения этих операций в единую цепочку (мы обсудим поли -
номиальные признаки и конвейеры подробнее в разделе «Проектирование при -
знаков» данной главы):
In[10]: from sklearn.preprocessing import PolynomialFeatures
 from sklearn.linear_model import LinearRegression
 from sklearn.pipeline import make_pipeline
 def PolynomialRegression(degree=2, **kwargs):
 return make_pipeline(PolynomialFeatures(degree),
 LinearRegression(**kwargs))

\1
In[11]: import numpy as np
 def make_data(N, err=1.0, rseed=1):
 # Создаем случайные выборки данных
 rng = np.random.RandomState(rseed)
 X = rng.rand(N, 1) ** 2
 y = 10 - 1. / (X.ravel() + 0.1)
 if err > 0:

--- СТРАНИЦА 418 ---

\1ng.randn(N)
 return X, y 
 X, y = make_data(40)

\1
In[12]: %matplotlib inline
 import matplotlib.pyplot as plt
 import seaborn; seaborn.set() # plot formatting
 X_test = np.linspace(-0.1, 1.1, 500)[:, None]
 plt.scatter(X.ravel(), y, color='black')
 axis = plt.axis()
 for degree in [1, 3, 5]:
 y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)
 plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))
 plt.xlim(-0.1, 1.0)
 plt.ylim(-2, 12)
 plt.legend(loc='best');
Рис. 5.27. Аппроксимации набора данных тремя различными 
полиномиальными моделями
Параметром, служащим для управления сложностью модели, в данном случае яв -
ляется степень многочлена, которая может быть любым неотрицательным числом. 
Не помешает задать себе вопрос: какая степень многочлена обеспечивает подходящий 
компромисс между систематической ошибкой (недообучение) и дисперсией (пере -
обучение)?

--- СТРАНИЦА 419 ---
Гиперпараметры и проверка модели 419
Чтобы решить этот вопрос, визуализируем кривую проверки для этих конкретных 
данных и моделей. Проще всего сделать это с помощью предоставляемой библио-
текой Scikit-Learn удобной утилиты validation_curve. Эта функция, получив на 
входе модель, данные, название параметра и диапазон для анализа, автоматически 
вычисляет в этом диапазоне значение как оценки эффективности для обучения, 
так и оценки эффективности для проверки (рис. 5.28):
In[13]:
from sklearn.learning_curve import validation_curve
degree = np.arange(0, 21)
train_score, val_score = validation_curve(PolynomialRegression(), X, y,
 'polynomialfeatures__degree',
 degree, cv=7)
plt.plot(degree, np.median(train_score, 1), color='blue', 
 label='training score') # Оценка обучения
plt.plot(degree, np.median(val_score, 1), color='red', 
 label='validation score') # Оценка проверки
plt.legend(loc='best')
plt.ylim(0, 1)
plt.xlabel('degree') # Степень
plt.ylabel('score'); # Оценка
Рис. 5.28. Кривая проверки для приведенных на рис. 5.27 данных (ср.: рис. 5.26)
Этот график в точности демонстрирует ожидаемое нами качественное поведение: 
оценка эффективности для обучения на всем диапазоне превышает оценку 
эффективности для проверки; оценка эффективности для обучения монотон -
но растет с ростом сложности модели, а оценка эффективности для проверки 

--- СТРАНИЦА 420 ---

\1n[14]: plt.scatter(X.ravel(), y)
 lim = plt.axis()
 y_test = PolynomialRegression(3).fit(X, y).predict(X_test)
 plt.plot(X_test.ravel(), y_test);
 plt.axis(lim);

\1
In[15]: X2, y2 = make_data(200)
 plt.scatter(X2.ravel(), y2);

--- СТРАНИЦА 421 ---

\1
In[16]:
degree = np.arange(21)
train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2,
 'polynomialfeatures__degree',
 degree, cv=7)
plt.plot(degree, np.median(train_score2, 1), color='blue',
 label='training score')
plt.plot(degree, np.median(val_score2, 1), color='red', 
 label='validation score')
plt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3,
 linestyle='dashed')
plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3,
 linestyle='dashed')
plt.legend(loc='lower center')
plt.ylim(0, 1)
plt.xlabel('degree')
plt.ylabel('score');
Сплошные линии показывают новые результаты, а более бледные штриховые 
линии — результаты предыдущего меньшего набора данных. Из кривой проверки 
ясно, что этот больший набор данных позволяет использовать намного более слож-
ную модель: максимум, вероятно, возле степени 6, но даже модель со степенью 20 
не выглядит сильно переобученной — оценки эффективности для проверки и обу-
чения остаются очень близки друг к другу.

--- СТРАНИЦА 422 ---

\1ning curve).
Поведение кривой обучения должно быть следующим.
 Модель заданной сложности окажется переобученной на слишком маленьком 
наборе данных. Это значит, что оценка эффективности для обучения будет от-
носительно высокой, а оценка эффективности для проверки — относительно 
низкой.
 Модель заданной сложности окажется недообученной на слишком большом 
наборе данных. Это значит, что оценка эффективности для обучения будет 
снижаться, а оценка эффективности для проверки — повышаться по мере роста 
размера набора данных.
 Модель никогда, разве что случайно, не покажет на проверочном наборе лучший 
результат, чем на обучающей последовательности. Это значит, что кривые будут 
сближаться, но никогда не пересекутся.
Учитывая эти особенности, можно ожидать, что кривая обучения будет выглядеть 
качественно схожей с изображенной на рис. 5.32.

--- СТРАНИЦА 423 ---
Гиперпараметры и проверка модели 423
Заметная особенность кривой обучения — сходимость к конкретном у значению 
оценки при росте числа обучающих выборок. В частности, если количество точек 
достигло значения, при котором данная конкретная модель сошлась, то добавление 
новых обучающих данных не поможет ! Единственным способом улучшить каче -
ство модели в этом случае будет использование другой (зачастую более сложной) 
модели.
 Рис. 5.32. Схематическое изображение типичной кривой обучения
Кривые обучения в библиотеке Scikit-Learn. Библиотека Scikit-Learn предо -
ставляет удобные утилиты для вычисления кривых обучения для моделей. В этом 
разделе мы вычислим кривую обучения для нашего исходного набора данных с по-
линомиальными моделями второй и девятой степени (рис. 5.33):
In[17]:
from sklearn.learning_curve import learning_curve
fig, ax = plt.subplots(1, 2, figsize=(16, 6))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)
for i, degree in enumerate([2, 9]):
 N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),
 X, y, cv=7,
 train_sizes=np.linspace(0.3, 1, 25))
 ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')
 ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')
 ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1], 

--- СТРАНИЦА 424 ---

\1nestyle='dashed')
 ax[i].set_ylim(0, 1)
 ax[i].set_xlim(N[0], N[-1])
 ax[i].set_xlabel('training size') # Размерность обучения
 ax[i].set_ylabel('score')
 ax[i].set_title('degree = {0}'.format(degree), size=14)
 ax[i].legend(loc='best')
Рис. 5.33. Кривые обучения для модели низкой (слева) 
и высокой сложности (справа)
Это ценный показатель, поскольку он наглядно демонстрирует нам реакцию нашей 
модели на увеличение объема обучающих данных. В частности, после того момента, 
когда кривая обучения уже сошлась к какому-то значению (то есть когда кривые 
обучения и проверки уже близки друг к другу), добавление дополнительных обу-
чающих данных не улучшит аппроксимацию существенно! Эта ситуация отражена 
на левом рисунке с кривой обучения для модели второй степени.
Единственный способ улучшения оценки уже сошедшейся кривой — использовать 
другую (обычно более сложную) модель. Это видно на пр авом рисунке: перейдя 
к более сложной модели, мы улучшаем оценку для точки сходимости (отмеченную 
штриховой линией) за счет более высокой дисперсии модели (соответствующей 
расстоянию между оценками эффективности для обучения и проверки). Если бы 
нам пришлось добавить еще больше точек, кривая обучения для более сложной из 
этих моделей все равно в итоге бы сошлась.
Построение графика кривой обучения для конкретных модели и набора данных 
облегчает принятие решения о том, как продвинуться еще дальше на пути улуч -
шения анализа данных.

--- СТРАНИЦА 425 ---
Гиперпараметры и проверка модели 425
Проверка на практике: поиск по сетке
Из предшествующего обсуждения вы должны были понять смысл компромисса 
между систематической ошибкой и дисперсией и его зависимость от сложности 
модели и размера обучающей последовательности. На практике у моделей обычно 
больше одного параметра, поэтому графики кривых проверки и обучения превра-
щаются из двумерных линий в многомерные поверхности. Выполнение подобных 
визуализаций в таких случаях представляет собой непростую задачу, поэтому 
лучше отыскать конкретную модель, при которой оценка эффективности для про-
верки достигает максимума.
Библиотека Scikit-Learn предоставляет для этой цели специальные автоматиче -
ские инструменты, содержащиеся в модуле grid_search. Рассмотрим трехмерную 
сетку признаков модели — степени многочлена, флага, указывающего, нужно ли 
подбирать точку пересечения с осью координат, и флага, указывающего, следует 
ли выполнять нормализацию. Выполнить эти настройки можно с помощью мета-
оценивателя GridSearchCV библиотеки Scikit-Learn:
In[18]: from sklearn.grid_search import GridSearchCV
 param_grid = {'polynomialfeatures__degree': np.arange(21),
 'linearregression__fit_intercept': [True, False],
 'linearregression__normalize': [True, False]}
 grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)

\1
In[19]: grid.fit(X, y);

\1
In[20]: grid.best_params_
Out[20]: {'linearregression__fit_intercept': False,
 'linearregression__normalize': True,
 'polynomialfeatures__degree': 4}

\1
In[21]: model = grid.best_estimator_
 plt.scatter(X.ravel(), y)
 lim = plt.axis()

--- СТРАНИЦА 426 ---

\1n, посвященной поиску по сетке 
( http://scikit-learn.org/stable/modules/grid_search.html).
Резюме
В этом разделе мы приступили к изучению понятий проверки модели и оптимиза-
ции гиперпараметров, фокусируя внимание на наглядных аспектах компромисса 
между систематической ошибкой и дисперсией и его работы при подгонке моделей 
к данным. В частности, мы обнаружили, что крайне важно использовать провероч-
ный набор или перекрестную проверку при настройке параметров, чтобы избежать 
переобучения более сложных/гибких моделей.
В следующих разделах мы детально рассмотрим некоторые особенно удобные 
модели, попутно обсуждая имеющиеся возможности их настройки и влияние 
этих свободных параметров на сложность модели. Не забывайте уроки этого раз -
дела при дальнейшем чтении книги и изучении упомянутых методов машинного 
обучения!

--- СТРАНИЦА 427 ---
Проектирование признаков 427
Проектирование признаков
В предыдущих разделах мы обрисовали базовые понятия машинного обучения, 
но во всех предполагалось, что наши данные находятся в аккуратном формате 
[n_samples, n_features] . На практике же данные редко поступают к нам в по -
добном виде. Поэтому одним из важнейших этапов использования машинного 
обучения на практике становится проектирование признаков (feature engineering), 
то есть преобразование всей касающейся задачи информации в числа, пригодные 
для построения матрицы признаков.
В этом разделе мы рассмотрим несколько часто встречающихся примеров задач 
проектирования признаков: признаки для представления категориальных данных 
(categorical data), признаки для представления текста и признаки для представле-
ния изображений. Кроме того, мы обсудим использование производных признаков 
(derived features) для повышения сложности модели и заполнение отсутствующих 
данных. Этот процесс часто называют векторизацией, так как он включает преоб-
разование данных в произвольной форме в аккуратные векторы.
Категориальные признаки
Категориальные данные — один из распространенных типов нечисловых данных. 
Например, допустим, что мы анализируем какие-то данные по ценам на жилье 
и, помимо числовых признаков, таких как «цена» и «количество комнат», в них 
имеется информация о микрорайоне (neighborhood). Например, пусть наши дан -
ные выглядят следующим образом:
In[1]: data = [
 {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},
 {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},
 {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},
 {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}
 ]

\1
In[2]: {'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3};
Но оказывается, что в библиотеке Scikit-Learn такой подход не очень удобен: мо-
дели данного пакета исходят из базового допущения о том, что числовые признаки 
отражают алгебраические величины. Следовательно, подобное отображение будет 
подразумевать, например, что Queen Anne < Fremont < Wallingford или даже что 
Wallingford – Queen Anne = Fremont , что, не считая сомнительных демографических 
шуток, не имеет никакого смысла.

--- СТРАНИЦА 428 ---

\1ne-hot 
encoding), означающее создание дополнительных столбцов-индикаторов наличия/
отсутствия категории с помощью значений 1 или 0 соответственно. При наличии 
данных в виде списка словарей для этой цели можно воспользоваться утилитой 
DictVectorizer библиотеки Scikit-Learn:
In[3]: from sklearn.feature_extraction import DictVectorizer
 vec = DictVectorizer(sparse=False, dtype=int)
 vec.fit_transform(data)
Out[3]: array([[ 0, 1, 0, 850000, 4],
 [ 1, 0, 0, 700000, 3],
 [ 0, 0, 1, 650000, 3],
 [ 1, 0, 0, 600000, 2]], dtype=int64)
Обратите внимание, что столбец neighborhood превратился в три отдельных 
столбца, отражающих три метки микрорайонов, и что в каждой строке стоит 1 
в соответствующем ее микрорайону столбце. После под обного кодирования 
категориальных признаков можно продолжить обучение модели Scikit-Learn 
обычным образом.

\1
In[4]: vec.get_feature_names()
Out[4]: ['neighborhood=Fremont',
 'neighborhood=Queen Anne',
 'neighborhood=Wallingford',
 'price',
 'rooms']

\1
In[5]: vec = DictVectorizer(sparse=True, dtype=int)
 vec.fit_transform(data)
Out[5]: <4x5 sparse matrix of type '<class 'numpy.int64'>'
 with 12 stored elements in Compressed Sparse Row format>
Многие (хотя пока не все) оцениватели библиотеки Scikit-Learn допускают пере-
дачу им подобных разреженных входных данных при обучении и оценке моделей. 
Для поддержки подобного кодирования библиотека Scik it-Learn включает две 
дополнительные утилиты: sklearn.preprocessing.OneHotEncoder и skle arn.fea-
ture_extraction.FeatureHasher.

--- СТРАНИЦА 429 ---

\1
In[6]: sample = ['problem of evil',
 'evil queen',
 'horizon problem']
Для векторизации этих данных на основе числа слов можно создать столбцы, соот-
ветствующие словам problem, evil, horizon и т. д. Хотя это можно сделать вручную, 
мы избежим нудной работы, воспользовавшись утилитой CountVectorizer библио-
теки Scikit-Learn:
In[7]: from sklearn.feature_extraction.text import CountVectorizer
 vec = CountVectorizer()
 X = vec.fit_transform(sample)
 X 
Out[7]: <3x5 sparse matrix of type '<class 'numpy.int64'>'
 with 7 stored elements in Compressed Sparse Row format>

\1
In[8]: import pandas as pd
 pd.DataFrame(X.toarray(), columns=vec.get_feature_names())
Out[8]: evil horizon of problem queen
 0 1 0 1 1 0
 1 1 0 0 0 1
 2 0 1 0 1 0
У этого подхода существуют проблемы: использование непосредственно количеств 
слов ведет к признакам, с которыми встречающимся очень часто словам придается 
слишком большое значение, а это в некоторых алгоритмах классификации может 
оказаться субоптимальным. Один из подходов к решению этой проблемы известен 
под названием «терма - обратная частотность документа» (term frequency-
inverse document frequency) или TF-IDF. При нем слова получают вес с учетом 

--- СТРАНИЦА 430 ---

\1n[9]: from sklearn.feature_extraction.text import TfidfVectorizer
 vec = TfidfVectorizer()
 X = vec.fit_transform(sample)
 pd.DataFrame(X.toarray(), columns=vec.get_feature_names())
Out[9]: evil horizon of problem queen
 0 0.517856 0.000000 0.680919 0.517856 0.000000
 1 0.605349 0.000000 0.000000 0.000000 0.795961
 2 0.000000 0.795961 0.000000 0.605349 0.000000
Чтобы увидеть пример использования TF-IDF в задачах классификации, см. раз -
дел «Заглянем глубже: наивная байесовская классификация» данной главы.
Признаки для изображений
Достаточно часто для задач машинного обучения требуется соответствующим образом 
кодировать изображения. Простейший подход — тот, который мы использовали для 
набора данных рукописных цифр в разделе «Знакомство с библиотекой Scikit-Learn» 
этой главы, — использовать значения интенсивности самих пикселов. Но подобные 
подходы могут, в зависимости от прикладной задачи, оказаться неоптимальными.
Всесторонний обзор методов выделения признаков для изображений выходит дале-
ко за рамки данного раздела, но вы можете найти отличные реализации множества 
стандартных подходов в проекте Scikit-Image ( http://scikit-image.org/). Впрочем, один 
пример совместного использования библиотеки Scikit-Learn и пакета Scikit-Image 
вы можете найти в разделе «Прикладная задача: конвейер распознавания лиц» 
данной главы.
Производные признаки
Еще один удобный тип признаков — выведенные математически из каких-либо 
входных признаков. Мы уже встречались с ними в разделе «Гиперпараметры и про-
верка модели» этой главы, когда создавали полиномиальные признаки из входных 
данных. Мы видели, что можно преобразовать линейную регрессию в полиноми-
альную регрессию не путем изменения модели, а преобразования входных данных! 
Этот метод, известный под названием регрессии по комбинации базисных функций 
(basis function regression), рассматривается подробнее в разделе «Заглянем глубже: 
линейная регрессия» текущей главы.

\1

--- СТРАНИЦА 431 ---
Проектирование признаков 431
In[10]: %matplotlib inline
 import numpy as np
 import matplotlib.pyplot as plt
 x = np.array([1, 2, 3, 4, 5])
 y = np.array([4, 2, 1, 3, 7])
 plt.scatter(x, y);
Рис. 5.35. Данные, которые нельзя хорошо 
описать с помощью прямой линии
Тем не менее мы можем подобрать разделяющую прямую для этих данных с помо-
щью функции LinearRegression и получить оптимальный результат (рис. 5.36):
In[11]: from sklearn.linear_model import LinearRegression
 X = x[:, np.newaxis]
 model = LinearRegression().fit(X, y)
 yfit = model.predict(X)
 plt.scatter(x, y)
 plt.plot(x, yfit);
Рис. 5.36. Неудачная прямолинейная аппроксимация

--- СТРАНИЦА 432 ---

\1n[12]: from sklearn.preprocessing import PolynomialFeatures
 poly = PolynomialFeatures(degree=3, include_bias=False)
 X2 = poly.fit_transform(X)
 print(X2)
[[ 1. 1. 1.]
 [ 2. 4. 8.]
 [ 3. 9. 27.]
 [ 4. 16. 64.]
 [ 5. 25. 125.]]

\1
In[13]: model = LinearRegression().fit(X2, y)
 yfit = model.predict(X2)
 plt.scatter(x, y)
 plt.plot(x, yfit);
Рис. 5.37. Линейная аппроксимация по производным 
полиномиальным признакам
Идея улучшения модели путем не изменения самой модели, а преобразования 
входных данных является базовой для многих более продвинутых методов ма -
шинного обучения. Мы обсудим эту идею подробнее в разделе «Заглянем глуб -
же: линейная регрессия» данной главы в контексте регрессии по комбинации 
базисных функций. В общем случае это путь к набору обладающих огромными 
возможностями методик, известных под названием «ядерные методы» (kernel 

--- СТРАНИЦА 433 ---

\1
In[14]: from numpy import nan
 X = np.array([[ nan, 0, 3 ],
 [ 3, 7, 9 ],
 [ 3, 5, 2 ],
 [ 4, nan, 6 ],
 [ 8, 8, 1 ]])
 y = np.array([14, 16, -1, 8, -5])
При использовании для подобных данных типичной модели машинного обучения 
необходимо сначала заменить отсутствующие данные каким-либо подходящим 
значением. Это действие называется заполнением (imputation) пропущенных зна-
чений, и методики его выполнения варьируются от простых (например, замены 
пропущенных значений средним значением по столбцу) до сложных (например, 
с использованием восстановления матриц (matrix completion) или ошибкоустой-
чивого алгоритма для обработки подобных данных).
Сложные подходы, как правило, очень сильно зависят от конкретной прикладной 
задачи, и мы не станем углубляться в них. Для реализации стандартного подхода 
к заполнению пропущенных значений (с использованием среднего значения, медиа-
ны или часто встречающегося значения) библиотека Scikit-Learn предоставляет 
класс Imputer:
In[15]: from sklearn.preprocessing import Imputer
 imp = Imputer(strategy='mean')
 X2 = imp.fit_transform(X)
 X2
Out[15]: array([[ 4.5, 0. , 3. ],
 [ 3. , 7. , 9. ],
 [ 3. , 5. , 2. ],
 [ 4. , 5. , 6. ],
 [ 8. , 8. , 1. ]])
В результате мы получили данные, в которых два пропущенные значения заменены 
на среднее значение остальных элементов соответствующего столбца. Эти данные 
можно передать, например, непосредственно оценивателю LinearRegression:

--- СТРАНИЦА 434 ---

\1n[16]: model = LinearRegression().fit(X2, y)
 model.predict(X2)
Out[16]:
array([ 13.14869292, 14.3784627 , -1.15539732, 10.96606197, -5.33782027])
Конвейеры признаков
Во всех предыдущих примерах может быстро надоесть выполнять преобразования 
вручную, особенно если нужно связать цепочкой несколько шагов. Например, нам 
может понадобиться следующий конвейер обработки.

\1n предоставляет объект конвейера, который можно использовать следующим 
образом:
In[17]: from sklearn.pipeline import make_pipeline
 model = make_pipeline(Imputer(strategy='mean'),
 PolynomialFeatures(degree=2),
 LinearRegression())
Этот конвейер выглядит и функционирует аналогично обычному объекту 
библиотеки Scikit-Learn, и выполняет все заданные шаги для любых входных 
данных.
In[18]: model.fit(X, y) # Вышеприведенный массив X с пропущенными значениями
 print(y)
 print(model.predict(X))
[1416 -1 8 -5]
[ 14. 16. -1. 8. -5.]
Все шаги этой модели выполняются автоматически. Обратите внимание, что для 
простоты демонстрации мы применили модель к тем данным, на которых она была 
обучена, именно поэтому она сумела столь хорошо предсказать результат (более 
подробное обсуждение этого вопроса можно найти в ра зделе «Гиперпараметры 
и проверка модели» данной главы).
Некоторые примеры работы конвейеров библиотеки Scikit-Learn вы увидите 
в следующем разделе, посвященном наивной байесовской классификации, а также 
в разделах «Заглянем глубже: линейная регрессия» и «Заглянем глубже: метод 
опорных векторов» этой главы.

--- СТРАНИЦА 435 ---

\1
Все, что нам теперь нужно, — модель, с помощью которой можно было бы вы -
числить P(признаков | L i ) для каждой из меток. Подобная модель называется 
порождающей моделью (generative model), поскольку определяет гипотетический 
случайный процесс генерации данных. Задание порождающей модели для каждой 
из меток/категорий — основа обучения подобного байесовского классиф икатора. 
Обобщенная версия подобного шага обучения — непростая задача, но мы упро -
стим ее, приняв некоторые упрощающие допущения о виде модели.

--- СТРАНИЦА 436 ---

\1n[1]: %matplotlib inline
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns; sns.set()

\1
In[2]: from sklearn.datasets import make_blobs
 X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)
 plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');
Рис. 5.38. Данные для наивной байесовской классификации
Один из самых быстрых способов создания простой модели — допущение о том, 
что данные подчиняются нормальному распределению без ковариации между 
измерениями. Для обучения этой модели достаточно найти среднее значение 

--- СТРАНИЦА 437 ---
Заглянем глубже: наивная байесовская классификация 437
и стандартное отклонение точек внутри каждой из категорий — это все, что 
требуется для описания подобного распределения. Результат этого наивного 
Гауссова допущения показан на рис. 5.39.
Эллипсы на этом рисунке представляют Гауссову порождающую модель для 
каждой из меток с ростом вероятности по мере приближении к центру эллипса. 
С помощью этой порождающей модели для каждого класса мы можем легко 
вычислить вероятность P(признаков | L i ) для каждой точки данных, а следо -
вательно, быстро рассчитать соотношение для апостериорной вероятности 
и определить, какая из меток с большей вероятностью соответствует конкрет -
ной точке.
Эта процедура реализована в оценивателе sklearn.naive_bayes.GaussianNB:
In[3]: from sklearn.naive_bayes import GaussianNB
 model = GaussianNB()
 model.fit(X, y);

\1
In[4]: rng = np.random.RandomState(0)
 Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)
 ynew = model.predict(Xnew)
Теперь у нас есть возможность построить график этих новых данных и понять, где 
пролегает граница принятия решений (decision boundary) (рис. 5.40):
In[5]: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')
 lim = plt.axis()
 plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu',
 alpha=0.1)
 plt.axis(lim);

--- СТРАНИЦА 438 ---

\1n[6]: yprob = model.predict_proba(Xnew)
 yprob[-8:].round(2)
Out[6]: array([[ 0.89, 0.11],
 [ 1. , 0. ],
 [ 1. , 0. ],
 [ 1. , 0. ],
 [ 1. , 0. ],
 [ 1. , 0. ],
 [ 0. , 1. ],
 [ 0.15, 0.85]])

\1
In[7]: from sklearn.datasets import fetch_20newsgroups
 data = fetch_20newsgroups()
 data.target_names
Out[7]: ['alt.atheism',
 'comp.graphics',
 'comp.os.ms-windows.misc',
 'comp.sys.ibm.pc.hardware',
 'comp.sys.mac.hardware',
 'comp.windows.x',
 'misc.forsale',
 'rec.autos',
 'rec.motorcycles',
 'rec.sport.baseball',
 'rec.sport.hockey',
 'sci.crypt',
 'sci.electronics',
 'sci.med',
 'sci.space',
 'soc.religion.christian',
 'talk.politics.guns',
 'talk.politics.mideast',
 'talk.politics.misc',
 'talk.religion.misc']

--- СТРАНИЦА 440 ---

\1n[8]:
categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space',
 'comp.graphics']
train = fetch_20newsgroups(subset='train', categories=categories)
test = fetch_20newsgroups(subset='test', categories=categories)

\1
In[9]: print(train.data[5])
From: dmcgee@uluhe.soest.hawaii.edu (Don McGee)
Subject: Federal Hearing
Originator: dmcgee@uluhe
Organization: School of Ocean and Earth Science and Technology
Distribution: usa
Lines: 10
Fact or rumor....? Madalyn Murray O'Hare an atheist who eliminated the
use of the bible reading and prayer in public schools 15 years ago is now
going to appear before the FCC with a petition to stop the reading of the
Gospel on the airways of America. And she is also campaigning to remove
Christmas programs, songs, etc from the public schools. If it is true
then mail to Federal Communications Commission 1919 H Street Washington DC
20054 expressing your opposition to her request. Reference Petition number

\1n[10]: from sklearn.feature_extraction.text import TfidfVectorizer
 from sklearn.naive_bayes import MultinomialNB
 from sklearn.pipeline import make_pipeline
 model = make_pipeline(TfidfVectorizer(), MultinomialNB())

\1
In[11]: model.fit(train.data, train.target)
 labels = model.predict(test.data)

\1

--- СТРАНИЦА 441 ---
Заглянем глубже: наивная байесовская классификация 441
In[12]:
from sklearn.metrics import confusion_matrix
mat = confusion_matrix(test.target, labels)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,
 xticklabels=train.target_names, yticklabels=train.target_names)
plt.xlabel('true label')
plt.ylabel('predicted label');

\1
In[13]: def predict_category(s, train=train, model=model):
 pred = model.predict([s])
 return train.target_names[pred[0]]

--- СТРАНИЦА 442 ---

\1n[14]: predict_category('sending a payload to the ISS')
Out[14]: 'sci.space'
In[15]: predict_category('discussing islam vs atheism')
Out[15]: 'soc.religion.christian'
In[16]: predict_category('determining the screen resolution')
Out[16]: 'comp.graphics'

\1
In[1]: %matplotlib inline
 import matplotlib.pyplot as plt
 import seaborn as sns; sns.set()
 import numpy as np

\1

--- СТРАНИЦА 444 ---

\1n[2]: rng = np.random.RandomState(1)
 x = 10 * rng.rand(50)
 y = 2 * x - 5 + rng.randn(50)
 plt.scatter(x, y);
Воспользуемся оценивателем LinearRegression из библиотеки Scikit-Learn для 
обучения на этих данных и поиска оптимальной прямой (рис. 5.43):
In[3]: from sklearn.linear_model import LinearRegression
 model = LinearRegression(fit_intercept=True)
 model.fit(x[:, np.newaxis], y)
 xfit = np.linspace(0, 10, 1000)
 yfit = model.predict(xfit[:, np.newaxis])
 plt.scatter(x, y)
 plt.plot(xfit, yfit);
Подбираемые параметры модели (в библиотеке Scikit-Learn всегда содержат 
в конце знак подчеркивания) включают угловой коэффициент и точку пересе -
чения с осью координат. В данном случае соответствующие параметры — coef_ 
и intercept_:
In[4]: print("Model slope: ", model.coef_[0])
 print("Model intercept:", model.intercept_)
Model slope: 2.02720881036
Model intercept: -4.99857708555

--- СТРАНИЦА 445 ---
Заглянем глубже: линейная регрессия 445
Рис. 5.43. Линейная регрессионная модель
Видим, что результаты очень близки к входным данным, как мы и надеялись.
Однако возможности оценивателя LinearRegression намного шире этого: помимо 
аппроксимации прямыми линиями, он может также работать с многомерными 
линейными моделями следующего вида:
y = a 0 + a 1 x 1 +a 2x 2+…
с несколькими величинами x . Геометрически это подобно подбору плоскости 
для точек в трех измерениях или гиперплоскости для точек в пространстве с еще 
большим числом измерений.

\1
In[5]: rng = np.random.RandomState(1)
 X = 10 * rng.rand(100, 3)
 y = 0.5 + np.dot(X, [1.5, -2., 1.])
 model.fit(X, y)
 print(model.intercept_)
 print(model.coef_)
0.5
[ 1.5 -2. 1. ]
Здесь данные величины y сформированы из трех случайных значений величи ны x , 
а линейная регрессия восстанавливает использовавшие ся для их формирования 
коэффициенты.

--- СТРАНИЦА 446 ---

\1nearRegression для 
аппроксимации наших данных прямыми, плоскостями и гиперплоскостями. По-
прежнему складывается впечатление, что этот подход ограничивается лишь строго 
линейными отношениями между переменными, но оказывается, что ослабление 
этого требования также возможно.
Регрессия по комбинации базисных функций
Один из трюков, позволяющих приспособить линейную регрессию к нелинейным 
отношениям между переменными, — преобразование данных в соответствии с но-
выми базисными функциями. Один из вариантов этого трюка мы уже встречали 
в конвейере PolynomialRegression, который использовался в разделах «Гиперпа-
раметры и проверка модели» и «Проектирование признаков» данной главы. Идея 
состоит в том, чтобы взять многомерную линейную модель:
y = a 0 + a 1 x 1 + a 2 x 2 + a 3 x 3 +…
и построить x 1 , x 2 , x 3 и т. д. на основе имеющегося одномерного входного значения x . 
То есть у нас x n = f n( x ), где f n( x ) — некая функция, выполняющая преобразование 
данных.
Например, если f n( x ) = x n, наша модель превращается в полиномиальную регрес -
сию:
y = a 0 + a 1 x + a 2 x 2 + a 3 x 3 +…
Обратите внимание, что модель по-прежнему остается линейной — линейность 
относится к тому, что коэффициенты a n никогда не умножаются и не делятся друг 
на друга. Фактически мы взяли наши одномерные значения x и выполнили про -
екцию их на более многомерное пространство, так что с помощью линейной ап -
проксимации мы можем теперь отражать более сложные зависимости между x и y.
Полиномиальные базисные функции
Данная полиномиальная проекция настолько удобна, что была встроена в библио-
теку Scikit-Learn в виде преобразователя PolynomialFeatures:
In[6]: from sklearn.preprocessing import PolynomialFeatures
 x = np.array([2, 3, 4])
 poly = PolynomialFeatures(3, include_bias=False)
 poly.fit_transform(x[:, None])
Out[6]: array([[ 2., 4., 8.],
 [ 3., 9., 27.],
 [ 4., 16., 64.]])

--- СТРАНИЦА 447 ---

\1
In[7]: from sklearn.pipeline import make_pipeline
 poly_model = make_pipeline(PolynomialFeatures(7), LinearRegression())

\1
In[8]: rng = np.random.RandomState(1)
 x = 10 * rng.rand(50)
 y = np.sin(x) + 0.1 * rng.randn(50)
 poly_model.fit(x[:, np.newaxis], y)
 yfit = poly_model.predict(xfit[:, np.newaxis])
 plt.scatter(x, y)
 plt.plot(xfit, yfit);
Рис. 5.44. Полиномиальная аппроксимация нелинейной 
обучающей последовательности
С нашей линейной моделью, используя полиномиальные базисные функции 
седьмого порядка, мы получили великолепную аппроксимацию этих нелиней -
ных данных!

--- СТРАНИЦА 448 ---

\1n, но мы можем написать для их 
создания пользовательский преобразователь, как показано ниже и проиллюстри -
ровано на рис. 5.46 (преобразователи библиотеки Scikit-Learn реализованы как 
классы языка Python; чтение исходного кода библиотеки Scikit-Learn — отличный 
способ разобраться с их созданием):
In[9]:
from sklearn.base import BaseEstimator, TransformerMixin
class GaussianFeatures(BaseEstimator, TransformerMixin):
 """Равномерно распределенные Гауссовы признаки 
 для одномерных входных данных"""
 def __init__(self, N, width_factor=2.0):
 self.N = N
 self.width_factor = width_factor
 @staticmethod
 def _gauss_basis(x, y, width, axis=None):

--- СТРАНИЦА 449 ---
Заглянем глубже: линейная регрессия 449
 arg = (x - y) / width
 return np.exp(-0.5 * np.sum(arg ** 2, axis))
 def fit(self, X, y=None):
 # Создаем N центров, распределенных по всему диапазону данных
 self.centers_ = np.linspace(X.min(), X.max(), self.N)
 self.width_ = self.width_factor * 
 (self.centers_[1] - self.centers_[0])
 return self
 def transform(self, X):
 return self._gauss_basis(X[:, :, np.newaxis], self.centers_,
 self.width_, axis=1)
gauss_model = make_pipeline(GaussianFeatures(20),
 LinearRegression())
gauss_model.fit(x[:, np.newaxis], y)
yfit = gauss_model.predict(xfit[:, np.newaxis])
plt.scatter(x, y)
plt.plot(xfit, yfit)
plt.xlim(0, 10);
Мы привели этот пример лишь для того, чтобы подчеркнуть, что в полиноми -
альных базисных функциях нет никакого колдовства. Если у вас есть какие-то 
дополнительные сведения о процессе генерации ваших данных, исходя из кото -
рых у вас есть основания полагать, что наиболее под ходящим будет тот или иной 
базис, — тоже можете его использовать.
Рис. 5.46. Аппроксимация Гауссовыми базисными функциями, вычисленными с помощью 
пользовательского преобразователя

--- СТРАНИЦА 450 ---

\1n[10]: model = make_pipeline(GaussianFeatures(30),
 LinearRegression())
 model.fit(x[:, np.newaxis], y)
 plt.scatter(x, y)
 plt.plot(xfit, model.predict(xfit[:, np.newaxis]))
 plt.xlim(0, 10)
 plt.ylim(-1.5, 1.5);

\1
In[11]: def basis_plot(model, title=None):
 fig, ax = plt.subplots(2, sharex=True)
 model.fit(x[:, np.newaxis], y)
 ax[0].scatter(x, y)
 ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))
 ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))
 if title:
 ax[0].set_title(title)

--- СТРАНИЦА 451 ---
Заглянем глубже: линейная регрессия 451
 ax[1].plot(model.steps[0][1].centers_,
 model.steps[1][1].coef_)
 ax[1].set(xlabel='basis location', # Базовое местоположение
 ylabel='coefficient', # Коэффициент
 xlim=(0, 10))
 model = make_pipeline(GaussianFeatures(30), LinearRegression())
 basis_plot(model)
Рис. 5.48. Коэффициенты при Гауссовых базисных функциях 
в чрезмерно сложной модели
Нижняя часть рис. 5.48 демонстрирует амплитуду базисной функции в каждой 
из точек. Это типичное поведение для переобучения с перекрытием областей 
определения базисных функций: коэффициенты соседних базисных функций 
усиливают и подавляют друг друга. Мы знаем, что подобное поведение при -
водит к проблемам и было бы неплохо ограничивать по добные пики в модели 
явным образом, «накладывая штраф» на большие значения параметров модели. 
Подобное «штрафование» известно под названием регуляризации и существует 
в нескольких вариантах.
Гребневая регрессия (L2-регуляризация)
Вероятно, самый часто встречающийся вид регуляризации — гребневая регрессия 
(ridge regression), или L 2 -регуляризация (L 2 -regularization), также иногда называ -
емая регуляризацией Тихонова (Tikhonov regularization). Она заключается в нало-
жении штрафа на сумму квадратов (евклидовой нормы) коэффициентов модели. 

\1

--- СТРАНИЦА 452 ---

\1n в виде оценивателя Ridge 
(рис. 5.49):
In[12]: from sklearn.linear_model import Ridge
 model = make_pipeline(GaussianFeatures(30), Ridge(alpha=0.1))
 basis_plot(model, title='Ridge Regression') # Гребневая регрессия

\1
In[13]: from sklearn.linear_model import Lasso
 model = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.001))
 basis_plot(model, title='Lasso Regression') # Лассо-регуляризация
Рис. 5.50. Применение лассо-регуляризации к слишком сложной модели (ср. с рис. 5.48)
При использовании штрафа лассо-регрессии большинство коэффициентов в точ -
ности равны нулю, а функциональное поведение моделируется небольшим под -
множеством из имеющихся базисных функций. Как и в случае гребневой регуля -
ризации, параметр □ управляет уровнем штрафа и его следует определять путем 
перекрестной проверки (см. раздел «Гиперпараметры и проверка модели» данной 
главы).
Пример: предсказание велосипедного трафика
В качестве примера посмотрим, сможем ли мы предсказать количество пересека-
ющих Фримонтский мост в Сиэтле велосипедов, основываясь на данных о погоде, 
времени года и других факторах. Мы уже работали с этими данными в разделе 
«Работа с временными рядами» главы 3.

--- СТРАНИЦА 454 ---

\1ndas дает 
нам возможность с легкостью соединить эти два источника данных. Мы установим 
отношение между погодой и другой информацией с количество м велосипедов , 
выполнив простую линейную регрессию, чтобы оценить, как изменения этих па -
раметров повлияют на число велосипедистов в заданный день.
В частности, это пример использования подобных библиотеке Scikit-Learn инстру -
ментов в фреймворке статистического моделирования, где предполагается осмыс-
ленность параметров модели. Это отнюдь не стандартный подход для машинного 
обучения, но для некоторых моделей подобная трактовка возможна.

\1
In[14]:
import pandas as pd
counts = pd.read_csv('fremont_hourly.csv', index_col='Date', parse_dates=True)
weather = pd.read_csv('599021.csv', index_col='DATE', parse_dates=True)

\1
In[15]: daily = counts.resample('d', how='sum')
 daily['Total'] = daily.sum(axis=1)
 daily = daily[['Total']] # удаляем остальные столбцы

\1
In[16]: days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
 for i in range(7):
 daily[days[i]] = (daily.index.dayofweek == i).astype(float)

\1
In[17]: from pandas.tseries.holiday import USFederalHolidayCalendar
 cal = USFederalHolidayCalendar()
 holidays = cal.holidays('2012', '2016')
 daily = daily.join(pd.Series(1, index=holidays, name='holiday'))
 daily['holiday'].fillna(0, inplace=True)

\1

--- СТРАНИЦА 455 ---
Заглянем глубже: линейная регрессия 455
In[18]: 
 def hours_of_daylight(date, axis=23.44, latitude=47.61):
 """Рассчитываем длительность светового дня для заданной даты"""
 days = (date - pd.datetime(2000, 12, 21)).days
 m = (1. - np.tan(np.radians(latitude))
 * np.tan(np.radians(axis) *
 np.cos(days * 2 * np.pi / 365.25)))
 return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.
 daily['daylight_hrs'] = list(map(hours_of_daylight, daily.index))
 daily[['daylight_hrs']].plot();

\1
In[19]: # Температуры указаны в десятых долях градуса Цельсия; 
 # преобразуем в градусы
 weather['TMIN'] /= 10
 weather['TMAX'] /= 10
 weather['Temp (C)'] = 0.5 * (weather['TMIN'] + weather['TMAX'])
 # Осадки указаны в десятых долях миллиметра; преобразуем в дюймы
 weather['PRCP'] /= 254
 weather['dry day'] = (weather['PRCP'] == 0).astype(int)
 daily = daily.join(weather[['PRCP', 'Temp (C)', 'dry day']])

--- СТРАНИЦА 456 ---

\1n[20]: daily['annual'] = (daily.index - daily.index[0]).days / 365.

\1
In[21]: daily.head()
Out[21]:
 Total Mon Tue Wed Thu Fri Sat Sun holiday daylight_hrs \\
Date
2012-10-03 3521 0 0 1 0 0 0 0 0 11.277359
2012-10-04 3475 0 0 0 1 0 0 0 0 11.219142
2012-10-05 3148 0 0 0 0 1 0 0 0 11.161038
2012-10-06 2006 0 0 0 0 0 1 0 0 11.103056
2012-10-07 2142 0 0 0 0 0 0 1 0 11.045208
 PRCP Temp (C) dry day annual
Date
2012-10-03 0 13.35 1 0.000000
2012-10-04 0 13.60 1 0.002740
2012-10-05 0 15.30 1 0.005479
2012-10-06 0 15.85 1 0.008219
2012-10-07 0 15.85 1 0.010959
После этого можно выбрать нужные столбцы и обучить линейную регрессионную 
модель на наших данных. Зададим параметр fit_intercept = False , поскольку 
флаги для дней, по сути, выполняют подбор точек пересечения с осями координат 
по дням:
In[22]:
column_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'holiday',
 'daylight_hrs', 'PRCP', 'dry day', 'Temp (C)', 'annual']
X = daily[column_names]
y = daily['Total'] # Всего
model = LinearRegression(fit_intercept=False)
model.fit(X, y)
daily['predicted'] = model.predict(X)

\1
In[23]: daily[['Total', 'predicted']].plot(alpha=0.5);

--- СТРАНИЦА 457 ---

\1
In[24]: params = pd.Series(model.coef_, index=X.columns)
 params
Out[24]: Mon 503.797330
 Tue 612.088879
 Wed 591.611292
 Thu 481.250377
 Fri 176.838999
 Sat -1104.321406
 Sun -1134.610322
 holiday -1187.212688
 daylight_hrs 128.873251
 PRCP -665.185105
 dry day 546.185613
 Temp (C) 65.194390
 annual 27.865349
 dtype: float64

--- СТРАНИЦА 458 ---

\1n[25]: from sklearn.utils import resample
 np.random.seed(1)
 err = np.std([model.fit(*resample(X, y)).coef_
 for i in range(1000)], 0)

\1
In[26]: print(pd.DataFrame({'effect': params.round(0),
 'error': err.round(0)}))
 effect error
Mon 504 85
Tue 612 82
Wed 592 82
Thu 481 85
Fri 177 81
Sat -1104 79
Sun -1135 82
holiday -1187 164
daylight_hrs 129 9
PRCP -665 62
dry day 546 33
Temp (C) 65 4
annual 28 18
Прежде всего мы видим, что существует довольно устойчивая тенденция относи -
тельно еженедельного минимума: по будним дням велосипедистов намного больше, 
чем по выходным и праздникам. Мы видим, что с каждым дополнительным часом 
светлого времени суток велосипедистов становится больше на 129 ± 9; рост темпе -
ратуры на 1 градус Цельсия стимулирует 65 ± 4 челов ек взяться за велосипед; сухой 
день означает в среднем на 546 ± 33 больше велосипедистов; каждый дюйм осадков 
означает, что на 665 ± 62 больше людей оставляют велосипед дома. После учета всех 
влияний мы получаем умеренный рост ежедневного количества велосипедистов на 
28 ± 18 человек в год.
Нашей модели почти наверняка недостает определенной относящейся к делу 
информации. Например, нелинейные влияния (такие как совместное влияние 
осадков и низкой температуры) и нелинейные тренды в пределах каждой из 
переменных (такие как нежелание ездить на велосипедах в очень холодную 
и очень жаркую погоду) не могут быть учтены в этой модели. Кроме того, мы 
отбросили некоторые нюансы (такие как различие между дождливым утром 

\1nes, SVMs) — очень мощный 
и гибкий класс алгоритмов обучения с учителем как д ля классификации, так 
и регрессии. В этом разделе мы научимся интуитивно понимать, как использо -
вать метод опорных векторов в задачах классификации. 

\1
In[1]: %matplotlib inline
 import numpy as np
 import matplotlib.pyplot as plt
 from scipy import stats
 # Воспользуемся настройками по умолчанию библиотеки Seaborn
 import seaborn as sns; sns.set()
Основания для использования метода 
опорных векторов
В ходе нашего обсуждения байесовской классификации (см. раздел «Загля -
нем глубже: наивная байесовская классификация» данной главы) мы изучили 
простую модель, описывающую распределение всех базовых классов, и вос -
пользовались подобными порождающими моделями для вероятностного опре -
деления меток для новых точек. Это был пример порождающей классификации 
(generative classification), здесь же мы рассмотрим разделяющую классификацию 
(discriminative classification). 

\1
In[2]: from sklearn.datasets.samples_generator import make_blobs
 X, y = make_blobs(n_samples=50, centers=2,
 random_state=0, cluster_std=0.60)
 plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');

--- СТРАНИЦА 460 ---

\1n[3]: xfit = np.linspace(-1, 3.5)
 plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
 plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2,
 markersize=10)
 for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:
 plt.plot(xfit, m * xfit + b, '-k')
 plt.xlim(-1, 3.5);
На рис. 5.54 показаны три очень разных разделителя, тем не менее прекрасно раз -
деляющих наши выборки. В зависимости от того, какой из них вы выберете, новой 
точке данных (например, отмеченной знаком «X» на рис. 5.54) будут присвоены 
различные метки! Очевидно, наш интуитивный подход с «проведением прямой 
между классами» работает недостаточно хорошо и нужно подойти к вопросу с более 
глубоких позиций.
Метод опорных векторов: максимизируем отступ
Метод опорных векторов предоставляет способ решения этой проблемы. Идея 
заключается в следующем: вместо того чтобы рисовать между классами прямую 
нулевой ширины, можно нарисовать около каждой из прямых отступ (margin) 
некоторой ширины, простирающийся до ближайшей точки. Вот пример того, как 
подобный подход мог бы выглядеть (рис. 5.55).
Рис. 5.55. Визуализация «отступов» в разделяющих классификаторах
In[4]:
xfit = np.linspace(-1, 3.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')

--- СТРАНИЦА 462 ---

\1n [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:
 yfit = m * xfit + b plt.plot(xfit, yfit, '-k')
 plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',
 color='#AAAAAA', alpha=0.4)
plt.xlim(-1, 3.5);
В методе опорных векторов в качестве оптимальной модели выбирается линия, 
максимизирующая этот отступ. Метод опорных векторов — пример оценивателя 
с максимальным отступом (maximum margin estimator).

\1
In[5]: from sklearn.svm import SVC # "Классификатор на основе метода опорных 
 # векторов"
 model = SVC(kernel='linear', C=1E10)
 model.fit(X, y)
Out[5]: SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,
 decision_function_shape=None, degree=3, gamma='auto', 
 kernel='linear',
 max_iter=-1, probability=False, random_state=None, shrinking=True,
 tol=0.001, verbose=False)

\1
In[6]: 
 def plot_svc_decision_function(model, ax=None, plot_support=True):
 """Строим график решающей функции для двумерной SVC"""
 if ax is None:
 ax = plt.gca()
 xlim = ax.get_xlim()
 ylim = ax.get_ylim()
 # Создаем координатную сетку для оценки модели
 x = np.linspace(xlim[0], xlim[1], 30)
 y = np.linspace(ylim[0], ylim[1], 30)
 Y, X = np.meshgrid(y, x)
 xy = np.vstack([X.ravel(), Y.ravel()]).T
 P = model.decision_function(xy).reshape(X.shape)
 # Рисуем границы принятия решений и отступы
 ax.contour(X, Y, P, colors='k',

--- СТРАНИЦА 463 ---
Заглянем глубже: метод опорных векторов 463
 levels=[-1, 0, 1], alpha=0.5,
 linestyles=['--', '-', '--'])
 # Рисуем опорные векторы
 if plot_support:
 ax.scatter(model.support_vectors_[:, 0],
 model.support_vectors_[:, 1],
 s=300, linewidth=1, facecolors='none');
 ax.set_xlim(xlim)
 ax.set_ylim(ylim)
In[7]: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
 plot_svc_decision_function(model);
Рис. 5.56. Обучение классификатора на основе метода опорных векторов. На рисунке показаны 
границы отступов (штриховые линии) и опорные векторы (окружности)
Эта разделяющая линия максимизирует отступ между двумя наборами точек. Об-
ратите внимание, что некоторые из обучающих точек лишь касаются отступа. Они 
отмечены на рис. 5.56 черными окружностями. Эти точки — ключевые элементы 
аппроксимации, они известны под названием опорных векторов (support vectors), 
в их честь алгоритм и получил свое название. В библиотеке Scikit-Learn данные об 
этих точках хранятся в атрибуте support_vectors_ классификатора:
In[8]: model.support_vectors_
Out[8]: array([[ 0.44359863, 3.11530945],
 [ 2.33812285, 3.43116792],
 [ 2.06156753, 1.96918596]])
Ключ к успеху классификатора в том, что значение имеет только расположе -
ние опорных векторов. Все, находящиеся на правильной стороне, но дальше от 

--- СТРАНИЦА 464 ---

\1n[9]: def plot_svm(N=10, ax=None):
 X, y = make_blobs(n_samples=200, centers=2,
 random_state=0, cluster_std=0.60)
 X = X[:N]
 y = y[:N]
 model = SVC(kernel='linear', C=1E10)
 model.fit(X, y)
 ax = ax or plt.gca()
 ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
 ax.set_xlim(-1, 4)
 ax.set_ylim(-1, 6)
 plot_svc_decision_function(model, ax)
 fig, ax = plt.subplots(1, 2, figsize=(16, 6))
 fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)
 for axi, N in zip(ax, [60, 120]):
 plot_svm(N, axi)
 axi.set_title('N = {0}'.format(N))
Рис. 5.57. Влияние новых обучающих точек на модель SVM
На левом рисунке мы видим модель и опорные векторы для 60 обучающих точек. 
На правом рисунке мы удвоили количество обучающих точек, но модель не из -
менилась: три опорных вектора с левого рисунка остались опорными векторами 
и справа. Подобная невосприимчивость к тому, как именно ведут себя удаленные 
точки, — одна из сильных сторон модели SVM.

--- СТРАНИЦА 465 ---
Заглянем глубже: метод опорных векторов 465
Если вы работаете с этим блокнотом в интерактивном режиме, можете восполь -
зоваться интерактивными виджетами оболочки IPython для интерактивного про-
смотра этой возможности модели SVM (рис. 5.58):
In[10]: from ipywidgets import interact, fixed
 interact(plot_svm, N=[10, 200], ax=fixed(None));
Рис. 5.58. Первый кадр интерактивной визуализации SVM (см. полную версию 
в онлайн-приложении (https://github.com/jakevdp/PythonDataScienceHandbook))
Выходим за границы линейности: SVM-ядро
Возможности метода SVM особенно расширяются при его комбинации с ядрами 
(kernels). Мы уже сталкивались с ними ранее, в регрессии по комбинации базисных 
функций из раздела «Заглянем глубже: линейная регрессия» данной главы. Там 
мы занимались проекцией данных в пространство с большей размерностью, опре-
деляемое полиномиальными и Гауссовыми базисными фун кциями, и благодаря 
этому имели возможность аппроксимировать нелинейные зависимости с помощью 
линейного классификатора.

\1
In[11]: from sklearn.datasets.samples_generator import make_circles
 X, y = make_circles(100, factor=.1, noise=.1)
 clf = SVC(kernel='linear').fit(X, y)
 plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
 plot_svc_decision_function(clf, plot_support=False);

--- СТРАНИЦА 466 ---

\1n[12]: r = np.exp(-(X ** 2).sum(1))

\1
In[13]: from mpl_toolkits import mplot3d
 def plot_3D(elev=30, azim=30, X=X, y=y):
 ax = plt.subplot(projection='3d')
 ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')
 ax.view_init(elev=elev, azim=azim)
 ax.set_xlabel('x')
 ax.set_ylabel('y')
 ax.set_zlabel('r')
 interact(plot_3D, elev=[-90, 90], azip=(-180, 180),
 X=fixed(X), y=fixed(y));
Как видим, при наличии третьего измерения данные можно элементарно раз -
делить линейно путем проведения разделяющей плоскости на высоте, скажем, 
r = 0,7 .

--- СТРАНИЦА 467 ---
Заглянем глубже: метод опорных векторов 467
Рис. 5.60. Добавление в данные третьего измерения 
дает возможность линейного разделения
Нам пришлось тщательно выбрать и внимательно настроить нашу проекцию: если 
бы мы не центрировали радиальную базисную функцию должным образом, то не 
получили бы столь «чистых», разделяемых линейно результатов. Необходимость 
подобного выбора — задача, требующая решения: хотелось бы каким-то образом 
автоматически находить оптимальные базисные функции.
Одна из применяемых с этой целью стратегий состоит в вычислении базисных 
функций, центрированных по каждой из точек набора данных, с тем чтобы далее 
алгоритм SVM проанализировал полученные результаты. Эта разновидность пре -
образования базисных функций, известная под названием преобразования ядра 
(kernel transformation), основана на отношении подобия (или ядре) между каждой 
парой точек.
Потенциальная проблема с этой методикой — проекцией N точек на N измере -
ний — состоит в том, что при росте N она может потребовать колоссальных объ -
емов вычислений. Однако благодаря изящной процедуре, известной под названием 
kernel trick ( https://en.wikipedia.org/wiki/Kernel_method), обучение на преобразованных 
с помощью ядра данных можно произвести неявно, то есть даже без построения 
полного N-мерного представления ядерной проекции! Этот kernel trick является 
частью SVM и одной из причин мощи этого метода.
В библиотеке Scikit-Learn, чтобы применить алгоритм SVM с использованием 
ядерного преобразования, достаточно просто заменить линейное ядро на ядро RBF 
(radial basis function — «радиальная базисная функция») с помощью гиперпараме -
тра модели kernel (рис. 5.61):
In[14]: clf = SVC(kernel='rbf', C=1E6)
 clf.fit(X, y)

--- СТРАНИЦА 468 ---

\1ne, coef0=0.0,
 decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
 max_iter=-1, probability=False, random_state=None, shrinking=True,
 tol=0.001, verbose=False)
In[15]: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
 plot_svc_decision_function(clf)
 plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],
 s=300, lw=1, facecolors='none');
Рис. 5.61. Обучение ядерного SVM на наших данных
С помощью этого ядерного метода опорных векторов мы можем определить под-
ходящую нелинейную границу решений. Такая методика ядерного преобразования 
часто используется в машинном обучении для превращения быстрых линейных 
методов в быстрые нелинейные, особенно для моделей, в которых можно восполь -
зоваться kernel trick.

\1
In[16]: X, y = make_blobs(n_samples=100, centers=2,
 random_state=0, cluster_std=1.2)
 plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');

--- СТРАНИЦА 469 ---
Заглянем глубже: метод опорных векторов 469
Рис. 5.62. Данные с определенным перекрытием
На этот случай в реализации метода SVM есть небольшой поправочный парам етр 
для «размытия» отступа. Данный параметр разрешает некоторым точкам «за -
ходить» на отступ в тех случаях, когда это приводит к лучшей аппроксимации. 
Степень размытости отступа управляется настроечным параметром, известным под 
названием C. При очень большом значении параметра C отступ является «жестким» 
и точки не могут находиться на нем. При меньшем значе нии параметра C отступ 
становится более размытым и может включать в себя некоторые точки.
На рис. 5.63 показана наглядная картина того, какое влияние изменение параме-
тра C оказывает на итоговую аппроксимацию посредством размытия отступа.
Рис. 5.63. Влияние параметра C на аппроксимацию методом опорных векторов
In[17]: X, y = make_blobs(n_samples=100, centers=2,
 random_state=0, cluster_std=0.8)
 fig, ax = plt.subplots(1, 2, figsize=(16, 6))

--- СТРАНИЦА 470 ---

\1n zip(ax, [10.0, 0.1]):
 model = SVC(kernel='linear', C=C).fit(X, y)
 axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
 plot_svc_decision_function(model, axi)
 axi.scatter(model.support_vectors_[:, 0],
 model.support_vectors_[:, 1],
 s=300, lw=1, facecolors='none');
 axi.set_title('C = {0:.1f}'.format(C), size=14)
Оптимальное значение параметра C зависит от конкретного набора данных. Его сле-
дует настраивать с помощью перекрестной проверки или какой-либо аналогичной 
процедуры (дальнейшую информацию см. в разделе «Гиперпараметры и проверка 
модели» данной главы).
Пример: распознавание лиц
В качестве примера работы метода опорных векторов рассмотрим задачу распознава-
ния лиц. Мы воспользуемся набором данных Labeled Faces in the Wild 1 (LFW), состоя -
щим из нескольких тысяч упорядоченных фотографий различных общественных дея-
телей. В библиотеку Scikit-Learn встроена утилита для загрузки этого набора данных:
In[18]: from sklearn.datasets import fetch_lfw_people
 faces = fetch_lfw_people(min_faces_per_person=60)
 print(faces.target_names)
 print(faces.images.shape)
['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'
 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']
(1348, 62, 47)

\1
In[19]: fig, ax = plt.subplots(3, 5)
 for i, axi in enumerate(ax.flat):
 axi.imshow(faces.images[i], cmap='bone')
 axi.set(xticks=[], yticks=[],
 xlabel=faces.target_names[faces.target[i]])
Каждое изображение содержит 62 × 47, то есть примерно 3000 пикселов. Мы можем 
рассматривать каждый пиксел как признак, но эффективнее использовать какой-
либо препроцессор для извлечения более осмысленных признаков. В данном слу-
чае мы воспользуемся методом главных компонент (см. раздел «Заглянем глубже: 
метод главных компонент» данной главы) для извлечения 150 базовых компонент, 

\1n[20]: from sklearn.svm import SVC
 from sklearn.decomposition import RandomizedPCA
 from sklearn.pipeline import make_pipeline
 pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)
 svc = SVC(kernel='rbf', class_weight='balanced')
 model = make_pipeline(pca, svc)
Рис. 5.64. Примеры из набора данных Labeled Faces in the Wild

\1
In[21]: from sklearn.cross_validation import train_test_split
 Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, 
 faces.target,
 random_state=42)

\1

--- СТРАНИЦА 472 ---

\1n[22]: from sklearn.grid_search import GridSearchCV
 param_grid = {'svc__C': [1, 5, 10, 50],
 'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}
 grid = GridSearchCV(model, param_grid)
 %time grid.fit(Xtrain, ytrain)
 print(grid.best_params_)
CPU times: user 47.8 s, sys: 4.08 s, total: 51.8 s
Wall time: 26 s
{'svc__gamma': 0.001, 'svc__C': 10}

\1
In[23]: model = grid.best_estimator_
 yfit = model.predict(Xtest)

\1
Рис. 5.65. Прогнозируемые имена. Неверные метки выделены полужирным

--- СТРАНИЦА 473 ---
Заглянем глубже: метод опорных векторов 473
In[24]: fig, ax = plt.subplots(4, 6)
 for i, axi in enumerate(ax.flat):
 axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')
 axi.set(xticks=[], yticks=[])
 axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],
 color='black' if yfit[i] == ytest[i] else 'red')
 fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);

\1
In[25]: from sklearn.metrics import classification_report
 print(classification_report(ytest, yfit,
 target_names=faces.target_names))
 precision recall f1-score support
 Ariel Sharon 0.65 0.73 0.69 15
 Colin Powell 0.81 0.87 0.84 68
 Donald Rumsfeld 0.75 0.87 0.81 31
 George W Bush 0.93 0.83 0.88 126
Gerhard Schroeder 0.86 0.78 0.82 23
 Hugo Chavez 0.93 0.70 0.80 20
Junichiro Koizumi 0.80 1.00 0.89 12
 Tony Blair 0.83 0.93 0.88 42
 avg / total 0.85 0.85 0.85 337

\1
In[26]: from sklearn.metrics import confusion_matrix
 mat = confusion_matrix(ytest, yfit)
 sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,
 xticklabels=faces.target_names,
 yticklabels=faces.target_names)
 plt.xlabel('true label')
 plt.ylabel('predicted label');
Эта информация позволяет нам понять, какие метки, вероятно, оцениватель опре -
делит неверно.
В реальных задачах распознавания лиц, в которых фотографии не кадрированы пред-
варительно в аккуратные сетки, единственное отличие в схеме классификации лиц 
будет состоять в выборе признаков. Необходимо будет использовать более сложный 
алгоритм для поиска лиц и извлекать не зависящие от пикселизации признаки. Для 
подобных приложений удобно применять библиотеку OpenCV ( http://opencv.org/), 
которая, помимо прочего, включает заранее обученные реализации современных 
инструментов выделения признаков для изображений вообще и лиц в частности.

--- СТРАНИЦА 474 ---

\1ndom forests). 
Случайные леса — пример одного из методов ансамблей (ensemble), основанных 
на агрегировании результатов ансамбля более простых оценивателей. Несколько 
неожиданный результат использования подобных методов ансамблей — то, что 
целое в данном случае оказывается больше суммы составных частей. Результат 
«голосования» среди достаточного количества оценивателей может оказаться 
лучше результата любого из отдельных участников «голосования»! Мы увидим 
примеры этого в следующих разделах. Начнем с обычных импортов:
In[1]: %matplotlib inline
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns; sns.set()
Движущая сила случайных лесов: деревья 
принятия решений
Случайные леса — пример обучаемого ансамбля на основе деревьев принятия ре-
шений. Поэтому мы начнем с обсуждения самих деревьев решений.

--- СТРАНИЦА 476 ---

\1n[2]: from sklearn.datasets import make_blobs
 X, y = make_blobs(n_samples=300, centers=4,
 random_state=0, cluster_std=1.0)
 plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');

--- СТРАНИЦА 477 ---
Заглянем глубже: деревья решений и случайные леса 477
Рис. 5.68. Данные для классификатора на основе дерева принятия решений
Простое дерево принятия решений для этих данных будет многократно разделять 
данные по одной или нескольким осям, в соответствии с определенным количе -
ственным критерием, и на каждом уровне маркировать новую область согласно 
большинству лежащих в ней точек. На рис. 5.69 приведена визуализация первых 
четырех уровней классификатора для этих данных, созданного на основе дерева 
принятия решений.
Рис. 5.69. Визуализация разбиения данных деревом принятия решений
Обратите внимание, что после первого разбиения все точки в верхней ветке оста-
ются неизменными, поэтому необходимости в дальнейшем ее разбиении нет. За ис-
ключением узлов, в которых присутствует только один цвет, на каждом из уровней 
все области снова разбиваются по одному из двух признаков.
Процесс обучения дерева принятия решений на наших данных можно выполнить 
в Scikit-Learn с помощью оценивателя DecisionTreeClassifier:
In[3]: from sklearn.tree import DecisionTreeClassifier
 tree = DecisionTreeClassifier().fit(X, y)

\1

--- СТРАНИЦА 478 ---

\1n[4]: 
 def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):
 ax = ax or plt.gca()
 # Рисуем обучающие точки
 ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,
 clim=(y.min(), y.max()), zorder=3)
 ax.axis('tight')
 ax.axis('off')
 xlim = ax.get_xlim()
 ylim = ax.get_ylim()
 # Обучаем оцениватель
 model.fit(X, y)
 xx, yy = np.meshgrid(np.linspace(*xlim, num=200),
 np.linspace(*ylim, num=200))
 Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
 # Создаем цветной график с результатами
 n_classes = len(np.unique(y))
 contours = ax.contourf(xx, yy, Z, alpha=0.3,
 levels=np.arange(n_classes + 1) - 0.5,
 cmap=cmap, clim=(y.min(), y.max()),
 zorder=1)
 ax.set(xlim=xlim, ylim=ylim)

\1
In[5]: visualize_classifier(DecisionTreeClassifier(), X, y)
Рис. 5.70. Визуализация классификации на основе дерева принятия решений

--- СТРАНИЦА 479 ---
Заглянем глубже: деревья решений и случайные леса 479
Если вы применяете интерактивный блокнот, то можете воспользоваться вспомо-
гательным сценарием, включенным в онлайн-приложение ( https://github.com/jakevdp/
PythonDataScienceHandbook) для вызова интерактивной визуализации процесса по -
строения дерева принятия решений (рис. 5.71):
In[6]: # Модуль helpers_05_08 можно найти в онлайн-приложении к книге
 # (https://github.com/jakevdp/PythonDataScienceHandbook)
 import helpers_05_08
 helpers_05_08.plot_tree_interactive(X, y);
Рис. 5.71. Первый кадр интерактивного виджета дерева принятия решений. Полную версию 
см. в онлайн-приложении (https://github.com/jakevdp/PythonDataScienceHandbook)
Обратите внимание, что по мере возрастания глубины мы получаем области клас-
сификации очень странной формы. Например, на глубине, равной 5, между желтой 
и синей областями появляется узкая и вытянутая в высоту фиолетовая область. 
Очевидно, что такая конфигурация является скорее не результатом собственно 
распределения данных, а конкретной их дискретизации или свойств шума в них. 
То есть это дерево принятия решений на глубине всег о лишь пяти уровней уже 
очевидным образом переобучено.
Деревья принятия решений и переобучение
Подобное переобучение присуще всем деревьям принятия решений: нет ничего 
проще, чем дойти до слишком глубокого уровня дерева, аппроксимируя таким об -
разом нюансы конкретных данных вместо общих характеристик распределений, 
из которых они получены. Другой способ увидеть это переобучение — обратиться 
к моделям, обученным на различных подмножествах набора данных, например, 
на рис. 5.72 показано обучение двух различных деревьев, каждое на половине ис-
ходного набора данных.

--- СТРАНИЦА 480 ---

\1n[7]: # Модуль helpers_05_08 можно найти в онлайн-приложении к книге
 # (https://github.com/jakevdp/PythonDataScienceHandbook)
 import helpers_05_08
 helpers_05_08.randomized_tree_interactive(X, y)
Рис. 5.73. Первый кадр интерактивного виджета случайного дерева принятия решений. Полную 
версию см. в онлайн-приложении (https://github.com/jakevdp/PythonDataScienceHandbook)

--- СТРАНИЦА 481 ---
Заглянем глубже: деревья решений и случайные леса 481
Аналогично тому, как использование информации из двух деревьев позволяет 
достичь лучшего результата, применение информации из многих обеспечит еще 
лучший результат.
Ансамбли оценивателей: случайные леса
Идея комбинации нескольких переобученных оценивателей для снижения эффек-
та этого переобучения лежит в основе метода ансамблей под названием «баггинг» 
(bagging). Баггинг использует ансамбль (например, своеобразную «шляпу фокус-
ника») параллельно работающих переобучаемых оценивателей и усредняет резуль-
таты для получения оптимальной классификации. Ансамбль случайных деревьев 
принятия решений называется случайным лесом (random forest).
Выполнить подобную баггинг-классификацию можно вручную с помощью ме -
таоценивателя BaggingClassifier из библиотеки Scikit-Learn, как показано на 
рис. 5.74:
In[8]: from sklearn.tree import DecisionTreeClassifier
 from sklearn.ensemble import BaggingClassifier
 tree = DecisionTreeClassifier()
 bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,
 random_state=1)
 bag.fit(X, y)
 visualize_classifier(bag, X, y)
Рис. 5.74. Границы принятия решений для ансамбля случайных деревьев решений
В этом примере мы рандомизировали данные путем обучения всех оценивателей 
на случайном подмножестве, состоящем из 80 % обучающих точек. На практике 

--- СТРАНИЦА 482 ---

\1n ( http://scikit-learn.org/stable/modules/ensemble.
html#forest) и упомянутых в ней справочных руководствах.
В библиотеке Scikit-Learn подобный оптимизированный ансамбль случайных дере -
вьев принятия решений, автоматически выполняющий всю рандомизацию, реали -
зован в оценивателе RandomForestClassifier. Все, что остается сделать, — выбрать 
количество оценивателей и он очень быстро (при необходимости параллельно) 
обучит ансамбль деревьев (рис. 5.75):
In[9]: from sklearn.ensemble import RandomForestClassifier
 model = RandomForestClassifier(n_estimators=100, random_state=0)
 visualize_classifier(model, X, y);
Рис. 5.75. Ансамбль случайных деревьев принятия решений
Как видим, путем усреднения более чем 100 случайно возмущенных моделей мы 
получаем общую модель, намного более близкую к нашим интуитивным представ-
лениям о правильном разбиении параметрического пространства.
Регрессия с помощью случайных лесов
В предыдущем разделе мы рассмотрели случайные леса в контексте класси-
фикации. Случайные леса могут также оказаться полезными для регрессии 
(то есть непрерывных, а не категориальных величин). В этом случае используется 
оцениватель RandomForestRegressor, синтаксис которого напоминает показанный 
выше.

--- СТРАНИЦА 483 ---

\1
In[10]: rng = np.random.RandomState(42)
 x = 10 * rng.rand(200)
 def model(x, sigma=0.3):
 fast_oscillation = np.sin(5 * x)
 slow_oscillation = np.sin(0.5 * x)
 noise = sigma * rng.randn(len(x))
 return slow_oscillation + fast_oscillation + noise
 y = model(x)
 plt.errorbar(x, y, 0.3, fmt='o');

\1
In[11]: from sklearn.ensemble import RandomForestRegressor
 forest = RandomForestRegressor(200)
 forest.fit(x[:, None], y)
 xfit = np.linspace(0, 10, 1000)
 yfit = forest.predict(xfit[:, None])
 ytrue = model(xfit, sigma=0)
 plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)
 plt.plot(xfit, yfit, '-r');
 plt.plot(xfit, ytrue, '-k', alpha=0.5);

--- СТРАНИЦА 484 ---

\1n» этой главы). Воспользуемся ими снова, чтобы посмотреть 
на применение классификатора на основе случайных лесов в данном контексте.
In[12]: from sklearn.datasets import load_digits
 digits = load_digits()
 digits.keys()
Out[12]: dict_keys(['target', 'data', 'target_names', 'DESCR', 'images'])

\1
In[13]:
# Настройки рисунка
fig = plt.figure(figsize=(6, 6)) # размер рисунка в дюймах
fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, 
wspace=0.05)
# Рисуем цифры: размер каждого изображения 8 x 8 пикселов
for i in range(64):
 ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])
 ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')

--- СТРАНИЦА 485 ---

\1
In[14]:
from sklearn.cross_validation import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,
 random_state=0)
model = RandomForestClassifier(n_estimators=1000)
model.fit(Xtrain, ytrain)
ypred = model.predict(Xtest)

\1
In[15]: from sklearn import metrics
 print(metrics.classification_report(ypred, ytest))
 precision recall f1-score support
 0 1.00 0.97 0.99 38
 1 1.00 0.98 0.99 44
 2 0.95 1.00 0.98 42

--- СТРАНИЦА 486 ---

\1n[16]: from sklearn.metrics import confusion_matrix
 mat = confusion_matrix(ytest, ypred)
 sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
 plt.xlabel('true label')
 plt.ylabel('predicted label');
Рис. 5.79. Матрица различий для классификации цифр 
с помощью случайных лесов
Оказалось, что простой, не настроенный каким-то специальным образом случай-
ный лес дает очень точную классификацию данных по рукописным цифрам.
Резюме по случайным лесам
В этом разделе мы познакомили вас с понятием ансамблей оценивателей (ensemble 
estimators) и, в частности, модели случайного леса — ансамбля случайных деревьев 

--- СТРАНИЦА 487 ---
Заглянем глубже: метод главных компонент 487
принятия решений. Случайные леса — мощный метод, обладающий несколькими 
достоинствами.
 Как обучение, так и предсказание выполняются очень быстро в силу простоты 
лежащих в основе модели деревьев принятия решений. Кроме того, обе задачи 
допускают эффективную параллелизацию, так как отдельные деревья пред -
ставляют собой совершенно независимые сущности.
 Вариант с несколькими деревьями дает возможность использования вероят -
ностной классификации: решение путем «голосования» оценивателей дает 
оценку вероятности (в библиотеке Scikit-Learn ее можно получить с помощью 
метода predict_proba()).
 Непараметрическая модель исключительно гибка и может э ффективно ра -
ботать с задачами, на которых другие оцениватели оказываются недообучен -
ными.
Основной недостаток случайных лесов состоит в том, что результаты сложно ин-
терпретировать.Чтобы сделать какие-либо выводы относительно смысла модели 
классификации, случайные леса — не лучший вариант.
Заглянем глубже: метод главных компонент
До сих пор мы подробно изучали оцениватели для машинного обучения с учителем, 
предсказывающие метки на основе маркированных обучающих данных. Здесь же 
мы изучим несколько оценивателей без учителя, позволяющих подчеркнуть инте-
ресные аспекты данных безотносительно каких-либо известных меток.
Мы обсудим, возможно, один из наиболее широко используемых алгоритмов ма -
шинного обучения без учителя — метод главных компонент (principal component 
analysis, PCA). PCA представляет собой алгоритм понижения размерности, но он 
может быть также удобен в качестве инструмента визуализации, фильтрации шума, 
выделения и проектирования признаков, а также многого другого. После краткого 
концептуального обзора алгоритма PCA мы рассмотрим несколько примеров при -
кладных задач. Начнем с обычных импортов:
In[1]: %matplotlib inline
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns; sns.set()
Знакомство с методом главных компонент
Метод главных компонент — быстрый и гибкий метод машинного обучения без 
учителя, предназначенный для понижения размерности данных. Мы познакоми-
лись с ним в разделе «Знакомство с библиотекой Scikit-Learn» этой главы. Легче 

--- СТРАНИЦА 488 ---

\1n[2]: rng = np.random.RandomState(1)
 X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T
 plt.scatter(X[:, 0], X[:, 1])
 plt.axis('equal');
Визуально очевидно, что зависимость между величинами x и y практически ли -
нейна. Это напоминает данные линейной регрессии, которые мы изучали в разделе 
«Заглянем глубже: линейная регрессия» этой главы, но постановка задачи здесь 
несколько иная: задача машинного обучения без учителя состоит в выяснении за-
висимости между величинами x и y, а не в предсказании значений величины y по 
значениям величины x .
Рис. 5.80. Данные для демонстрации алгоритма PCA
В методе главных компонент выполняется количественная оценка этой зависимо-
сти путем нахождения списка главных осей координат (principal axes) данных и их 
использования для описания набора данных. Выполнить это с помощью оценива-
теля PCA из библиотеки Scikit-Learn можно следующим образом:
In[3]: from sklearn.decomposition import PCA
 pca = PCA(n_components=2)
 pca.fit(X)
Out[3]: PCA(copy=True, n_components=2, whiten=False)
При обучении алгоритм определяет некоторые относящиеся к данным величины, 
самые важные из них — компоненты и объяснимая дисперсия (explained variance):
In[4]: print(pca.components_)

--- СТРАНИЦА 489 ---
Заглянем глубже: метод главных компонент 489
[[ 0.94446029 0.32862557]
 [ 0.32862557 -0.94446029]]
In[5]: print(pca.explained_variance_)
[ 0.75871884 0.01838551]

\1
In[6]: def draw_vector(v0, v1, ax=None):
 ax = ax or plt.gca()
 arrowprops=dict(arrowstyle='->',
 linewidth=2,
 shrinkA=0, shrinkB=0)
 ax.annotate('', v1, v0, arrowprops=arrowprops)
 # Рисуем данные
 plt.scatter(X[:, 0], X[:, 1], alpha=0.2)
 for length, vector in zip(pca.explained_variance_, pca.components_):
 v = vector * 3 * np.sqrt(length)
 draw_vector(pca.mean_, pca.mean_ + v)
 plt.axis('equal');
Рис. 5.81. Визуализация главных осей данных
Эти векторы отражают главные оси координат данных, а показанная на рис. 5.81 
длина соответствует «важности» роли данной оси при описании распределения 
данных, точнее говоря, это мера дисперсии данных при проекции на эту ось. Про -
екции точек данных на главные оси и есть главные компоненты данных.

--- СТРАНИЦА 490 ---

\1ne transformation). По существу, это значит, что оно 
состоит из сдвига (translation), вращения (rotation) и пропорционального масшта -
бирования (uniform scaling).

\1
In[7]: pca = PCA(n_components=1)
 pca.fit(X)
 X_pca = pca.transform(X)
 print("original shape: ", X.shape)
 print("transformed shape:", X_pca.shape)
original shape: (200, 2)
transformed shape: (200, 1)

--- СТРАНИЦА 491 ---

\1
In[8]: X_new = pca.inverse_transform(X_pca)
 plt.scatter(X[:, 0], X[:, 1], alpha=0.2)
 plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)
 plt.axis('equal');
Рис. 5.83. Визуализация PCA как метода понижения размерности
Более светлые точки — исходные данные, а более темные — спроецированная 
версия. Из рисунка становится понятно, что означает понижение размерности 
с помощью PCA: информация по наименее важной главной оси/осям координат 
уничтожается и остается только компонента (-ы) данных с максимальной дис -
персией. Отсекаемая часть дисперсии (пропорциональная разбросу точек рядом 
с линией, показанному на рис. 5.83) является приближенной мерой того, сколько 
«информации» отбрасывается при этом понижении размерности.
Набор данных пониженной размерности в каком-то смысле «достаточно хорошо» 
подходит для кодирования важнейших зависимостей между точка ми: несмотря 
на понижение размерности данных на 50 %, общая зависимость между точками 
данных по большей части была сохранена.
Использование метода PCA для визуализации: рукописные цифры
Полезность метода понижения размерности, возможно, не вполне ясна в случае 
двух измерений, но становится более очевидной при работе с многомерными дан-
ными. Рассмотрим приложение метода PCA к данным по рукописным цифрам, 
с которыми мы уже работали в разделе «Заглянем глубже: деревья принятия ре-
шений и случайные леса» этой главы.

--- СТРАНИЦА 492 ---

\1n[9]: from sklearn.datasets import load_digits
 digits = load_digits()
 digits.data.shape
Out[9]:
(1797, 64)

\1
In[10]: pca = PCA(2) # Проекция из 64-мерного в двумерное пространство
 projected = pca.fit_transform(digits.data)
 print(digits.data.shape)
 print(projected.shape)
(1797, 64)
(1797, 2)

\1
In[11]: plt.scatter(projected[:, 0], projected[:, 1],
 c=digits.target, edgecolor='none', alpha=0.5,
 cmap=plt.cm.get_cmap('spectral', 10))
 plt.xlabel('component 1') # Компонента 1
 plt.ylabel('component 2') # Компонента 2
 plt.colorbar();

\1
Один из возможных способов понижения размерности этих данных — обнуление 
большей части базисных векторов. Например, если мы будем использовать только 
первые восемь пикселов, то получим восьмимерную проекцию данных (рис. 5.85), 
но она будет плохо отражать изображение в целом: мы отбрасываем почти 90 % 
пикселов!
Рис. 5.85. Наивный метод понижения размерности 
путем отбрасывания пикселов
Верхний ряд на рисунке демонстрирует отдельные пикселы, а нижний — общий 
вклад этих пикселов в структуру изображения. С помощью только восьми из 
компонент пиксельного базиса можно сконструировать лишь небольшую часть 
64-пиксельного изображения. Продолжив эту последовательность действий и ис-
пользовав все 64 пиксела, мы бы получили исходное изображение.

--- СТРАНИЦА 494 ---

\1ned variance ratio) в виде функции от количества компонент (рис. 5.87):
In[12]: pca = PCA().fit(digits.data)
 plt.plot(np.cumsum(pca.explained_variance_ratio_))
 plt.xlabel('number of components')
 plt.ylabel('cumulative explained variance');

\1
In[13]: 
 def plot_digits(data):
 fig, axes = plt.subplots(4, 10, figsize=(10, 4),
 subplot_kw={'xticks':[], 'yticks':[]},
 gridspec_kw=dict(hspace=0.1, wspace=0.1))
 for i, ax in enumerate(axes.flat):

--- СТРАНИЦА 496 ---

\1nary', interpolation='nearest',
 clim=(0, 16))
 plot_digits(digits.data)

\1
In[14]: np.random.seed(42)
 noisy = np.random.normal(digits.data, 4)
 plot_digits(noisy)

\1
In[15]: pca = PCA(0.50).fit(noisy)
 pca.n_components_
Out[15]: 12

--- СТРАНИЦА 497 ---

\1
In[16]: components = pca.transform(noisy)
 filtered = pca.inverse_transform(components)
 plot_digits(filtered)
Рис. 5.90. Цифры с устраненным с помощью метода PCA шумом
Эти возможности по сохранению сигнала/фильтрации шума делают метод PCA 
очень удобной процедурой выбора признаков, например, вместо обучения клас -
сификатора на чрезвычайно многомерных данных можно обучить его на низко -
размерном представлении, что автоматически приведет к фильтрации случайного 
шума во входных данных.
Пример: метод Eigenfaces
Ранее мы рассмотрели пример использования проекции PCA в качестве процеду-
ры выбора признаков для распознавания лиц с помощью метода опорных векторов 
(см. раздел «Заглянем глубже: метод опорных векторов» данной главы). Теперь мы 
вернемся к этому примеру и рассмотрим его подробнее. Напоминаю, что мы исполь-
зуем набор данных Labeled Faces in the Wild (LFW), доступный через библиотеку 
Scikit-Learn:
In[17]: from sklearn.datasets import fetch_lfw_people
 faces = fetch_lfw_people(min_faces_per_person=60)
 print(faces.target_names)
 print(faces.images.shape)
['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'
 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']
(1348, 62, 47)
Выясним, какие главные оси координат охватывают этот набор данных. Поскольку 
набор данных велик, воспользуемся классом RandomizedPCA — содержащийся в нем 

--- СТРАНИЦА 498 ---

\1n[18]: from sklearn.decomposition import RandomizedPCA
 pca = RandomizedPCA(150)
 pca.fit(faces.data)
Out[18]: RandomizedPCA(copy=True, iterated_power=3, n_components=150,
 random_state=None, whiten=False)
В нашем случае будет интересно визуализировать изображения, соответствующие 
первым нескольким главным компонентам (эти компоненты формально носят на -
звание собственных векторов (eigenvectors), так что подобные изображения часто 
называют «собственными лицами» (eigenfaces)). Как вы видите из рис. 5.91, они 
такие же жуткие, как и их название:
In[19]: fig, axes = plt.subplots(3, 8, figsize=(9, 4),
 subplot_kw={'xticks':[], 'yticks':[]},
 gridspec_kw=dict(hspace=0.1, wspace=0.1))
 for i, ax in enumerate(axes.flat):
 ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')

\1
In[20]: plt.plot(np.cumsum(pca.explained_variance_ratio_))
 plt.xlabel('number of components')
 plt.ylabel('cumulative explained variance');

--- СТРАНИЦА 499 ---

\1
In[21]: # Вычисляем компоненты и проекции лиц
 pca = RandomizedPCA(150).fit(faces.data)
 components = pca.transform(faces.data)
 projected = pca.inverse_transform(components)
In[22]: # Рисуем результаты
 fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),
 subplot_kw={'xticks':[], 'yticks':[]},
 gridspec_kw=dict(hspace=0.1, wspace=0.1))
 for i in range(10):
 ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')
 ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')
 ax[0, 0].set_ylabel('full-dim\ninput')
 # Полноразмерные входные данные
 ax[1, 0].set_ylabel('150-dim\nreconstruction');
 # 150-мерная реконструкция
Рис. 5.93. 150-мерная реконструкция данных из LFW с помощью метода PCA

--- СТРАНИЦА 500 ---

\1n содержит несколько интересных вариантов метода 
PCA, включая классы RandomizedPCA и SparsePCA, находящиеся в модуле sklearn.
decomposition. RandomizedPCA, который мы уже встречали ранее, использует неде -
терминированный метод для быстрой аппроксимации нескольких первых из глав-
ных компонент данных с очень высокой размерностью, а SparsePCA вводит понятие 
регуляризации (см. раздел «Заглянем глубже: линейная регрессия» данной главы), 
служащее для обеспечения разреженности компонент.
В следующих разделах мы рассмотрим другие методы машинного обучения без 
учителя, основывающиеся на некоторых идеях метода PCA.
Заглянем глубже: обучение на базе многообразий
Мы уже ознакомились с возможностями метода главных компонент для решения 
задачи понижения размерности — снижения количества признаков набора данных 
с сохранением существенных зависимостей между точками. Хотя метод PCA гибок, 

--- СТРАНИЦА 501 ---
Заглянем глубже: обучение на базе многообразий 501
быстр и обеспечивает легкость интерпретации результатов, в случае нелинейных 
зависимостей в данных он работает не столь хорошо.
Чтобы справиться с этой проблемой, можно обратиться к классу методов, извест-
ных под названием обучения на базе многообразий (manifold learning). Это класс 
оценивателей без учителя, нацеленных на описание наборов данных как низкораз-
мерных многообразий, вложенных в пространство большей размерности. Чтобы 
понять, что такое многообразие, представьте себе лист бумаги: это двумерный объ -
ект в нашем привычном трехмерном мире, который можно изогнуть или свернуть 
в трех измерениях. В терминах обучения на базе многообразий можно считать этот 
лист двумерным многообразием, вложенным в трехмерное пространство.
Вращение, смена ориентации или растяжение листа бумаги в трехмерном про -
странстве не меняет его плоской геометрии: подобные операции эквивалентны 
линейному вложению. Если согнуть, скрутить или скомкать бумагу, она все равно 
остается двумерным многообразием, но вложение ее в трехмерное пространство 
уже не будет линейным. Алгоритмы обучения на базе многообразий нацелены на 
изучение базовой двумерной природы листа бумаги, даже если он был смят ради 
размещения в трехмерном пространстве.
В этом разделе мы продемонстрируем настройки методов обучения на базе много-
образий, причем наиболее подробно рассмотрим многомерное масштабирова -
ние (multidimensional scaling, MDS), локально линейное вложение (locally linear 
embedding, LLE) и изометрическое отображение (isometric mapping, Isomap). 

\1
In[1]: %matplotlib inline
 import matplotlib.pyplot as plt
 import seaborn as sns; sns.set()
 import numpy as np

\1
In[2]:
def make_hello(N=1000, rseed=42):
 # Создаем рисунок с текстом "HELLO"; сохраняем его в формате PNG
 fig, ax = plt.subplots(figsize=(4, 1))
 fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
 ax.axis('off')
 ax.text(0.5, 0.4, 'HELLO', va='center', ha='center', weight='bold',
 size=85)
 fig.savefig('hello.png')
 plt.close(fig)

--- СТРАНИЦА 502 ---

\1ng')[::-1, :, 0].T
 rng = np.random.RandomState(rseed)
 X = rng.rand(4 * N, 2)
 i, j = (X * data.shape).astype(int).T
 mask = (data[i, j] < 1)
 X = X[mask]
 X[:, 0] *= (data.shape[0] / data.shape[1])
 X = X[:N]
 return X[np.argsort(X[:, 0])]

\1
In[3]: X = make_hello(1000)
 colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5))
 plt.scatter(X[:, 0], X[:, 1], **colorize)
 plt.axis('equal');

\1

--- СТРАНИЦА 503 ---
Заглянем глубже: обучение на базе многообразий 503
In[4]: 
 def rotate(X, angle):
 theta = np.deg2rad(angle)
 R = [[np.cos(theta), np.sin(theta)],
 [-np.sin(theta), np.cos(theta)]]
 return np.dot(X, R)
 X2 = rotate(X, 20) + 5
 plt.scatter(X2[:, 0], X2[:, 1], **colorize)
 plt.axis('equal');
Рис. 5.95. Набор данных после вращения
Это говорит о том, что значения x и y не обязательно важны для внутренних за -
висимостей данных. Существенно в таком случае расстояние между каждой из 
точек и всеми остальными точками набора данных. Для представления его часто 
используют так называемую матрицу расстояний: при N точках создается такой 
массив размера N × N , что его элемент (i , j) содержит расстояние между точками 
i и j . Воспользуемся эффективной функцией pairwise_distances из библиотеки 
Scikit-Learn, чтобы выполнить этот расчет для наших исходных данных:
In[5]: from sklearn.metrics import pairwise_distances
 D = pairwise_distances(X)
 D.shape
Out[5]: (1000, 1000)

\1
In[6]: plt.imshow(D, zorder=2, cmap='Blues', interpolation='nearest')
 plt.colorbar();

--- СТРАНИЦА 504 ---

\1n[7]: D2 = pairwise_distances(X2)
 np.allclose(D, D2)
Out[7]: True

\1
In[8]: from sklearn.manifold import MDS
 model = MDS(n_components=2, dissimilarity='precomputed', 
 random_state=1)
 out = model.fit_transform(D)
 plt.scatter(out[:, 0], out[:, 1], **colorize)
 plt.axis('equal');

\1
In[9]: 
 def random_projection(X, dimension=3, rseed=42):
 assert dimension >= X.shape[1]
 rng = np.random.RandomState(rseed)
 C = rng.randn(dimension, dimension)
 e, V = np.linalg.eigh(np.dot(C, C.T))
 return np.dot(X, V[:X.shape[1]])
 X3 = random_projection(X, 3)
 X3.shape
Out[9]: (1000, 3)

\1
In[10]: from mpl_toolkits import mplot3d
 ax = plt.axes(projection='3d')
 ax.scatter3D(X3[:, 0], X3[:, 1], X3[:, 2],
 **colorize)
 ax.view_init(azim=70, elev=50)

--- СТРАНИЦА 506 ---

\1n[11]: model = MDS(n_components=2, random_state=1)
 out3 = model.fit_transform(X3)
 plt.scatter(out3[:, 0], out3[:, 1], **colorize)
 plt.axis('equal');

\1
In[12]: def make_hello_s_curve(X):
 t = (X[:, 0] - 2) * 0.75 * np.pi
 x = np.sin(t)
 y = X[:, 1]
 z = np.sign(t) * (np.cos(t) - 1)
 return np.vstack((x, y, z)).T 
 XS = make_hello_s_curve(X)

\1
In[13]: from mpl_toolkits import mplot3d
 ax = plt.axes(projection='3d')
 ax.scatter3D(XS[:, 0], XS[:, 1], XS[:, 2],
 **colorize);
Рис. 5.100. Нелинейно вложенные в трехмерное пространство данные

--- СТРАНИЦА 508 ---

\1n[14]: from sklearn.manifold import MDS
 model = MDS(n_components=2, random_state=2)
 outS = model.fit_transform(XS)
 plt.scatter(outS[:, 0], outS[:, 1], **colorize)
 plt.axis('equal');

\1
In[15]:
from sklearn.manifold import LocallyLinearEmbedding
model = LocallyLinearEmbedding(n_neighbors=100, n_components=2, 
method='modified',
 eigen_solver='dense')
out = model.fit_transform(XS)
fig, ax = plt.subplots()
ax.scatter(out[:, 0], out[:, 1], **colorize)
ax.set_ylim(0.15, -0.15);

--- СТРАНИЦА 510 ---

\1n ( https://github.com/mmp2/
megaman) реализованы методы обучения на базе многообразий, масштабиру-
ющиеся гораздо лучше).
С учетом всего этого единственное безусловное преимущество методов обучения 
на базе многообразий перед PCA состоит в их способности сохранять нелинейные 
зависимости в данных. Именно поэтому я стараюсь сначала изучать данные с по-
мощью PCA, а затем использовать методы обучения на базе многообразий.
В библиотеке Scikit-Learn реализовано несколько рас пространенных вариантов обу-
чения на базе многообразий и локально линейного вложения: в документации Scikit-
Learn имеется их обсуждение и сравнение ( http://scikit-learn.org/stable/modules/manifold.html). 
Исходя из моего собственного опыта, могу дать вам следующие рекомендации.
 В модельных задачах, подобных S-образной кривой, локально линейное вло -
жение (LLE) и его варианты (особенно модифицированный метод LLE ) демон -
стрируют отличные результаты. Они реализованы в классе sklearn.mani fold.Lo-
callyLinearEmbedding.
 В случае многомерных данных, полученных из реальных источников, метод LLE 
часто работает плохо, и изометрическое отображение (Isomap), похоже, выдает 
более осмысленные вложения. Оно реализовано в классе sklearn.manifold.Isomap.
 Для сильно кластеризованных данных отличные результаты демонстрирует 
метод стохастического вложения соседей на основе распределения Стьюдента 
(t-distributed stochastic neighbor embedding), хотя и работает иногда очень медленно 
по сравнению с другими методами. Он реализован в классе sklearn.manifold.TSNE.
Если вы хотите посмотреть, как они работают, запустите каждый из них на данных 
из этого раздела.
Пример: использование Isomap для распознавания лиц
Обучение на базе многообразий часто применяется при исследовании зависимо-
стей между многомерными точками данных. Один из распространенных случаев 
многомерных данных — изображения. Например, набор изображений, состоящих 
каждое из 1000 пикселов, можно рассматривать как набор точек в 1000-мерном 
пространстве — яркость каждого пиксела в каждом изображении соответствует 
координате в соответствующем измерении.
Применим алгоритм Isomap к данным, содержащим какие-либо лица. Воспользуем -
ся набором данных Labeled Faces in the Wild (LFW), с которым уже сталкивались 
в разделах «Заглянем глубже: метод опорных векторов» и «Заглянем глубже: метод 

--- СТРАНИЦА 512 ---

\1n[16]: from sklearn.datasets import fetch_lfw_people
 faces = fetch_lfw_people(min_faces_per_person=30)
 faces.data.shape
Out[16]: (2370, 2914)

\1
In[17]: fig, ax = plt.subplots(4, 8, subplot_kw=dict(xticks=[], yticks=[]))
 for i, axi in enumerate(ax.flat):
 axi.imshow(faces.images[i], cmap='gray')

\1
In[18]: from sklearn.decomposition import RandomizedPCA
 model = RandomizedPCA(100).fit(faces.data)
 plt.plot(np.cumsum(model.explained_variance_ratio_))
 plt.xlabel('n components') # Количество компонент
 plt.ylabel('cumulative variance'); # Интегральная дисперсия

--- СТРАНИЦА 513 ---

\1
In[19]: from sklearn.manifold import Isomap
 model = Isomap(n_components=2)
 proj = model.fit_transform(faces.data)
 proj.shape
Out[19]: (2370, 2)

\1
In[20]: from matplotlib import offsetbox
 def plot_components(data, model, images=None, ax=None,
 thumb_frac=0.05, cmap='gray'):
 ax = ax or plt.gca()
 proj = model.fit_transform(data)
 ax.plot(proj[:, 0], proj[:, 1], '.k')
 if images is not None:

--- СТРАНИЦА 514 ---

\1n_dist_2 = (thumb_frac * max(proj.max(0) –
 proj.min(0))) ** 2
 shown_images = np.array([2 * proj.max(0)])
 for i in range(data.shape[0]):
 dist = np.sum((proj[i] - shown_images) ** 2, 1)
 if np.min(dist) < min_dist_2:
 # Не отображаем слишком близко расположенные точки
 Continue
 shown_images = np.vstack([shown_images, proj[i]])
 imagebox = offsetbox.AnnotationBbox(
 offsetbox.OffsetImage(images[i], cmap=cmap),
 proj[i])
 ax.add_artist(imagebox)

\1
In[21]: fig, ax = plt.subplots(figsize=(10, 10))
 plot_components(faces.data,
 model=Isomap(n_components=2),
 images=faces.images[:, ::2, ::2])
Рис. 5.106. Вложение с помощью Isomap данных о лицах

--- СТРАНИЦА 515 ---
Заглянем глубже: обучение на базе многообразий 515
Результат интересен: первые два измерения Isomap, вероятно, описывают общие 
признаки изображения: низкую или высокую яркость изображения слева направо 
и общее расположение лица снизу вверх. Это дает нам общее представление о не-
которых базовых признаках данных.
Далее можно перейти к классификации этих данных, возможно, с помощью при-
знаков на базе многообразий в качестве входных данных для алгоритма класси -
фикации, аналогично тому, как мы поступили в разделе «Заглянем глубже: метод 
опорных векторов» этой главы.
Пример: визуализация структуры цифр
В качестве еще одного примера использования обучения на базе многообразий 
для визуализации рассмотрим набор MNIST рукописных цифр. Эти данные 
аналогичны цифрам, с которыми мы сталкивались в разделе «Заглянем глубже: 
деревья принятия решений и случайные леса» этой главы, но с намного большей 
детализацией изображений. Скачать их можно с сайта http://mldata.org/ с помощью 
утилиты библиотеки Scikit-Learn:
In[22]: from sklearn.datasets import fetch_mldata
 mnist = fetch_mldata('MNIST original')
 mnist.data.shape
Out[22]: (70000, 784)

\1
In[23]: fig, ax = plt.subplots(6, 8, subplot_kw=dict(xticks=[], yticks=[]))
 for i, axi in enumerate(ax.flat):
 axi.imshow(mnist.data[1250 * i].reshape(28, 28), cmap='gray_r')
Рис. 5.107. Примеры цифр из набора MNIST

--- СТРАНИЦА 516 ---

\1n[24]:
# используем только 1/30 часть данных: 
# вычисления для полного набора данных занимают длительное время!
data = mnist.data[::30]
target = mnist.target[::30]
model = Isomap(n_components=2)
proj = model.fit_transform(data)
plt.scatter(proj[:, 0], proj[:, 1], c=target, cmap=plt.cm.get_cmap('jet', 10))
plt.colorbar(ticks=range(10))
plt.clim(-0.5, 9.5);

\1
In[25]: from sklearn.manifold import Isomap
 # Выбираем для проекции 1/4 цифр "1"
 data = mnist.data[mnist.target == 1][::4]

--- СТРАНИЦА 517 ---
Заглянем глубже: обучение на базе многообразий 517
 fig, ax = plt.subplots(figsize=(10, 10))
 model = Isomap(n_neighbors=5, n_components=2, eigen_solver='dense')
 plot_components(data, model, images=data.reshape((-1, 28, 28)),
 ax=ax, thumb_frac=0.05, cmap='gray_r')
Рис. 5.109. Isomap-вложение только для единиц из набора данных о цифрах
Результат дает нам представление о разнообразии форм, которые может принимать 
цифра 1 в этом наборе данных. Данные располагаются вдоль широкой кривой 
в пространстве проекции, отражающей ориентацию цифры. При перемещении 
вверх по графику мы видим единицы со «шляпками» и/или «подошвами», хотя 
они в этом наборе данных редки. Проекция дает нам возможность обнаружить 
аномальные значения с проблемами в данных (например, части соседних цифр, 
попавших в извлеченные изображения).
Хотя само по себе для задачи классификации цифр это и не особо полезно, но мо -
жет помочь нам получить представление о данных и подсказать, что делать дальше, 
например какой предварительной обработке необходимо подвергнуть данные до 
создания конвейера классификации.

--- СТРАНИЦА 518 ---

\1n и других местах имеется множество алгоритмов класте -
ризации, но, вероятно, наиболее простой для понимания — алгоритм кластеризации 
методом k-средних (k-means clustering), реализованный в классе sklearn.cluster.KMe-
ans. Начнем с обычных импортов:
In[1]: %matplotlib inline
 import matplotlib.pyplot as plt
 import seaborn as sns; sns.set() # для стилизации графиков
 import numpy as np

\1
In[2]: from sklearn.datasets.samples_generator import make_blobs
 X, y_true = make_blobs(n_samples=300, centers=4,
 cluster_std=0.60, random_state=0)
 plt.scatter(X[:, 0], X[:, 1], s=50);
Визуально выделить здесь четыре кластера не представляет труда. Алгоритм 
k -средних делает это автоматически, используя в библ иотеке Scikit-Learn API 
статистических оценок:
In[3]: from sklearn.cluster import KMeans
 kmeans = KMeans(n_clusters=4)
 kmeans.fit(X)
 y_kmeans = kmeans.predict(X)

--- СТРАНИЦА 519 ---

\1
In[4]: plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')
 centers = kmeans.cluster_centers_
 plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);
Рис. 5.111. Центры кластеров метода k-средних с окрашенными в разные цвета кластерами
Хорошая новость состоит в том, что алгоритм k -средних (по крайней мере в этом 
простом случае) задает соответствие точек кластерам очень схоже с тем, как мы 
бы могли сделать это визуально. Но у вас может возникнуть вопрос: как этому 

--- СТРАНИЦА 520 ---

\1n-maximization, EM).
Алгоритм k-средних: максимизация математического 
ожидания
Максимизация математического ожидания (EM) — мощный алгоритм, встречающий -
ся во множестве контекстов науки о данных. Метод k -средних — особенно простое 
и понятное приложение этого алгоритма, и мы рассмотрим его здесь вкратце. Подход 
максимизации математического ожидания состоит из следующей процедуры.

\1n), назван так потому, что включает актуа -
лизацию математического ожидания того, к каким кластерам относятся точки. 
M-шаг, или шаг максимизации (maximization), назван так потому, что включа-
ет максимизацию некоторой целевой функции, описывающей расположения 
центров кластеров. В таком случае максимизация достигается путем простого 
усреднения данных в кластере.
Этому алгоритму посвящена обширная литература, но если коротко, то мож -
но подвести его итоги следующим образом: при обычных обстоятельствах 
каждая итерация шагов E и M всегда будет приводить к улучшению оценки 
показателей кластера. Визуализировать этот алгоритм можно так, как по -
казано на рис. 5.112.
Рис. 5.112. Визуализация EM-алгоритма для метода k-средних

--- СТРАНИЦА 521 ---
Заглянем глубже: кластеризация методом k-средних 521
Для показанных на рис. 5.112 конкретных начальных значений кластеры схо-
дятся всего за три итерации. Интерактивную версию рисунка можно увидеть 
в онлайн-приложении ( https://github.com/jakevdp/PythonDataScienceHandbook).

\1
In[5]: from sklearn.metrics import pairwise_distances_argmin
 def find_clusters(X, n_clusters, rseed=2):
 # 1. Выбираем кластеры случайным образом
 rng = np.random.RandomState(rseed)
 i = rng.permutation(X.shape[0])[:n_clusters]
 centers = X[i]
 while True:
 # 2a. Присваиваем метки в соответствии с ближайшим центром
 labels = pairwise_distances_argmin(X, centers)
 # 2b. Находим новые центры, исходя из средних значений точек
 new_centers = np.array([X[labels == i].mean(0)
 for i in range(n_clusters)])
 # 2c. Проверяем сходимость
 if np.all(centers == new_centers):
 break
 centers = new_centers
 return centers, labels
 centers, labels = find_clusters(X, 4)
 plt.scatter(X[:, 0], X[:, 1], c=labels,
 s=50, cmap='viridis');
Рис. 5 113. Данные, маркированные с помощью метода k-средних

--- СТРАНИЦА 522 ---

\1n[6]: centers, labels = find_clusters(X, 4, rseed=0)
 plt.scatter(X[:, 0], X[:, 1], c=labels,
 s=50, cmap='viridis');
Рис. 5.114. Пример плохой сходимости в методе k-средних
В этом случае EM-метод сошелся к глобально неоптимальной конфигурации, 
поэтому его часто выполняют для нескольких начальных гипотез, что и делает 
по умолчанию библиотека Scikit-Learn (это задается с помощью параметра 
n_init, по умолчанию имеющего значение 10).
 Количество кластеров следует выбирать заранее. Еще одна часто встреча-
ющаяся проблема с методом k -средних заключается в том, что ему необходимо 
сообщить, какое количество кластеров вы ожидаете: он не умеет вычислять 
количество кластеров на основе данных. Например, если предложить алгоритму 
выделить шесть кластеров, он с радостью это сделает и найдет шесть оптималь-
ных кластеров (рис. 5.115):

--- СТРАНИЦА 523 ---
Заглянем глубже: кластеризация методом k-средних 523
In[7]: labels = KMeans(6, random_state=0).fit_predict(X)
 plt.scatter(X[:, 0], X[:, 1], c=labels,
 s=50, cmap='viridis');
Рис. 5.115. Пример неудачного выбора количества кластеров
Осмысленный ли результат получен — сказать трудно. Один из полезных 
в этом случае и интуитивно довольно понятных подходов, который мы не 
станем обсуждать подробно, — силуэтный анализ ( http://scikit-learn.org/stable/
auto_examples/cluster/plot_kmeans_silhouette_analysis.html ).
В качестве альтернативы можно воспользоваться более сложным алго -
ритмом кластеризации с лучшим количественным показателем зависимо -
сти качества аппроксимации от количества кластеров (например, смесь 
Гаус совых распределений, см. раздел «Заглянем глубже: смеси Гауссовых 
распределений» данной главы) или с возможностью выбора приемлемого 
количества кластеров (например, методы DBSCAN, сдвиг среднего или рас -
пространения аффинности (affinity propagation), находящиеся в подмодуле 
sklearn.cluster ).
 Применение метода k-средних ограничивается случаем линейных границ класте-
ров. Базовое допущение модели k -средних (точки должны быть ближе к центру 
их собственного кластера, чем других) означает, что этот алгоритм зачастую 
будет неэффективен в случае сложной геометрии кластеров.

\1

--- СТРАНИЦА 524 ---

\1n[8]: from sklearn.datasets import make_moons
 X, y = make_moons(200, noise=.05, random_state=0)
In[9]: labels = KMeans(2, random_state=0).fit_predict(X)
 plt.scatter(X[:, 0], X[:, 1], c=labels,
 s=50, cmap='viridis');
Рис. 5.116. Неудача метода k-средних в случае нелинейных границ
Ситуация напоминает обсуждавшееся в разделе «Заглянем глубже: метод 
опорных векторов» этой главы, где мы использовали ядерное преобразова -
ние для проецирования данных в пространство более высокой размерности, 
в котором возможно линейное разделение. Можно попробовать воспользо -
ваться той же уловкой, чтобы метод k -средних стал распознавать нелинейные 
границы.
Одна из версий этого ядерного метода k -средних реализована в библиотеке 
Scikit-Learn в оценивателе SpectralClustering. Она использует граф ближай-
ших соседей для вычисления представления данных более высокой размер -
ности, после чего задает соответствие меток с помощью алгоритма k -средних 
(рис. 5.117):
In[10]: from sklearn.cluster import SpectralClustering
 model = SpectralClustering(n_clusters=2,
 affinity='nearest_neighbors',
 assign_labels='kmeans')
 labels = model.fit_predict(X)
 plt.scatter(X[:, 0], X[:, 1], c=labels,
 s=50, cmap='viridis');
Как видим, метод k -средних с помощью этого ядерного преобразования спо -
собен обнаруживать более сложные нелинейные границы между кластерами.

--- СТРАНИЦА 525 ---
Заглянем глубже: кластеризация методом k-средних 525
Рис. 5.117. Нелинейные границы, обнаруженные с помощью SpectralClustering
 Метод k-средних работает довольно медленно в случае большого количества вы-
борок. Алгоритм может работать довольно медленно при росте числа выборок, 
ведь при каждой итерации методу k -средних необходимо обращаться к каждой 
точке в наборе данных. Интересно, можно ли смягчить это требование относи-
тельно использования всех данных при каждой итерации? Например, можно 
применить лишь подмножество данных для корректировки центров кластеров 
на каждом шаге. Эта идея лежит в основе пакетных алгоритмов k -средних, один 
из которых реализован в классе sklearn.cluster.MiniBatchKMeans. Их интер -
фейс не отличается от обычного KMeans. В дальнейшем мы рассмотрим пример 
их использования.
Примеры
При соблюдении некоторой осторожности в плане вышеупомянутых ограничений 
можно успешно использовать метод k -средних во множестве ситуаций. Рассмотрим 
несколько примеров.
Пример 1: применение метода k-средних для рукописных цифр
Для начала рассмотрим применение метода k -средних к тем же простым данным 
по цифрам, которые мы уже видели в разделах «Заглянем глубже: деревья приня-
тия решений и случайные леса» и «Заглянем глубже: метод главных компонент» 
данной главы. Мы попробуем воспользоваться методом k -средних для распознания 
схожих цифр без использования информации об исходных метках. Это напоминает 

--- СТРАНИЦА 526 ---

\1ns. Напомним, что набор данных по цифрам состоит из 1797 выборок 
с 64 признаками, где каждый из признаков представляет собой яркость одного 
пиксела в изображении размером 8 × 8:
In[11]: from sklearn.datasets import load_digits
 digits = load_digits()
 digits.data.shape
Out[11]: (1797, 64)

\1
In[12]: kmeans = KMeans(n_clusters=10, random_state=0)
 clusters = kmeans.fit_predict(digits.data)
 kmeans.cluster_centers_.shape
Out[12]: (10, 64)

\1
In[13]: fig, ax = plt.subplots(2, 5, figsize=(8, 3))
 centers = kmeans.cluster_centers_.reshape(10, 8, 8)
 for axi, center in zip(ax.flat, centers):
 axi.set(xticks=[], yticks=[])
 axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)
Рис. 5.118. Центры кластеров
Как видим, алгоритм KMeans даже без меток способен определить кластеры, чьи 
центры представляют собой легко узнаваемые цифры, возможно, за исключением 
1 и 8.

--- СТРАНИЦА 527 ---

\1
In[14]: from scipy.stats import mode
 labels = np.zeros_like(clusters)
 for i in range(10):
 mask = (clusters == i)
 labels[mask] = mode(digits.target[mask])[0]

\1
In[15]: from sklearn.metrics import accuracy_score
 accuracy_score(digits.target, labels)
Out[15]: 0.79354479688369506

\1
In[16]: from sklearn.metrics import confusion_matrix
 mat = confusion_matrix(digits.target, labels)
 sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,
 xticklabels=digits.target_names,
 yticklabels=digits.target_names)
 plt.xlabel('true label')
 plt.ylabel('predicted label');
Рис. 5.119. Матрица различий для классификатора методом k-средних

--- СТРАНИЦА 528 ---

\1n[17]: from sklearn.manifold import TSNE
 # Проекция данных: выполнение этого шага займет несколько секунд
 tsne = TSNE(n_components=2, init='pca', random_state=0)
 digits_proj = tsne.fit_transform(digits.data)
 # Расчет кластеров
 kmeans = KMeans(n_clusters=10, random_state=0)
 clusters = kmeans.fit_predict(digits_proj)
 # Перестановка меток местами
 labels = np.zeros_like(clusters)
 for i in range(10):
 mask = (clusters == i)
 labels[mask] = mode(digits.target[mask])[0]
 # Оценка точности
 accuracy_score(digits.target, labels)
Out[17]: 0.93356149137451305
Точность, даже без использования меток , составляет почти 94 %. При разумном 
использовании алгоритмы машинного обучения без учителя демонстрируют от -
личные результаты: они могут извлекать информацию из набора данных даже 
тогда, когда сделать это вручную или визуально очень непросто.
Пример 2: использование метода k-средних 
для сжатия цветов
Одно из интересных приложений кластеризации — сжатие цветов в изображениях. 
Например, допустим, что у нас есть изображение с миллионами цветов. Почти во 
всех изображениях большая часть цветов не используется и цвета многих пиксе -
лов изображения будут похожи или даже совпадать.
Например, рассмотрим изображение, показанное на рис. 5.120, взятом из модуля 
datasets библиотеки Scikit-Learn (для работы следующего кода у вас должен быть 
установлен пакет pillow языка Python):

--- СТРАНИЦА 529 ---
Заглянем глубже: кластеризация методом k-средних 529
In[18]: # Обратите внимание: для работы этого кода 
 # должен быть установлен пакет pillow
 from sklearn.datasets import load_sample_image
 china = load_sample_image("china.jpg")
 ax = plt.axes(xticks=[], yticks=[])
 ax.imshow(china);

\1
In[19]: china.shape
Out[19]: (427, 640, 3)
Этот набор пикселов можно рассматривать как облако точек в трехмерном цвето-
вом пространстве. Изменим форму данных на [n_samples × n_features] и масшта-
бируем шкалу цветов так, чтобы они располагались между 0 и 1:
In[20]: data = china / 255.0 # используем шкалу 0...1
 data = data.reshape(427 * 640, 3)
 data.shape
Out[20]: (273280, 3)

\1
In[21]: 
 def plot_pixels(data, title, colors=None, N=10000):
 if colors is None:
 colors = data
 # Выбираем случайное подмножество
 rng = np.random.RandomState(0)

--- СТРАНИЦА 530 ---

\1ng.permutation(data.shape[0])[:N]
 colors = colors[i]
 R, G, B = data[i].T 
 fig, ax = plt.subplots(1, 2, figsize=(16, 6))
 ax[0].scatter(R, G, color=colors, marker='.')
 ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))
 ax[1].scatter(R, B, color=colors, marker='.')
 ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))
 fig.suptitle(title, size=20);
In[22]: plot_pixels(data, title='Input color space: 16 million possible
 colors') # Исходное цветовое пространство: 16 миллионов 
 # возможных цветов

\1
In[23]: from sklearn.cluster import MiniBatchKMeans
 kmeans = MiniBatchKMeans(16)
 kmeans.fit(data)
 new_colors = kmeans.cluster_centers_[kmeans.predict(data)]
 plot_pixels(data, colors=new_colors,
 title="Reduced color space: 16 colors") 
 # Редуцированное цветовое пространство: 16 цветов

--- СТРАНИЦА 531 ---
Заглянем глубже: кластеризация методом k-средних 531
Рис. 5.122. Шестнадцать кластеров в цветовом пространстве RGB
В результате исходные пикселы перекрашиваются в другие цвета: каждый пиксел 
получает цвет ближайшего центра кластера. Рисуя эти новые цвета в пространстве 
изображения вместо пространства пикселов, видим эффект от перекрашивания 
(рис. 5.123).
Рис. 5.123. Полноцветное изображение (слева) по сравнению с 16-цветным (справа)
In[24]:
china_recolored = new_colors.reshape(china.shape)
fig, ax = plt.subplots(1, 2, figsize=(16, 6),
 subplot_kw=dict(xticks=[], yticks=[]))
fig.subplots_adjust(wspace=0.05)
ax[0].imshow(china)
ax[0].set_title('Original Image', size=16) # Первоначальное изображение
ax[1].imshow(china_recolored)
ax[1].set_title('16-color Image', size=16); # 16-цветное изображение

--- СТРАНИЦА 532 ---

\1n[1]: %matplotlib inline
 import matplotlib.pyplot as plt
 import seaborn as sns; sns.set()
 import numpy as np

\1
In[2]: # Генерируем данные
 from sklearn.datasets.samples_generator import make_blobs
 X, y_true = make_blobs(n_samples=400, centers=4,
 cluster_std=0.60, random_state=0)
 X = X[:, ::-1] # Транспонируем для удобства оси координат
In[3]: # Выводим данные на график с полученными методом k-средних метками
 from sklearn.cluster import KMeans
 kmeans = KMeans(4, random_state=0)
 labels = kmeans.fit(X).predict(X)
 plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');

--- СТРАНИЦА 533 ---

\1
In[4]:
from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist
def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):
 labels = kmeans.fit_predict(X)
 # Выводим на рисунок входные данные
 ax = ax or plt.gca()
 ax.axis('equal')
 ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)
 # Выводим на рисунок представление модели k-средних
 centers = kmeans.cluster_centers_
 radii = [cdist(X[labels == i], [center]).max()

--- СТРАНИЦА 534 ---

\1nter in enumerate(centers)]
 for c, r in zip(centers, radii):
 ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, 
 zorder=1))
In[5]: kmeans = KMeans(n_clusters=4, random_state=0)
 plot_kmeans(kmeans, X)
Рис. 5.125. Круглые кластеры, подразумеваемые моделью k-средних
Немаловажный нюанс, касающийся метода k -средних, состоит в том, что эти модели 
кластеров обязательно должны иметь форму окружностей: метод k -средних не умеет 
работать с овальными или эллипсовидными кластерами. Так, например, если несколь-
ко преобразовать те же данные, присвоенные метки окажутся перепутаны (рис. 5.126).
Рис. 5.126. Неудовлетворительная работа метода k-средних в случае кластеров некруглой формы

--- СТРАНИЦА 535 ---
Заглянем глубже: смеси Гауссовых распределений 535
In[6]: rng = np.random.RandomState(13)
 X_stretched = np.dot(X, rng.randn(2, 2))
 kmeans = KMeans(n_clusters=4, random_state=0)
 plot_kmeans(kmeans, X_stretched)
Визуально заметно, что форма этих преобразованных кластеров некруглая, 
а значит, круглые кластеры плохо подойдут для их описания. Тем не менее ме -
тод k -средних недостаточно гибок для учета этого нюанса и пытается втиснуть 
данные в четыре круглых кластера. Это приводит к перепутанным меткам кла -
стеров в местах перекрытия получившихся окружностей, см. нижнюю правую 
часть графика. 
Можно было бы попытаться решить эту проблему путем предварительной об -
работки данных с помощью PCA (см. раздел «Заглянем глубже: метод главных 
компонент» этой главы), но на практике нет никаких гарантий, что подобная 
глобальная операция позволит разместить по окружностям отдельные точки 
данных.
Отсутствие гибкости в вопросе формы кластеров и отсутствие вероятностного 
присвоения меток кластеров — два недостатка метода k -средних, означающих, что 
для многих наборов данных (особенно низкоразмерных) он будет работать не столь 
хорошо, как хотелось бы.
Можно попытаться избавиться от этих недостатков пут ем обобщения модели 
k -средних. Например, можно оценивать степень достоверности присвоения меток 
кластеров, сравнивая расстояния от каждой точки до всех центров кластеров, а не 
сосредотачивая внимание лишь на ближайшем. Можно также разрешить эллип -
совидную форму границ кластеров, а не только круглую, чтобы учесть кластеры 
некруглой формы. Оказывается, что это базовые соста вляющие другой разновид-
ности модели кластеризации — смеси Гауссовых распределений.
Обобщение EM-модели: смеси Гауссовых 
распределений
Смесь Гауссовых распределений (gaussian mixture model, GMM) нацелена на поиск 
многомерных Гауссовых распределений вероятностей, моделирующих наилучшим 
возможным образом любой исходный набор данных. 

\1
In[7]: from sklearn.mixture import GMM
 gmm = GMM(n_components=4).fit(X)
 labels = gmm.predict(X)
 plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');

--- СТРАНИЦА 536 ---

\1n это можно сделать методом predict_proba. Он возвраща-
ет матрицу размера [n_samples, n_clusters], содержащую оценки вероятностей 
принадлежности точки к конкретному кластеру:
In[8]: probs = gmm.predict_proba(X)
 print(probs[:5].round(3))
[[ 0. 0. 0.475 0.525]
 [ 0. 1. 0. 0. ]
 [ 0. 1. 0. 0. ]
 [ 0. 0. 0. 1. ]
 [ 0. 1. 0. 0. ]]

\1
In[9]: size = 50 * probs.max(1) ** 2 # Возведение в квадрат усиливает 
 # влияние различий
 plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=size);
«Под капотом» смесь Гауссовых распределений очень напоминает метод k -средних: 
она использует подход с максимизацией математического ожидания, который с ка-
чественной точки зрения делает следующее.

\1n[10]:
from matplotlib.patches import Ellipse
def draw_ellipse(position, covariance, ax=None, **kwargs):
 """Рисует эллипс с заданными расположением и ковариацией"""
 ax = ax or plt.gca()
 # Преобразуем ковариацию к главным осям координат
 if covariance.shape == (2, 2):
 U, s, Vt = np.linalg.svd(covariance)
 angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))
 width, height = 2 * np.sqrt(s)
 else:
 angle = 0
 width, height = 2 * np.sqrt(covariance)

--- СТРАНИЦА 538 ---

\1nsig in range(1, 4):
 ax.add_patch(Ellipse(position, nsig * width, nsig * height,
 angle, **kwargs))
def plot_gmm(gmm, X, label=True, ax=None):
 ax = ax or plt.gca()
 labels = gmm.fit(X).predict(X)
 if label:
 ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)
 else:
 ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)
 ax.axis('equal')
 w_factor = 0.2 / gmm.weights_.max()
 for pos, covar, w in zip(gmm.means_, gmm.covars_, gmm.weights_):
 draw_ellipse(pos, covar, alpha=w * w_factor)

\1
In[11]: gmm = GMM(n_components=4, random_state=42)
 plot_gmm(gmm, X)

\1
In[12]: gmm = GMM(n_components=4, covariance_type='full', random_state=42)
 plot_gmm(gmm, X_stretched)

--- СТРАНИЦА 539 ---
Заглянем глубже: смеси Гауссовых распределений 539
Из этого ясно, что метод GMM решает две известные нам основные практические 
проблемы метода k -средних.
Рис. 5.130. Четырехкомпонентный метод GMM 
в случае некруглых кластеров
Выбор типа ковариации
Если вы внимательно посмотрите на предыдущие фрагменты кода, то увидите, 
что в каждом из них были заданы различные значения параметра covariance_
type. Этот гиперпараметр управляет степенями свободы форм кластеров. Оче нь 
важно для любой задачи задавать его значения аккуратно. Значение его по 
умолчанию — covariance_type="diag" , означающее возможность независимого 
задания размеров кластера по всем измерениям с выравниванием полученного 
эллипса по осям координат. 
Несколько более простая и быстро работающая модель — covariance_ty pe="spheri-
cal", ограничивающая форму кластера таким образом, что все измерения равно-
значны между собой. Получающаяся в этом случае кластеризация будет анало -
гична методу k -средних, хотя и не полностью идентична. 
Вариант с covariance_type="full" представляет собой более сложную и требу -
ющую б ольших вычислительных затрат модель (особенно при росте числа из -
мерений), в которой любой из кластеров может быть эллипсом с произвольной 
ориентацией.
Графическое представление этих трех вариантов для одного кластера приведено 
на рис. 5.131.

--- СТРАНИЦА 540 ---

\1ns библиотеки Scikit-Learn (показанные на рис. 5.132), которые мы уже рас -
сматривали в разделе «Заглянем глубже: кластеризация методом k-средних» этой 
главы:
In[13]: from sklearn.datasets import make_moons
 Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)
 plt.scatter(Xmoon[:, 0], Xmoon[:, 1]);

\1
In[14]: gmm2 = GMM(n_components=2, covariance_type='full', random_state=0)
 plot_gmm(gmm2, Xmoon)

\1
In[15]: gmm16 = GMM(n_components=16, covariance_type='full', random_state=0)
 plot_gmm(gmm16, Xmoon, label=False)
Рис. 5.134. Использование большого количества кластеров 
GMM для моделирования распределения точек

--- СТРАНИЦА 542 ---

\1n[16]: Xnew = gmm16.sample(400, random_state=42)
 plt.scatter(Xnew[:, 0], Xnew[:, 1]);
Рис. 5.135. Новые данные, полученные из 16-компонентного GMM
Метод GMM — удобное гибкое средство моделирования произвольного много -
мерного распределения данных.
Сколько компонент необходимо?
Благодаря тому что GMM — порождающая модель, у нас появляется естественная 
возможность определения оптимального количества компонент для заданного 
набора данных. Порождающая модель, по существу, представляет собой распреде -
ление вероятности для набора данных, поэтому можно легко вычислить функцию 
правдоподобия (likelihood function) для лежащих в ее основе данных, используя 
перекрестную проверку во избежание переобучения. Др угой способ введения по -
правки на переобучение — подстройка функции правдоподобия модели с помощью 
некоторого аналитического критерия, например информационного критерия Акаике 
(Akaike information criterion, AIC, см.: https://ru.wikipedia.org/wiki/ Информационный_кри-
терий_Акаи ке) или байесовского информационного критерия (bayesian information 

--- СТРАНИЦА 543 ---
Заглянем глубже: смеси Гауссовых распределений 543
criterion, BIC, см.: https://ru.wikipedia.org/wiki/Информационный_критерий). Оцениватель 
GMM библиотеки Scikit-Learn включает встроенные методы для вычисления этих 
критериев, что сильно упрощает указанный подход.
Посмотрим на критерии AIC и BIC как функции от количества компонент GMM 
для нашего набора данных moon (рис. 5.136):
Рис. 5.136. Визуализация AIC и BIC с целью выбора количества компонент GMM
In[17]: n_components = np.arange(1, 21)
 models = [GMM(n, covariance_type='full', random_state=0).fit(Xmoon)
 for n in n_components]
 plt.plot(n_components, [m.bic(Xmoon) for m in models], label='BIC')
 plt.plot(n_components, [m.aic(Xmoon) for m in models], label='AIC')
 plt.legend(loc='best')
 plt.xlabel('n_components');
Оптимальное количество кластеров — то, которое минимизирует AIC или BIC, 
в зависимости от требуемой аппроксимации. Согласно AIC, наших 16 компонент, 
вероятно, слишком много, лучше взять 8–12. Как это обычно бывает в подобных 
задачах, критерий BIC говорит в пользу более простой модели.
Обратите внимание на важный момент: подобный метод выбора числа компонент 
представляет собой меру успешности работы GMM как оценивателя плотности 
распределения, а не как алгоритма кластеризации. Я советовал бы вам рассматри-
вать GMM в основном как оцениватель плотности и использовать его для класте-
ризации только заведомо простых наборов данных.

--- СТРАНИЦА 544 ---

\1n:
In[18]: from sklearn.datasets import load_digits
 digits = load_digits()
 digits.data.shape
Out[18]: (1797, 64)

\1
In[19]: def plot_digits(data):
 fig, ax = plt.subplots(10, 10, figsize=(8, 8),
 subplot_kw=dict(xticks=[], yticks=[]))
 fig.subplots_adjust(hspace=0.05, wspace=0.05)
 for i, axi in enumerate(ax.flat):
 im = axi.imshow(data[i].reshape(8, 8), cmap='binary')
 im.set_clim(0, 16)
 plot_digits(digits.data)

\1
In[20]: from sklearn.decomposition import PCA
 pca = PCA(0.99, whiten=True)
 data = pca.fit_transform(digits.data)
 data.shape
Out[20]: (1797, 41)

\1
In[21]: n_components = np.arange(50, 210, 10)
 models = [GMM(n, covariance_type='full', random_state=0)

--- СТРАНИЦА 545 ---
Заглянем глубже: смеси Гауссовых распределений 545
 for n in n_components]
 aics = [model.fit(data).aic(data) for model in models]
 plt.plot(n_components, aics);
Рис. 5.137. Исходные рукописные цифры
Рис. 5.138. Кривая AIC для выбора подходящего количества компонент GMM

--- СТРАНИЦА 546 ---

\1n[22]: gmm = GMM(110, covariance_type='full', random_state=0)
 gmm.fit(data)
 print(gmm.converged_)
True

\1
In[23]: data_new = gmm.sample(100, random_state=0)
 data_new.shape
Out[23]: (100, 41)

\1
In[24]: digits_new = pca.inverse_transform(data_new)
 plot_digits(digits_new)

\1
In[1]: %matplotlib inline
 import matplotlib.pyplot as plt
 import seaborn as sns; sns.set()
 import numpy as np

\1
In[2]: 
 def make_data(N, f=0.3, rseed=1):
 rand = np.random.RandomState(rseed)
 x = rand.randn(N)
 x[int(f * N):] += 5

--- СТРАНИЦА 548 ---

\1n x 
 x = make_data(1000)
Такую обычную гистограмму на основе числа точек можно создать с помощью 
функции plt.hist(). Задав параметр normed гистограммы, мы получим норма -
лизованную гистограмму, в которой высота интервалов отражает не число точек, 
а плотность вероятности (рис. 5.140):
In[3]: hist = plt.hist(x, bins=30, normed=True)

\1
In[4]: density, bins, patches = hist
 widths = bins[1:] - bins[:-1]
 (density * widths).sum()
Out[4]: 1.0

\1
In[5]: x = make_data(20)
 bins = np.linspace(-5, 10, 10)

--- СТРАНИЦА 549 ---
Заглянем глубже: ядерная оценка плотности распределения 549
In[6]: fig, ax = plt.subplots(1, 2, figsize=(12, 4),
 sharex=True, sharey=True,
 subplot_kw={'xlim':(-4, 9),
 'ylim':(-0.02, 0.3)})
 fig.subplots_adjust(wspace=0.05)
 for i, offset in enumerate([0.0, 0.6]):
 ax[i].hist(x, bins=bins + offset, normed=True)
 ax[i].plot(x, np.full_like(x, -0.01), '|k',
 markeredgewidth=1)

\1
In[7]: fig, ax = plt.subplots()
 bins = np.arange(-3, 8)
 ax.plot(x, np.full_like(x, -0.1), '|k',
 markeredgewidth=1)
 for count, edge in zip(*np.histogram(x, bins)):
 for i in range(count):
 ax.add_patch(plt.Rectangle((edge, i), 1, 1,
 alpha=0.5))
 ax.set_xlim(-4, 8)
 ax.set_ylim(-0.2, 8)
Out[7]: (-0.2, 8)

--- СТРАНИЦА 550 ---

\1n[8]: x_d = np.linspace(-4, 8, 2000)
 density = sum((abs(xi - x_d) < 0.5) for xi in x)
 plt.fill_between(x_d, density, alpha=0.5)
 plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)
 plt.axis([-4, 8, -0.2, 8]);

\1
In[9]: from scipy.stats import norm
 x_d = np.linspace(-4, 8, 1000)
 density = sum(norm(xi).pdf(x_d) for xi in x)

--- СТРАНИЦА 551 ---
Заглянем глубже: ядерная оценка плотности распределения 551
 plt.fill_between(x_d, density, alpha=0.5)
 plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)
 plt.axis([-4, 8, -0.2, 5]);
Рис. 5.143. Гистограмма с центрированием блоков по отдельным точкам — пример ядерной 
оценки плотности распределения
Рис. 5.144. Ядерная оценка плотности распределения с Гауссовым ядром
Этот сглаженный график со вкладом Гауссового распределения в соответству ющих 
всем исходным точкам местам обеспечивает намного более точное представление 
о форме распределения данных, причем с намного меньшей дисперсией, то есть 
отличия выборок приводят к намного меньшим его изменениям.

--- СТРАНИЦА 552 ---

\1nel), определяющее форму распределения в каждой точке, и ширина ядра (kernel 
bandwidth), определяющая размер ядра в каждой точке. На практике для ядерной 
оценки плотности распределения существует множество различных ядер: в частности, 
реализация KDE библиотеки Scikit-Learn поддерживает использование одного из 
шести ядер, о которых вы можете прочитать в посвященной оцениванию плотности 
документации библиотеки Scikit-Learn ( http://scikit-learn.org/stable/modules/density.html).
Хотя в языке Python реализовано несколько вариантов ядерной оценки плотности 
(особенно в пакетах SciPy и StatsModels), я предпочитаю использовать вариант из 
Scikit-Learn по причине гибкости и эффективности. Он реализован в оценивателе 
sklearn.neighbors.KernelDensity, умеющем работать с KDE в многомерном про-
странстве с одним из шести ядер и одной из нескольких дюжин метрик. В силу 
того что метод KDE может потребовать значительных вычислительных затрат, 
этот оцениватель использует «под капотом» алгоритм на основе деревьев и умеет 
достигать компромисса между временем вычислений и точностью с помощью 
параметров atol (absolute tolerance, допустимая абсолютная погрешность) и rtol 
(relative tolerance, допустимая относительная погрешность). Определить ширину 
ядра — свободный параметр — можно стандартными инструментами перекрестной 
проверки библиотеки Scikit-Learn.
Рассмотрим простой пример воспроизведения предыдущего графика с помощью 
оценивателя KernelDensity библиотеки Scikit-Learn (рис. 5.145):
In[10]: from sklearn.neighbors import KernelDensity
 # Создание экземпляра модели KDE и ее обучение
 kde = KernelDensity(bandwidth=1.0, kernel='gaussian')
 kde.fit(x[:, None])
 # score_samples возвращает логарифм плотности 
 # распределения вероятности
 logprob = kde.score_samples(x_d[:, None])
 plt.fill_between(x_d, np.exp(logprob), alpha=0.5)
 plt.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)
 plt.ylim(-0.02, 0.22)
Out[10]: (-0.02, 0.22)

--- СТРАНИЦА 553 ---
Заглянем глубже: ядерная оценка плотности распределения 553
Рис. 5.145. Ядерная оценка плотности, вычисленная 
с помощью библиотеки Scikit-Learn
Результат нормализован так, что площадь под кривой равна 1.
Выбор ширины ядра путем перекрестной проверки. Выбор ширины ядра в методе 
KDE исключительно важен для получения удовлетворительной оценки плотно -
сти. Это и есть тот параметр, который при оценке плотности служит для выбора 
компромисса между систематической ошибкой и дисперсией. Слишком маленькая 
ширина ядра приводит к оценке с высокой дисперсией, то есть переобучению, 
при которой наличие или отсутствие одной-единственной точки может серьезно 
повлиять на модель. Слишком же широкое ядро ведет к оценке со значительной 
систематической ошибкой, то есть недообучению, при которой структура данных 
размывается этим широким ядром.
В статистике существует долгая предыстория методов быстрой оценки оптималь-
ной ширины ядра на основе довольно строгих допущений относительно данных: 
если заглянуть в реализации метода KDE в пакетах Sc iPy и StatModels, например, 
можно увидеть основанные на некоторых из этих правил реализации.
В контексте машинного обучения мы уже видели, что выбор подобных гипер -
параметров зачастую производится эмпирически посредством перекрестной 
проверки. Учитывая это, оцениватель KernelDensity из библиотеки Scikit-Learn 
спроектирован в расчете на непосредственное использование его в стандартных 
инструментах Scikit-Learn для поиска по сетке. В данном случае мы восполь -
зуемся классом GridSearchCV , чтобы выбрать оптимальную ширину ядра для 
предыдущего набора данных. Поскольку наш набор данных очень невелик, 
мы будем использовать перекрестную проверку по отдельным объектам, при 

--- СТРАНИЦА 554 ---

\1n[11]: from sklearn.grid_search import GridSearchCV
 from sklearn.cross_validation import LeaveOneOut
 bandwidths = 10 ** np.linspace(-1, 1, 100)
 grid = GridSearchCV(KernelDensity(kernel='gaussian'),
 {'bandwidth': bandwidths},
 cv=LeaveOneOut(len(x)))
 grid.fit(x[:, None]);

\1
In[12]: grid.best_params_
Out[12]: {'bandwidth': 1.1233240329780276}
Оптимальная ширина ядра оказалась очень близка к той, которую мы использо-
вали выше в примере, где ширина была равно 1.0 (это ширина ядра по умолчанию 
объекта scipy.stats.norm).
Пример: KDE на сфере
Вероятно, чаще всего KDE используется для визуального представления рас -
пределений точек. Например, KDE встроен в библиотеку визуализации Seaborn 
(которую мы обсуждали в разделе «Визуализация с пом ощью пакета Seaborn» 
главы 4) и применяется там автоматически для визуализации точек в одномерном 
и двумерном пространствах.
В этом разделе мы рассмотрим несколько более сложный сценарий использования 
KDE для визуализации распределений. Воспользуемся следующими географиче-
скими данными, которые можно загрузить с помощью библиотеки Scikit-Learn: 
географическое распределение зафиксированных наблюдений особей двух юж -
ноамериканских млекопитающих — Bradypus variegatus (бурогорлый ленивец) 
и Microryzomys minutus (малая лесная рисовая крыса).
Извлечем данные с помощью библиотеки Scikit-Learn следующим образом 1 :
In[13]: from sklearn.datasets import fetch_species_distributions

\1n, но 
соответствующие изменения планируются к внесению в самое ближайшее время 
и должны быть доступны на момент выхода данной книги.

--- СТРАНИЦА 555 ---
Заглянем глубже: ядерная оценка плотности распределения 555
 data = fetch_species_distributions() 
 # Получаем матрицы/массивы идентификаторов и местоположений животных
 latlon = np.vstack([data.train['dd lat'],
 data.train['dd long']]).T
 species = np.array([d.decode('ascii').startswith('micro')
 for d in data.train['species']], dtype='int')

\1
In[14]: from mpl_toolkits.basemap import Basemap
 from sklearn.datasets.species_distributions import construct_grids
 xgrid, ygrid = construct_grids(data)
 # Рисуем береговые линии с помощью Basemap
 m = Basemap(projection='cyl', resolution='c',
 llcrnrlat=ygrid.min(), urcrnrlat=ygrid.max(),
 llcrnrlon=xgrid.min(), urcrnrlon=xgrid.max())
 m.drawmapboundary(fill_color='#DDEEFF')
 m.fillcontinents(color='#FFEEDD')
 m.drawcoastlines(color='gray', zorder=2)
 m.drawcountries(color='gray', zorder=2)
 # Отображаем места, где наблюдались особи
 m.scatter(latlon[:, 1], latlon[:, 0], zorder=3,
 c=species, cmap='rainbow', latlon=True);
Рис. 5.146. Места, где наблюдались особи, в обучающей 
последовательности

--- СТРАНИЦА 556 ---

\1ne, подходящей 
для адекватного отображения расстояний на криволинейной поверхности.

\1
In[15]:
# Настраиваем сетку данных для контурного графика
X, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])
land_reference = data.coverages[6][::5, ::5]
land_mask = (land_reference > -9999).ravel()
xy = np.vstack([Y.ravel(), X.ravel()]).T xy = np.radians(xy[land_mask])
# Создаем два графика друг возле друга
fig, ax = plt.subplots(1, 2)
fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)
species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']
cmaps = ['Purples', 'Reds']
for i, axi in enumerate(ax):
 axi.set_title(species_names[i])
 # Рисуем береговые линии с помощью Basemap
 m = Basemap(projection='cyl', llcrnrlat=Y.min(),
 urcrnrlat=Y.max(), llcrnrlon=X.min(),
 urcrnrlon=X.max(), resolution='c', ax=axi)
 m.drawmapboundary(fill_color='#DDEEFF')
 m.drawcoastlines()
 m.drawcountries()
 # Формируем сферическую ядерную оценку плотности распределения
 kde = KernelDensity(bandwidth=0.03, metric='haversine')
 kde.fit(np.radians(latlon[species == i]))
 # Выполняем расчеты только на поверхности Земли:
 # -9999 соответствует океану
 Z = np.full(land_mask.shape[0], -9999.0)
 Z[land_mask] = np.exp(kde.score_samples(xy))
 Z = Z.reshape(X.shape)
 # Рисуем изолинии плотности
 levels = np.linspace(0, Z.max(), 25)
 axi.contourf(X, Y, Z, levels=levels, cmap=cmaps[i])

--- СТРАНИЦА 557 ---
Заглянем глубже: ядерная оценка плотности распределения 557
Рис. 5.147. Визуальное представление ядерной оценки 
плотности распределения особей
По сравнению с первоначальным простым контурным графиком, эта визуализация 
обеспечивает намного более понятную картину географического распределения 
наблюдений особей двух данных видов.
Пример: не столь наивный байес
В этом примере мы изучим выполнение байесовской порождающей классификации 
с помощью KDE и рассмотрим создание пользовательского оценивателя на основе 
архитектуры библиотеки Scikit-Learn.
В разделе «Заглянем глубже: наивная байесовская классификация» этой главы 
мы рассмотрели наивную байесовскую классификацию, в которой создали про -
стые порождающие модели для всех классов и построили на их основе быстрый 
классификатор. В случае наивного байесовского классификатора порождающая 
модель — это просто выровненная по осям координат Гауссова функция. Алгоритм 
оценки плотности, например KDE, позволяет убрать «наивную» составляющую 
и произвести ту же самую классификацию с более сложными порождающими 
моделями для каждого из классов. Эта классификация остается байесовской, но 
уже не будет «наивной».
Общая методика порождающей классификации такова.

\1n так, чтобы воспользо-
ваться поиском по сетке и перекрестной проверкой.
Вот код, реализующий этот алгоритм на базе фреймворка Scikit-Learn, мы после-
довательно рассмотрим его блок за блоком:
In[16]: from sklearn.base import BaseEstimator, ClassifierMixin
 class KDEClassifier(BaseEstimator, ClassifierMixin):
 """Байесовская порождающая классификация на основе метода KDE 
 Параметры
 ----------
 bandwidth : float
 Ширина ядра в каждом классе
 kernel : str
 Название ядра, передаваемое функции KernelDensity
 """
 def __init__(self, bandwidth=1.0, kernel='gaussian'):
 self.bandwidth = bandwidth
 self.kernel = kernel
 def fit(self, X, y):
 self.classes_ = np.sort(np.unique(y))
 training_sets = [X[y == yi] for yi in self.classes_]
 self.models_ = [KernelDensity(bandwidth=self.bandwidth,
 kernel=self.kernel).fit(Xi)
 for Xi in training_sets]
 self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])
 for Xi in training_sets]
 return self
 def predict_proba(self, X):
 logprobs = np.array([model.score_samples(X)
 for model in self.models_]).T
 result = np.exp(logprobs + self.logpriors_)
 return result / result.sum(1, keepdims=True)
 def predict(self, X):
 return self.classes_[np.argmax(self.predict_proba(X), 1)]

--- СТРАНИЦА 559 ---

\1
from sklearn.base import BaseEstimator, ClassifierMixin
class KDEClassifier(BaseEstimator, ClassifierMixin):
 """Байесовская порождающая классификация на основе метода KDE 
 Параметры
 ----------
 bandwidth : float
 Ширина ядра в каждом классе
 kernel : str
 Название ядра, передаваемое функции KernelDensity
 """
Каждый оцениватель в библиотеке Scikit-Learn представляет собой класс, насле -
дующий класс BaseEstimator, а также соответствующую примесь (mixin), кото -
рые обеспечивают стандартную функциональность. Например, помимо прочего, 
класс BaseEstimator включает логику, необходимую для клонирования/копиро -
вания оценивателя, чтобы использовать его в процедуре перекрестной проверки, 
а ClassifierMixin определяет используемый по умолчанию метод score(). Мы так-
же задали docstring, который будет собран справочной системой языка Python 
(см. раздел «Справка и документация в оболочке Python» главы 1).

\1
 def __init__(self, bandwidth=1.0, kernel='gaussian'):
 self.bandwidth = bandwidth
 self.kernel = kernel
Это тот код, который фактически выполняется при создании объекта посредством 
конструктора KDEClassifier(). В библиотеке Scikit-Learn важно, чтобы в методе 
инициализации не содержалось никаких команд, кроме присваивания объекту 
self переданных значений по имени. Причина в том, что содержащаяся в классе 
BaseEstimator логика необходима для клонирования и модификации оценивателей 
для перекрестной проверки, поиска по сетке и других целей. Аналогично все аргу-
менты метода __init__ должны быть объявлены явным образом, то есть следует 
избегать аргументов *args или **kwargs, так как они не могут быть корректно об-
работаны внутри процедур перекрестной проверки.

\1
 self.classes_ = np.sort(np.unique(y))
 training_sets = [X[y == yi] for yi in self.classes_]
 self.models_ = [KernelDensity(bandwidth=self.bandwidth,
 kernel=self.kernel).fit(Xi)
 for Xi in training_sets]

--- СТРАНИЦА 560 ---

\1np.log(Xi.shape[0] / X.shape[0])
 for Xi in training_sets]
 return self
В нем мы находим в обучающих данных уникальные классы, обучаем модель 
KernelDensity для всех классов и вычисляем априорные вероятности на основе 
количеств исходных выборок. Наконец, метод fit() должен всегда возвращать 
объект self, чтобы можно было связывать команды в цепочку. Например:
label = model.fit(X, y).predict(X)
Обратите внимание, что все сохраняемые результаты обучения сохраняются с под-
черкиванием в конце названия (например, self.logpriors_). Такие условные обо -
значения используются в библиотеке Scikit-Learn, чтобы можно было быстро про -
смотреть список членов оценивателя (с помощью TAB-автодополнения оболочки 
IPython) и выяснить, какие именно члены были обучены на обучающих данных.

\1
 logprobs = np.vstack([model.score_samples(X)
 for model in self.models_]).T
 result = np.exp(logprobs + self.logpriors_)
 return result / result.sum(1, keepdims=True)
 def predict(self, X):
 return self.classes_[np.argmax(self.predict_proba(X), 1)]
Поскольку мы имеем дело с вероятностным классификат ором, мы сначала ре -
ализовали метод predict_proba() , возвращающий массив формы [n_samples, 
n_classes] вероятностей классов. Элемент [i, j] этого массива представляет 
собой апостериорную вероятность того, что выборка i — член класса j, вычис -
ленная путем умножения функции правдоподобия на априорную вероятность 
и нормализации.

\1
In[17]: from sklearn.datasets import load_digits
 from sklearn.grid_search import GridSearchCV
 digits = load_digits()

--- СТРАНИЦА 561 ---
Заглянем глубже: ядерная оценка плотности распределения 561
 bandwidths = 10 ** np.linspace(0, 2, 100)
 grid = GridSearchCV(KDEClassifier(), {'bandwidth': bandwidths})
 grid.fit(digits.data, digits.target)
 scores = [val.mean_validation_score for val in grid.grid_scores_]

\1
In[18]: plt.semilogx(bandwidths, scores)
 plt.xlabel('bandwidth') # Ширина ядра
 plt.ylabel('accuracy') # Точность
 plt.title('KDE Model Performance') # Эффективность модели KDE
 print(grid.best_params_)
 print('accuracy =', grid.best_score_)
{'bandwidth': 7.0548023107186433}
accuracy = 0.966611018364

\1
In[19]: from sklearn.naive_bayes import GaussianNB
 from sklearn.cross_validation import cross_val_score
 cross_val_score(GaussianNB(), digits.data, digits.target).mean()
Out[19]: 0.81860038035501381
Рис. 5.148. Кривая проверки для основанного на KDE байесовского классификатора

--- СТРАНИЦА 562 ---

\1nelDensity, а не общую оценку точности пред -
сказания.
И наконец, если вы хотите приобрести опыт создания своих собственных оцени-
вателей, можете попробовать создать аналогичный байесовс кий классификатор 
с использованием смесей Гауссовых распределений вместо KDE.
Прикладная задача: конвейер распознавания лиц
В этой главе мы рассмотрели несколько основных идей и алгоритмов машинного 
обучения. Но перейти от теоретических идей к настоящим прикладным задачам 
может оказаться непростым делом. Реальные наборы данных часто бывают за -
шумлены и неоднородны, в них могут отсутствовать признаки, они могут содер -
жать данные в таком виде, который сложно преобразовать в аккуратную матрицу 
[n_samples, n_features]. Вам придется, прежде чем воспользоваться любым из 
изложенных здесь методов, сначала извлечь эти признаки из данных. Не существу-
ет готового единого шаблона, подходящего для всех предметных областей. В этом 
вопросе вам как исследователю данных придется использовать ваши собственные 
интуицию и накопленный опыт.
Одно из очень интересных приложений машинного обучения — анализ изобра -
жений, и мы уже видели несколько примеров его с использованием пиксельных 
признаков для классификации. На практике данные редко оказываются настолько 
однородными, и простых пикселов будет недостаточно. Это привело к появле -
нию обширной литературы, посвященной методам выделения признаков (feature 
extraction) для изображений (см. раздел «Проектирование признаков» данной 
главы).
В этом разделе мы рассмотрим одну из подобных методик выделения признаков, 
гистограмму направленных градиентов (histogram of oriented gradients, HOG, 
см. https://ru.wikipedia.org/wiki/Гистограмма_направленных_градиентов), которая преобра -
зует пикселы изображения в векторное представление, чувствительное к несущим 

--- СТРАНИЦА 563 ---

\1
In[1]: %matplotlib inline
 import matplotlib.pyplot as plt
 import seaborn as sns; sns.set()
 import numpy as np
Признаки в методе HOG
Гистограмма направленных градиентов — простая процедура выделения призна-
ков, разработанная для идентификации пешеходов на изображениях. Метод HOG 
включает следующие этапы.

\1n[2]: from skimage import data, color, feature
 import skimage.data
 image = color.rgb2gray(data.chelsea())
 hog_vec, hog_vis = feature.hog(image, visualise=True)
 fig, ax = plt.subplots(1, 2, figsize=(12, 6),
 subplot_kw=dict(xticks=[], yticks=[]))
 ax[0].imshow(image, cmap='gray')
 ax[0].set_title('input image')

\1nda install scikit-image

--- СТРАНИЦА 564 ---

\1n of HOG features');
Рис. 5.149. Визуализация HOG-признаков, вычисленных для изображения
Метод HOG в действии: простой детектор лиц
На основе этих признаков HOG можно создать простой алгоритм обнаружения 
лиц с помощью любого из оценивателей библиотеки Scikit-Learn. Мы воспользу-
емся линейным методом опорных векторов (см. раздел «Заглянем глубже: метод 
опорных векторов» данной главы). Алгоритм включает следующие шаги.

\1n the Wild (LFW), который 
можно скачать с помощью библиотеки Scikit-Learn:
In[3]: from sklearn.datasets import fetch_lfw_people
 faces = fetch_lfw_people()
 positive_patches = faces.images

--- СТРАНИЦА 565 ---
Прикладная задача: конвейер распознавания лиц 565
 positive_patches.shape
Out[3]: (13233, 62, 47)
Мы получили пригодную для обучения выборку из 13 000 изображений лиц.

\1n:
In[4]: from skimage import data, transform
 imgs_to_use = ['camera', 'text', 'coins', 'moon',
 'page', 'clock', 'immunohistochemistry',
 'chelsea', 'coffee', 'hubble_deep_field']
 images = [color.rgb2gray(getattr(data, name)())
 for name in imgs_to_use]
In[5]:
from sklearn.feature_extraction.image import PatchExtractor
def extract_patches(img, N, scale=1.0,
 patch_size=positive_patches[0].shape):
 extracted_patch_size = \
 tuple((scale * np.array(patch_size)).astype(int))
 extractor = PatchExtractor(patch_size=extracted_patch_size,
 max_patches=N, random_state=0)
 patches = extractor.transform(img[np.newaxis])
 if scale != 1:
 patches = np.array([transform.resize(patch, patch_size)
 for patch in patches])
 return patches
negative_patches = np.vstack([extract_patches(im, 1000, scale)
 for im in images for scale in [0.5, 1.0, 2.0]])
negative_patches.shape
Out[5]: (30000, 62, 47)

\1
In[6]: fig, ax = plt.subplots(6, 10)
 for i, axi in enumerate(ax.flat):
 axi.imshow(negative_patches[500 * i], cmap='gray')
 axi.axis('off')

--- СТРАНИЦА 566 ---

\1n[7]: from itertools import chain
 X_train = np.array([feature.hog(im)
 for im in chain(positive_patches,
 negative_patches)])
 y_train = np.zeros(X_train.shape[0])
 y_train[:positive_patches.shape[0]] = 1
In[8]: X_train.shape
Out[8]: (43233, 1215)
Итак, мы получили 43 000 обучающих выборок в 1215-мерном пространстве 
и наши данные находятся в подходящем для библиотеки Scikit-Learn виде!

\1nearSVC, поскольку он обычно 
лучше масштабируется при росте числа выборок по сравнению с SVC.

--- СТРАНИЦА 567 ---

\1
In[9]: from sklearn.naive_bayes import GaussianNB
 from sklearn.cross_validation import cross_val_score
 cross_val_score(GaussianNB(), X_train, y_train)
Out[9]: array([ 0.94 08785 , 0.87 52342 , 0.939 76823])

\1
In[10]: from sklearn.svm import LinearSVC
 from sklearn.grid_search import GridSearchCV
 grid = GridSearchCV(LinearSVC(), {'C': [1.0, 2.0, 4.0, 8.0]})
 grid.fit(X_train, y_train)
 grid.best_score_
Out[10]: 0.98 66768 44077 44083
In[11]: grid.best_params_
Out[11]: {'C': 4.0}

\1
In[12]: model = grid.best_estimator_
 model.fit(X_train, y_train)
Out[12]: LinearSVC(C=4.0, class_weight=None, dual=True,
 fit_intercept=True, intercept_scaling=1,
 loss='squared_hinge', max_iter=1000,
 multi_class='ovr', penalty='l2',
 random_state=None, tol=0.0001, verbose=0)

\1n[13]: test_image = skimage.data.astronaut()
 test_image = skimage.color.rgb2gray(test_image)
 test_image = skimage.transform.rescale(test_image, 0.5)
 test_image = test_image[:160, 40:180]
 plt.imshow(test_image, cmap='gray')
 plt.axis('off');

--- СТРАНИЦА 568 ---

\1n[14]: def sliding_window(img, patch_size=positive_patches[0].shape,
 istep=2, jstep=2, scale=1.0):
 Ni, Nj = (int(scale * s) for s in patch_size)
 for i in range(0, img.shape[0] - Ni, istep):
 for j in range(0, img.shape[1] - Ni, jstep):
 patch = img[i:i + Ni, j:j + Nj]
 if scale != 1:
 patch = transform.resize(patch, patch_size)
 yield (i, j), patch
 indices, patches = zip(*sliding_window(test_image))
 patches_hog = np.array([feature.hog(patch) for patch in patches])
 patches_hog.shape
Out[14]: (1911, 1215)

\1
In[15]: labels = model.predict(patches_hog)
 labels.sum()
Out[15]: 33.0

\1
In[16]: fig, ax = plt.subplots()
 ax.imshow(test_image, cmap='gray')

--- СТРАНИЦА 569 ---
Прикладная задача: конвейер распознавания лиц 569
 ax.axis('off')
 Ni, Nj = positive_patches[0].shape
 indices = np.array(indices)
 for i, j in indices[labels == 1]:
 ax.add_patch(plt.Rectangle((j, i), Nj, Ni, edgecolor='red',
 alpha=0.3, lw=2,
 facecolor='none'))
Рис. 5.152. Окна, в которых были обнаружены лица
Все обнаруженные фрагменты перекрываются и содержат имеющееся на изо -
бражении лицо! Отличный результат для всего нескольких строк кода на языке 
Python.
Предостережения и дальнейшие усовершенствования
Если посмотреть на предшествующий код и примеры немного внимательнее, можно 
обнаружить, что нужно сделать еще немало, прежде чем можно будет назвать наше 
приложение распознания лиц готовым к промышленной эксплуатации. В нашем 
коде имеется несколько проблемных мест. Кроме того, в него не помешает внести 
несколько усовершенствований.
 Наша обучающая последовательность, особенно в части отрицательных призна-
ков, неполна. Основная проблема заключается в том, что существует множество 
напоминающих лица текстур, не включенных в нашу обучающую последова -
тельность, поэтому нынешняя модель будет склонна выдавать ложноположи -
тельные результаты. Это будет заметно, если попытаться выполнить предыдущий 
алгоритм для полного изображения астронавта: текущая модель приведет к мно-
жеству ложных обнаружений лиц в других областях изображения.

--- СТРАНИЦА 570 ---

\1negative mining . При подходе hard negative mining, берется новый, 
еще не виденный классификатором набор изображений и все фрагменты в нем, 
соответствующие ложноположительным результатам, явным образом добавля-
ются в качестве отрицательных примеров в обучающую последовательность до 
повторного обучения классификатора.
 Текущий конвейер выполняет поиск только при одном з начении масштаба. 
В текущем виде наш алгоритм будет распознавать только те лица, чей размер 
примерно равен 62 × 47 пикселов. Эту проблему можно решить довольно просто 
путем применения скользящих окон различных размеров и изменения размера 
каждого из фрагментов с помощью функции skimage.transform.resize до по-
дачи его на вход модели. На самом деле используемая здесь вспомогательная 
функция sliding_window() уже учитывает этот нюанс.
 Желательно комбинировать перекрывающиеся фрагменты, на которых обнару-
жены лица. В случае готового к промышленной эксплуатации конвейера полу-
чение 30 обнаружений одного и того же лица представляется нежелательным. 
Хотелось бы сократить перекрывающиеся группы обнаруженных лиц до одного. 
Это можно сделать с помощью одного из методов кластеризации без учителя 
(хороший кандидат на эту роль — кластеризация путем сдвига среднего значе-
ния (meanshift clustering)) или посредством процедурного подхода, например 
алгоритма подавления немаксимумов (nonmaximum suppression), часто исполь-
зуемого в сфере машинного зрения.
 Конвейер должен быть более продвинутым. После решение вышеописанных 
проблем неплохо было бы создать более продвинутый конвейер, который бы 
получал на входе обучающие изображения и выдавал предсказания на основе 
скользящих окон. Именно в этом вопросе язык Python как инструмент науки 
о данных демонстрирует все свои возможности: приложив немного труда, мы 
сможем скомпоновать наш предварительный код с качественно спроектиро -
ванным объектно-ориентированным API, обеспечивающим для пользователя 
легкость в использовании. Оставлю это в качестве упражнения читателю.
 Желательно обдумать возможность применения более современных средств 
предварительной обработки, таких как глубокое обучение. Наконец, мне хоте-
лось бы добавить, что HOG и другие процедурные методы выделения призна-
ков для изображений более не считаются современными. Вместо них многие 
современные конвейеры обнаружения объектов используют различные вари -
анты глубоких нейронных сетей. Нейронные сети можно рассматривать как 
оцениватель, определяющий оптимальную стратегию выделения признаков 
на основе самих данных, а не полагающийся на интуицию пользователя. Зна-
комство с методами глубоких нейронных сетей выходит за рамки этого раздела 
концептуально (и вычислительно!), хотя некоторые инструменты с открытым 

--- СТРАНИЦА 571 ---
Дополнительные источники информации по машинному обучению 571
исходным кодом, такие как TensorFlow ( https://www.tensorflow.org/), выпущенный 
корпорацией Google, сделали в последнее время подход глубокого обучения 
значительно более доступным. На момент написания книги глубокое обучение 
в языке Python остается еще довольно «незрелой» концепцией, поэтому я не 
могу рекомендовать вам какие-либо авторитетные источники информации по 
этому вопросу. Тем не менее список литературы в следующем разделе покажет 
вам, с чего можно начать.
Дополнительные источники информации 
по машинному обучению
В этой главе мы кратко рассмотрели машинное обучение в языке Python, в ос -
новном используя инструменты из библиотеки Scikit-Learn. Как бы объемна ни 
была эта глава, в ней все равно невозможно было охватить многие интересные 
и важные алгоритмы, подходы и вопросы. Я хотел бы предложить тем, кто желает 
узнать больше о машинном обучении, некоторые дополнительные источники ин-
формации.
Машинное обучение в языке Python
Если вы хотите узнать больше о машинном обучении, обратите внимание на сле-
дующие источники информации.
 Сайт библиотеки Scikit-Learn. На сайте библиотеки Scikit-Learn содержатся 
поразительные объемы документации и примеров, охватывающие не только 
некоторые из рассмотренных в книге моделей, но и многое другое. Если вам 
необходим краткий обзор наиболее важных и часто используемых алгоритмов 
машинного обучения, этот сайт будет для вас отличной отправной точкой.
 Обучающие видео с таких конференций, как SciPy, PyCon и PyD ata. Библиотека 
Scikit-Learn и другие вопросы машинного обучения — неизменные фавориты 
учебных пособий ежегодных конференций, посвященных языку Python, в част-
ности PyCon, SciPy и PyData. Найти наиболее свежие материалы можно путем 
поиска в Интернете.
 Книга Introduction to Machine Learning with Python ( «Введение в машинное обу чение 
с помощью Python», http://shop.oreilly.com/product/063 69200 30515.do1 ) . Написанная 
Андреасом Мюллером и Сарой Гвидо книга полностью освещает изложенные 
в данной главе вопросы. Если вы хотели бы детально разобраться в важнейших 
вопросах машинного обучения и узнать, как использовать набор инструментов 
библиотеки Scikit-Learn на все 100 %, эта книга — отличный источник информа -
ции, написанный одним из разработчиков команды Scikit-Learn.
1 http://www.williamspublishing.com/Books/978-5-99 08910-8-1.html.

--- СТРАНИЦА 572 ---

\1n Machine Learning ( «Python и машинное обучение», https://www.packt-
pub.com/big-data-and-business-intelligence/python-machine-learning1 ). Книга Себастьяна 
Рашки в меньшей степени акцентирует внимание на самой библиотеке Scikit-
Learn, и в большей — на диапазоне имеющихся в языке Python инструментов 
машинного обучения. В ней приведено очень полезное обсуждение масштабиро-
вания основанных на языке Python подходов машинного обучения на большие 
и сложные наборы данных.
Машинное обучение в целом
Машинное обучение не ограничивается только миром языка Python. Существует 
множество отличных источников информации, с помощью которых вы сможете 
расширить свои познания в этом вопросе. Я отмечу здесь несколько, на мой взгляд, 
наиболее полезных.
 Машинное обучение ( https://www.coursera.org/learn/machine-learning). Этот бесплат-
ный онлайн-курс, преподаваемый Эндрю Энгом из проекта Coursera, представ -
ляет собой исключительно ясно изложенный материал по основам машинного 
обучения с алгоритмической точки зрения. Он предполагает знания математики 
и программирования на уровне старших курсов университета и последовательно 
и подробно обсуждает некоторые из наиболее важных алгоритмов машинного 
обучения. Алгоритмически ранжированные домашние задания позволят вам 
реализовать некоторые из этих моделей самостоятельно.
 Книга Pattern Recognition and Machine Learning ( «Распознавание образов и машин-
ное обучение», https://www.springer.com/us/book/978 03873 10732) . Написанная Кри -
стофером Бишопом, эта классическая книга предназначена для специалистов. 
Она охватывает во всех подробностях рассмотренные в данной главе понятия 
машинного обучения. Если вы хотите продвинуться в вопросе дальше, вам не 
обойтись без этой книги на полке.
 Machine Learning: A Probabilistic Perspective («Машинное обучение: вероятностная 
точка зрения», https://mitpress.mit.edu/books/machine-learning-0). В пособии уровня 
выпускников университета, написанном Кевином Мерфи, исследуются прак -
тически все важные алгоритмы машинного обучения с единой вероятностной 
точки зрения.
Подход в этих источниках информации более формализован, чем представленный 
в данной книге материал, но подлинное понимание основ представленных мето -
дов требует некоторого углубления в математический аппарат. Если вы готовы 
попробовать свои силы и поднять свои знания науки о данных на новый уровень, 
ныряйте в них немедля!
1 http://dmkpress.com/catalog/computer/data/978-5-97060-409-0/.

--- СТРАНИЦА 573 ---
Об авторе
Джейк Вандер Плас — давнишний пользователь и разработчик стека научных ин-
струментов языка Python. В настоящее время он является руководителем группы 
по междисциплинарным исследованиям Вашингтонского университета, занима -
ется собственными астрономическими исследованиями, а также консультирует 
и консультируется с учеными в разнообразных областях науки.

--- СТРАНИЦА 574 ---
 Дж. Вандер Плас
Python	для	сложных	задач:	 
наука	о	данных	и	машинное	обучение
Перевели с английского И. Пальти 
 Заведующая редакцией Ю. Сергиенко
 Руководитель проекта О. Сивченко
 Ведущий редактор Н. Гринчик
 Литературные редакторы О. Андриевич, Н. Хлебина
 Художественный редактор С. Заматевская
 Корректор Т. Курьянович
 Верстка О. Богданович
Изготовлено в России. Изготовитель: ООО «Питер Пресс». 
Место нахождения и фактический адрес: 192102, Россия, город Санкт-Петербург, 
улица Андреевская, дом 3, литер А, помещение 7Н. Тел.: +78127037373.
Дата изготовления: 09.2017. Наименование: книжная продукция. Срок годности: не ограничен.
Налоговая льгота — общероссийский классификатор продукции ОК 034-2014, 58.11.12 — 
Книги печатные профессиональные, технические и научные.
Подписано в печать 23.08.17. Формат 70×100/16. Бумага офсетная. У сл. п. л. 46,440. Тираж 1200. Заказ 0000.
Отпечатано в ОАО «Первая Образцовая типография». Филиал «Чеховский Печатный Двор». 
142300, Московская область, г. Чехов, ул. Полиграфистов, 1. 
Сайт: www.chpk.ru. E-mail: marketing@chpk.ru 
Факс: 8(496) 726-54-10, телефон: (495) 988-63-87