
================================================================================
ФАЙЛ: Анализ данных и машинное обучение, ч2 лекция 4
ИСТОЧНИК: /Users/23108022/Documents/repositories/mephi-data-analysis-part2-2025-fall/bdz-article/lectures/Анализ данных и машинное обучение, ч2 лекция 4.pdf
КОНВЕРТИРОВАНО: pypdf (улучшенная версия)
================================================================================


============================================================
СТРАНИЦА 1
============================================================
Анализ данных и машинное
обучение, ч. 2
Лекция 4. Обучение без учителя. Факторный анализ. Метод главных
компонент. Диагностика и оценка результатов
Киреев В.С.,
к.т.н., доцент
Москва, 2025

============================================================
СТРАНИЦА 2
============================================================
Обучение без учителя. Методы сокращения
размерности. Извлечение признаков
Разложение
матрицы
PCA SVD
Построение графа
соседства
TSNE
\1n
import numpy as np
from scipy.linalg import svd
# Создадим случайную матрицу
np.random.seed(42)

\1np.random.randn(5, 3) * 10
print("Исходная матрица A:\n", A)
# Вычисляем SVD
U, s,
\1nt("\nЛевые сингулярные векторы U:\n", U)
print("\nСингулярные значения s:\n", s)
print("\nПравые сингулярные векторы V^T:\n", Vt)
# Проверка восстановления

\1np.zeros((A.shape[0], A.shape[1]))
np.fill_diagonal(Sigma, s)

\1nt("\nВосстановленная матрица:\n", A_reconstructed)
print("\nОшибка восстановления:", np.linalg.norm(A - A_reconstructed))

============================================================

\1

\1Столбцы

\1Столбцы  - это нагрузки;

\1Столбцы -  это главные оси (также известные как главные направления, также
известные как собственные векторы).

============================================================
СТРАНИЦА 15
============================================================
Двумерные диаграммы, биплоты (biplots).
Отображение исходных переменных. Пример

============================================================
СТРАНИЦА 16
============================================================
Биплот для набора Ирисы Фишера. Python
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# Данные Iris для примера
from sklearn.datasets import load_iris

\1ndardScaler()

\1nsform(X)
# PCA

\1
\1nsform(X_scaled)
# Визуализация объектов
plt.figure(
\1n enumerate(data.feature_names):
plt.arrow(0, 0, pca.components_[0, i]*3, pca.components_[1, i]*3,

\1nents_[0, i]*3.2, pca.components_[1, i]*3.2, feature,

\1
\1n.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

\1ne learning",
"Machine learning is fun",
"Python is great for data science",
"Data science and machine learning are related"
]
# TF-IDF

\1nglish')

\1nsform(documents)
# SVD (TruncatedSVD — для больших матриц)

\1ncatedSVD(
\1
\1nsform(X_tfidf)
# Визуализация
plt.scatter(X_svd[:, 0], X_svd[:, 1])
for i, doc in enumerate(documents):
plt.text(X_svd[i, 0], X_svd[i, 1], f'Doc {i+1}',
\1nent 1')
plt.ylabel('Component 2')
plt.title('LSA: Documents in Latent Space')
plt.grid(True)
plt.show()

============================================================
СТРАНИЦА 24
============================================================
Применение SVD. Обработка естественного
языка

============================================================
СТРАНИЦА 25
============================================================
Связь SVD и PCA
Аспект PCA SVD
Вход Центрированная матрица Любая матрица
Основа Ковариационная матрица Разложение матрицы
Главные компоненты Собственные векторы Столбцы  U  × \Sigma
Нагрузки Собственные векторы Столбцы
\1