
================================================================================
ФАЙЛ: Python для сложных задач_ наука о данных и машинное обучение.  П. Дж. Вандер
ИСТОЧНИК: /Users/23108022/Documents/repositories/mephi-data-analysis-part2-2025-fall/bdz-article/books/Python для сложных задач_ наука о данных и машинное обучение.  П. Дж. Вандер.pdf
КОНВЕРТИРОВАНО: pypdf (улучшенная версия)
================================================================================

ОГЛАВЛЕНИЕ:
• Выполнение вычислений над массивами библиотеки NumPy:
• Основания для использования функций query() и eval():
• Основания для использования функций query() и eval():
============================================================


============================================================

\1018.1
УДК 004.43
П37
	 Плас	Дж.	Вандер
П37 Python для сложных задач: наука о данных и машинное обучение. — СПб.: Пи-
тер, 2018. — 576 с.: ил. — (Серия «Бестселлеры O’Reilly»).
 ISBN 978-5-496-03068-7
Книга «Python для сложных задач: наука о данных и машинное обучение» — это подробное ру -
ководство по самым разным вычислительным и статистическим мето дам, без которых немыслима
любая интенсивная обработка данных, научные исследования и передовые разработки. Читатели,
уже имеющие опыт программирования и желающие эффективно использовать Python в сфере Data
Science, найдут в этой книге ответы на всевозможные вопросы, например: как считать этот формат
данных в скрипт? как преобразовать, очистить эти данные и манипулировать ими? как визуализиро-
вать данные такого типа? как при помощи этих данных разобраться в ситуации, получить ответы на
вопросы, построить статистические модели или реализовать машинное обучение?
16+ (В соответствии с Федеральным законом от 29 декабря 2010 г. № 436-ФЗ.)
 ББК 32.973.2-018.1
 УДК 004.43

\11491912058 англ. Authorized Russian translation of the English edition of Python Data Science
 Handbook, ISBN 9781491912058  © 2017 Jake VanderPlas
ISBN 978-5-496-03068-7 © Перевод на русский язык ООО Издательство «Питер», 2018
 © Издание на русском языке, оформление ООО Издательство «Питер», 2018
 © Серия «Бестселлеры O’Reilly», 2018

============================================================
СТРАНИЦА 5
============================================================
Оглавление
Предисловие  ....................................................................................................... 16
Что такое наука о данных ......................................................................................16
Для кого предназначена эта книга ........................................................................17
Почему Python .......................................................................................................18
Общая структура книги.......................................................................................... 19
Использование примеров кода ..............................................................................19
Вопросы установки ................................................................................................20
Условные обозначения ..........................................................................................21
Глава 1. IPython: за пределами обычного Python  .................................. 22
Командная строка или блокнот? ............................................................................23
Запуск командной оболочки IPython ................................................................23
Запуск блокнота Jupiter ...................................................................................23
Справка и документация в оболочке IPython .........................................................24

\1автодополнения ........................................................................................28
Сочетания горячих клавиш в командной оболочке IPython ....................................30
Навигационные горячие клавиши ....................................................................31
Горячие клавиши ввода текста ........................................................................31
Горячие клавиши для истории команд .............................................................32
Прочие горячие клавиши ................................................................................33

============================================================
СТРАНИЦА 6
============================================================

\1n .............................................................................33
Вставка блоков кода: %paste и %cpaste ..........................................................34
Выполнение внешнего кода: %run ..................................................................35
Длительность выполнения кода: %timeit .........................................................36
Справка по «магическим» функциям: ?, %magic и %lsmagic ...........................36
История ввода и вывода ........................................................................................37
Объекты In и Out оболочки IPython .................................................................37
Быстрый доступ к предыдущим выводам с помощью знака подчеркивания .....38
Подавление вывода .........................................................................................39
Соответствующие «магические» команды .......................................................39
Оболочка IPython и использование системного командного процессора ................40
Краткое введение в использование командного процессора ............................40
Инструкции командного процессора в оболочке IPython..................................42
Передача значений в командный процессор и из него.....................................42
«Магические» команды для командного процессора ..............................................43
Ошибки и отладка .................................................................................................44
Управление исключениями: %xmode ..............................................................44
Отладка: что делать, если чтения трассировок недостаточно .........................47
Профилирование и мониторинг скорости выполнения кода ...................................49
Оценка времени выполнения фрагментов кода: %timeit и %time ....................50
Профилирование сценариев целиком: %prun ..................................................52
Пошаговое профилирование с помощью %lprun ..............................................53
Профилирование использования памяти: %memit и %mprun ..........................54
Дополнительные источники информации об оболочке IPython ..............................56

\1ресурсы ....................................................................................................56
Книги ..............................................................................................................56
Глава 2. Введение в библиотеку NumPy  .................................................... 58
Работа с типами данных в языке Python ................................................................59
Целое число в языке Python — больше, чем просто целое число .....................60
Список в языке Python — больше, чем просто список ......................................62
Массивы с фиксированным типом в языке Python ............................................63

============================================================
СТРАНИЦА 7
============================================================
Оглавление  7
Создание массивов из списков языка Python ...................................................64
Создание массивов с нуля ...............................................................................65
Стандартные типы данных библиотеки NumPy ................................................66
Введение в массивы библиотеки NumPy ................................................................67
Атрибуты массивов библиотеки NumPy ...........................................................68
Индексация массива: доступ к отдельным элементам ......................................69
Срезы массивов: доступ к подмассивам ...........................................................70
Изменение формы массивов ............................................................................74
Слияние и разбиение массивов .......................................................................75
Выполнение вычислений над массивами библиотеки NumPy:
универсальные функции ........................................................................................77
Медлительность циклов ..................................................................................77
Введение в универсальные функции ...............................................................79
Обзор универсальных функций библиотеки NumPy .........................................80
Продвинутые возможности универсальных функций .......................................84
Универсальные функции: дальнейшая информация ........................................86
Агрегирование: минимум, максимум и все, что посередине ...................................86
Суммирование значений из массива ................................................................87
Минимум и максимум ......................................................................................87
Пример: чему равен средний рост президентов США ......................................90
Операции над массивами. Транслирование ...........................................................91
Введение в транслирование ............................................................................92
Правила транслирования ................................................................................94
Транслирование на практике ..........................................................................97
Сравнения, маски и булева логика ........................................................................98
Пример: подсчет количества дождливых дней ................................................98
Операторы сравнения как универсальные функции .......................................100
Работа с булевыми массивами .......................................................................102
Булевы массивы как маски ............................................................................104
«Прихотливая» индексация .................................................................................108
Исследуем возможности «прихотливой» индексации .....................................108

============================================================
СТРАНИЦА 8
============================================================

\1np.sort и np.argsort ........117
Частичные сортировки: секционирование .....................................................118
Пример: K ближайших соседей ......................................................................119
Структурированные данные: структурированные массивы
библиотеки NumPy ..............................................................................................123
Создание структурированных массивов .........................................................125
Более продвинутые типы данных ..................................................................126
Массивы записей: структурированные массивы
с дополнительными возможностями ..............................................................127
Вперед, к Pandas ...........................................................................................128
Глава 3. Манипуляции над данными
с помощью пакета Pandas  ............................................................................ 129
Установка и использование библиотеки Pandas ...................................................130
Знакомство с объектами библиотеки Pandas ........................................................131
Объект Series библиотеки Pandas ..................................................................131
Объект DataFrame библиотеки Pandas ...........................................................135
Объект Index библиотеки Pandas ...................................................................138
Индексация и выборка данных ............................................................................140
Выборка данных из объекта Series ................................................................140
Выборка данных из объекта DataFrame .........................................................144
Операции над данными в библиотеке Pandas ...................................................... 149
Универсальные функции: сохранение индекса ..............................................149
Универсальные функции: выравнивание индексов ........................................150
Универсальные функции: выполнение операции
между объектами DataFrame и Series .............................................................153
Обработка отсутствующих данных .......................................................................154
Компромиссы при обозначении отсутствующих данных .................................155

============================================================
СТРАНИЦА 9
============================================================
Оглавление  9
Отсутствующие данные в библиотеке Pandas ................................................155
Операции над пустыми значениями ...............................................................159
Иерархическая индексация ..................................................................................164
Мультииндексированный объект Series.......................................................... 164
Методы создания мультииндексов .................................................................168
Индексация и срезы по мультииндексу ..........................................................171
Перегруппировка мультииндексов .................................................................174
Агрегирование по мультииндексам ................................................................177
Объединение наборов данных: конкатенация и добавление в конец ...................178
Напоминание: конкатенация массивов NumPy ...............................................179
Простая конкатенация с помощью метода pd.concat ......................................180
Объединение наборов данных: слияние и соединение .........................................184
Реляционная алгебра ....................................................................................184
Виды соединений ..........................................................................................185
Задание ключа слияния .................................................................................187
Задание операций над множествами для соединений ....................................191
Пересекающиеся названия столбцов: ключевое слово suffixes ......................192
Пример: данные по штатам США ...................................................................193
Агрегирование и группировка ..............................................................................197
Данные о планетах ........................................................................................198
Простое агрегирование в библиотеке Pandas ................................................198
GroupBy: разбиение, применение, объединение ............................................200
Сводные таблицы ................................................................................................210
Данные для примеров работы со сводными таблицами .................................210
Сводные таблицы «вручную» ........................................................................211
Синтаксис сводных таблиц ............................................................................212
Пример: данные о рождаемости ....................................................................214
Векторизованные операции над строками ........................................................... 219
Знакомство со строковыми операциями библиотеки Pandas .......................... 219
Таблицы методов работы со строками библиотеки Pandas............................. 221
Пример: база данных рецептов .....................................................................226
Работа с временными рядами ..............................................................................230
Дата и время в языке Python .........................................................................231

============================================================
СТРАНИЦА 10
============================================================

\1ndas: индексация по времени .......................235
Структуры данных для временных рядов библиотеки Pandas ........................ 235
Периодичность и смещения дат..................................................................... 238
Где найти дополнительную информацию ......................................................246
Пример: визуализация количества велосипедов в Сиэтле .............................246
Увеличение производительности библиотеки Pandas: eval() и query() ................. 252
Основания для использования функций query() и eval():
составные выражения ...................................................................................254
Использование функции pandas.eval() для эффективных операций ...............255

\1ориентированный интерфейс ........................................................268
Простые линейные графики ................................................................................ 269
Настройка графика: цвета и стили линий ......................................................271
Настройка графика: пределы осей координат ...............................................273
Метки на графиках ........................................................................................276
Простые диаграммы рассеяния ............................................................................278
Построение диаграмм рассеяния с помощью функции plt.plot ........................279
Построение диаграмм рассеяния с помощью функции plt.scatter ...................281

============================================================
СТРАНИЦА 11
============================================================
Оглавление  1 1
Сравнение функций plot и scatter: примечание относительно
производительности ......................................................................................283
Визуализация погрешностей ................................................................................283
Простые планки погрешностей ......................................................................283
Непрерывные погрешности ...........................................................................285
Графики плотности и контурные графики ............................................................286
Гистограммы, разбиения по интервалам и плотность .......................................... 290
Двумерные гистограммы и разбиения по интервалам ....................................292
Ядерная оценка плотности распределения ....................................................294
Пользовательские настройки легенд на графиках ............................................... 295
Выбор элементов для легенды ......................................................................297
Задание легенды для точек различного размера ...........................................298
Отображение нескольких легенд ...................................................................300
Пользовательские настройки шкал цветов ...........................................................301
Выбор карты цветов ......................................................................................302
Ограничения и расширенные возможности по использованию цветов ...........305
Дискретные шкалы цветов ............................................................................306
Пример: рукописные цифры ..........................................................................306
Множественные субграфики ................................................................................308
plt.axes: создание субграфиков вручную .......................................................309
plt.subplot: простые сетки субграфиков .........................................................310
Функция plt.subplots: создание всей сетки за один раз ..................................311
Функция plt.GridSpec: более сложные конфигурации .....................................313
Текст и поясняющие надписи ..............................................................................314
Пример: влияние выходных дней на рождение детей в США .........................315
Преобразования и координаты текста ...........................................................317
Стрелки и поясняющие надписи ....................................................................319
Пользовательские настройки делений на осях координат ....................................321
Основные и промежуточные деления осей координат ...................................322
Прячем деления и/или метки ........................................................................323
Уменьшение или увеличение количества делений .........................................324
Более экзотические форматы делений ..........................................................325

============================================================
СТРАНИЦА 12
============================================================

\1n .................................................... 357
Seaborn по сравнению с Matplotlib .................................................................358
Анализируем графики библиотеки Seaborn ................................................... 360
Пример: время прохождения марафона ........................................................368
Дополнительные источники информации ............................................................ 377
Источники информации о библиотеке Matplotlib ............................................377
Другие графические библиотеки языка Python ..............................................377

\1Learn .................................................................391

\1Learn .............................................................391
API статистического оценивания библиотеки Scikit-Learn ..............................394

============================================================
СТРАНИЦА 13
============================================================
Оглавление  1 3
Прикладная задача: анализ рукописных цифр ...............................................403
Резюме ..........................................................................................................408
Гиперпараметры и проверка модели ................................................................... 408
Соображения относительно проверки модели ...............................................409
Выбор оптимальной модели ..........................................................................413
Кривые обучения ..........................................................................................420
Проверка на практике: поиск по сетке ..........................................................425
Резюме ..........................................................................................................426
Проектирование признаков .................................................................................427
Категориальные признаки .............................................................................427
Текстовые признаки ......................................................................................429
Признаки для изображений ...........................................................................430
Производные признаки .................................................................................430
Внесение отсутствующих данных ..................................................................433
Конвейеры признаков ...................................................................................434
Заглянем глубже: наивная байесовская классификация ...................................... 435
Байесовская классификация ..........................................................................435
Гауссов наивный байесовский классификатор ...............................................436
Полиномиальный наивный байесовский классификатор ................................439
Когда имеет смысл использовать наивный байесовский классификатор ........442
Заглянем глубже: линейная регрессия ................................................................ 443
Простая линейная регрессия .........................................................................443
Регрессия по комбинации базисных функций ................................................446
Регуляризация............................................................................................... 450
Пример: предсказание велосипедного трафика .............................................453
Заглянем глубже: метод опорных векторов .........................................................459
Основания для использования метода опорных векторов ..............................459
Метод опорных векторов: максимизируем отступ ..........................................461
Пример: распознавание лиц ..........................................................................470
Резюме по методу опорных векторов ............................................................474
Заглянем глубже: деревья решений и случайные леса ........................................ 475
Движущая сила случайных лесов: деревья принятия решений ......................475

============================================================
СТРАНИЦА 14
============================================================

\1nfaces ..............................................................................497

\1модели: смеси Гауссовых распределений ..............................535
GMM как метод оценки плотности распределения .........................................540
Пример: использование метода GMM для генерации новых данных .............. 544
Заглянем глубже: ядерная оценка плотности распределения ..............................547
Обоснование метода KDE: гистограммы ........................................................547
Ядерная оценка плотности распределения на практике ................................552
Пример: KDE на сфере ..................................................................................554
Пример: не столь наивный байес ..................................................................557

============================================================
СТРАНИЦА 15
============================================================
Оглавление  1 5
Прикладная задача: конвейер распознавания лиц ...............................................562
Признаки в методе HOG ................................................................................563
Метод HOG в действии: простой детектор лиц ..............................................564
Предостережения и дальнейшие усовершенствования ..................................569
Дополнительные источники информации по машинному обучению .....................571
Машинное обучение в языке Python ..............................................................571
Машинное обучение в целом .........................................................................572
Об авторе  ........................................................................................................... 573

============================================================
СТРАНИЦА 16
============================================================
Предисловие
Что такое наука о данных
Эта книга посвящена исследованию данных с помощью языка программирования
Python. Сразу же возникает вопрос: что же такое наука о данных  (data science)?

\1
 навыки специалиста по математической статистике, умеющего моделировать
наборы данных и извлекать из них основное;
 навыки специалиста в области компьютерных наук, умеющего проектировать
и использовать алгоритмы для эффективного хранения, обработки и визуали-
зации этих данных;
 экспертные знания предметной области, полученные в ходе традиционного
изучения предмета, — умение как формулировать правильные вопросы, так
и рассматривать ответы на них в соответствующем контексте.

\1научить задавать новые вопросы о вашей пред-
метной области и отвечать на них.
Для кого предназначена эта книга
«Как именно следует изучать Python?» — один из наиболее часто задаваемых мне
вопросов на различных технологических конференциях и встречах. Задают его за-
интересованные в технологиях студенты, разработчики или исследователи, часто
уже со значительным опытом написания кода и использования вычислительного
и цифрового инструментария. Большинству из них не нужен язык программирова-
ния Python в чистом виде, они хотели бы изучать его, чтобы применять в качестве
инструмента для решения задач, требующих вычислений  с обработкой больших
объемов данных.
Эта книга не планировалась в качестве введения в язык Python или в программиро-
вание вообще. Я предполагаю, что читатель знаком с языком Python, включая опи-
сание функций, присваивание переменных, вызов методов объектов, управление
потоком выполнения программы и решение других простейших задач. Она должна
помочь пользователям языка Python научиться применять стек инструментов ис-
следования данных языка Python — такие библиотеки, как IPython, NumPy, Pandas,

============================================================
СТРАНИЦА 18
============================================================

\1n и соответствующие инструменты, — для эффективного
хранения, манипуляции и понимания данных.
Почему Python
За последние несколько десятилетий язык программирования Python превратился
в первоклассный инструмент для научных вычислений, включая анализ и визуализа-
цию больших наборов данных. Это может удивить давних поклонников Python: сам
по себе этот язык не был создан в расчете на анализ данных или научные вычисления.
Язык программирования Python пригоден для науки о данных в основном благода-
ря большой и активно развивающейся экосистеме пакетов, созданных сторонними
разработчиками:
 библиотеки NumPy — для работы с однородными данными в виде массивов;
 библиотеки Pandas — для работы с неоднородными и поименованными дан -
ными;
 SciPy — для общих научных вычислительных задач;
 библиотеки Matplotlib — для визуализаций типографского качества;
 оболочки IPython — для интерактивного выполнения и совместного исполь -
зования кода;
 библиотеки Scikit-Learn — для машинного обучения и множества других ин -
струментов, которые будут упомянуты в дальнейшем.
Если вы ищете руководство по самому языку программирования Python, рекомен-
дую обратить ваше внимание на проект «Краткая экскурсия по языку программи-
рования Python» ( https://github.com/jakevdp/WhirlwindTourOfPython). Он знакомит с важ -
нейшими возможностями языка Python и рассчитан на исследователей данных, уже
знакомых с одним или несколькими языками программирования.
Языки программирования Python 2 и Python 3. В книге используется синтаксис
Python 3, содержащий несовместимые с выпусками 2.x языка программирования
Python расширения. Хотя Python 3.0 был впервые выпущен в 2008 году, он внедрялся
довольно медленно, особенно в научном сообществе и сообществе веб-разработчиков.

\1уки о данных были полностью совместимы как с языком Python 2, так и Python 3,
поэтому данная книга использует более новый синтаксис языка Python 3. Однако
абсолютное большинство фрагментов кода в этой книге будет также работать без
всяких модификаций на языке Python 2. Случаи, когда применяется несовмести-
мый с Python 2 синтаксис, я буду указывать.

============================================================

\1ющему существенную часть инструментария Python для исследования данных.
 IPython и Jupyter (глава 1)  — предоставляют вычислительную среду, в которой
работают многие использующие Python исследователи данных.
 NumPy (глава 2)  — предоставляет объект ndarray для эффективного хранения
и работы с плотными массивами данных в Python.
 Pandas (глава 3) — предоставляет объект DataFrame для эффективного хранения
и работы с поименованными/столбчатыми данными в Python.
 Matplotlib (глава 4) — предоставляет возможности для разнообразной гибкой
визуализации данных в Python.
 Scikit-Learn (глава 5) — предоставляет эффективные реализации на Python
большинства важных и широко известных алгоритмов машинного обучения.
Мир PyData гораздо шире представленных пакетов, и он растет день ото дня.
С учетом этого я использую каждую возможность в книге, чтобы сослаться на
другие интересные работы, проекты и пакеты, расширяющие пределы того,
что можно сделать на языке Python. Тем не менее сегодня эти пять пакетов
являются основополагающими для многого из того, что можно сделать в об -
ласти применения языка программирования Python к исследованию данных.
Я полагаю, что они будут сохранять свое значение и при росте окружающей их
экосистемы.
Использование примеров кода
Дополнительные материалы (примеры кода, упражнения и т. п.) доступны для
скачивания по адресу https://github.com/jakevdp/PythonDataScienceHandbook.

\1помочь вам делать вашу работу. Вы можете использовать лю-
бой пример кода из книги в ваших программах и документации. Обращаться к нам
за разрешением нет необходимости, разве что вы копир уете значительную часть
кода. Например, написание программы, использующей несколько фрагментов кода
из этой книги, не требует отдельного разрешения. Однако для продажи или рас-
пространения компакт-диска с примерами из книг O’Reilly разрешение требуется.

\1ние, автора, издательство и ISBN. Например, «Python для сложных задач. Наука

============================================================
СТРАНИЦА 20
============================================================

\1ns@oreilly.com.
Вопросы установки
Инсталляция Python и набора библиотек, обеспечивающих возможность научных
вычислений, не представляет сложности. В данном разделе будут рассмотрены
особенности, которые следует принимать во внимание при настройке.
Существует множество вариантов установки Python, но я предложил бы вос -
пользоваться дистрибутивом Anaconda, одинаково работающим в операционных
системах Windows, Linux и Mac OS X. Дистрибутив Anaconda существует в двух
вариантах.
 Miniconda  ( http://conda.pydata.org/miniconda.html ) содержит сам интерпретатор
языка программирования Python, а также утилиту командной строки conda,
функционирующую в качестве межплатформенной системы управления
пакетами, ориентированной на работу с пакетами Python и аналогичной по
духу утилитам apt и yum, хорошо знакомым пользователям операционной
системы Linux.
 Anaconda  ( https://www.continuum.io/downloads ) включает интерпретатор Python
и утилиту conda, а также набор предустановленных пакетов, ориентированных
на научные вычисления. Приготовьтесь к тому, что установка займет несколько
гигабайт дискового пространства.
Все включаемые в Anaconda пакеты можно также установить вручную поверх
Miniconda, именно поэтому я рекомендую вам начать с Miniconda.
Для начала скачайте и установите пакет Miniconda (не забудьте выбрать версию
с языком Python 3), после чего установите базовые пакеты, используемые в данной
книге:
[~]$ conda install numpy pandas scikit-learn matplotlib seaborn ipython-notebook

\1ные утилиты из научной экосистемы Python, установка которых сводится к выпол-
нению команды conda install название_пакета. За дополнительной информацией
об утилите conda, включая информацию о создании и использовании сред разра-
ботки conda (что я бы крайне рекомендовал), обратитесь к онлайн-документации
утилиты conda ( http://conda.pydata.org/docs/).

============================================================

\1титься к элементам программы вроде переменных, функций и типов данных.
Им также выделены имена и расширения файлов.
Полужирный моноширинный шрифт
Показывает команды или другой текст, который пользователь должен  ввести
самостоятельно.
Курсивный моноширинный шрифт
Показывает текст, который должен быть заменен значениями, введенными
пользователем, или значениями, определяемыми контекстом.

============================================================
СТРАНИЦА 22
============================================================
1
IPython: за пределами
обычного Python
Меня часто спрашивают, какой из множества вариантов среды разработки для
Python я использую в своей работе. Мой ответ иногда удивляет спрашивающих:
моя излюбленная среда представляет собой оболочку IPython ( http://ipython.org/)
плюс текстовый редактор (в моем случае редактор Emacs или Atom, в зависимости
от настроения). IPython (сокращение от «интерактивный Python» ) был основан
в 2001 году Фернандо Пересом в качестве продвинутого интерпретатора Python
и с тех пор вырос в проект, призванный обеспечить, по словам Переса, «утилиты
для всего жизненного цикла исследовательских расчетов». Если язык Python —
механизм решения нашей задачи в области науки о данных, то оболочку IPython
можно рассматривать как интерактивную панель управления.
Оболочка IPython является полезным интерактивным ин терфейсом для языка
Python и имеет несколько удобных синтаксических дополнений к нему. Боль -
шинство из них мы рассмотрим. Кроме этого, оболочка  IPython тесно связана
с проектом Jupiter ( http://jupyter.org/ ), предоставляющим своеобразный блокнот
(текстовый редактор) для браузера, удобный для разработки, совместной работы
и использования ресурсов, а также для публикации научных результатов. Блок -
нот оболочки IPython, по сути, частный случай более общей структуры блокнота
Jupiter, включающего блокноты для Julia, R и других языков программирования.
Не стоит далеко ходить за примером, показывающим удобства формата блокнота,
им служит страница, которую вы сейчас читаете: вся рукопись данной книги была
составлена из набора блокнотов IPython.
Оболочка IPython позволяет эффективно использовать язык Python для интер-
активных научных вычислений, требующих обработки большого количества
данных. В этой главе мы рассмотрим возможности оболочки IPython, полезные

============================================================
СТРАНИЦА 23
============================================================
Командная строка или блокнот?  2 3
при исследовании данных. Сосредоточим свое внимание на тех предоставляемых
IPython синтаксических возможностях, которые выходят за пределы стандарт -
ных возможностей языка Python. Немного углубимся в «магические» команды,
позволяющие ускорить выполнение стандартных задач при создании и исполь -
зовании предназначенного для исследования данных кода. И наконец, затронем
возможности блокнота, полезные для понимания смысла данных и совместного
использования результатов.

\1
Существует два основных способа использования оболочки IPython: командная
строка IPython и блокнот IPython. Большая часть изложенного в этой главе мате-
риала относится к обоим, а в примерах будет применяться более удобный в кон-
кретном случае вариант. Я буду отмечать немногие разделы, относящиеся только
к одному из них. Прежде чем начать, вкратце расскажу, как запускать командную
оболочку IPython и блокнот IPython.
Запуск командной оболочки IPython

\1сисом: формируемая при этом мышечная память принесет намного больше пользы,
чем простое чтение. Начнем с запуска интерпретатора оболочки IPython путем
ввода команды IPython в командной строке. Если же вы установили один из таких
дистрибутивов, как Anaconda или EPD, вероятно, у вас есть запускающий модуль
для вашей операционной системы (мы обсудим это подробнее в разделе «Справка
и документация в оболочке Python» данной главы).

\1
IPython 4.0.1 – An enhanced Interactive Python.
? | -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
Help | -> Python's own help system.
Object? | -> Details about 'object', use 'object??' for extra details.
In [1]:

\1браузерный графический интерфейс для командной оболоч -
ки IPython и богатый набор основанных на нем возможностей динамической

============================================================
СТРАНИЦА 24
============================================================

\1n: за пределами обычного Python
визуализации . Помимо выполнения операторов Python/IPython, блокнот позволя -
ет пользователю вставлять форматированный текст, статические и динамические
визуализации, математические уравнения, виджеты JavaScript и многое другое.
Более того, эти документы можно сохранять так, что другие люди смогут открывать
и выполнять их в своих системах.
Хотя просмотр и редактирование блокнота IPython осуществляется через окно
браузера, он должен подключаться к запущенному процессу Python для выпол -
нения кода. Для запуска этого процесса (называемого  ядром (kernel)) выполните
следующую команду в командной строке вашей операционной системы:
$ jupyter notebook

\1
$ jupyter notebook
[NotebookApp] Serving notebooks from local directory: /Users/jakevdp/…
[NotebookApp] 0 active kernels
[NotebookApp] The IPython Notebook is running at: http://localhost:8888/
[NotebookApp] Use Control-C to stop this server and shut down all kernels…

\1тически запуститься и перейти по указанному локальному URL; точный адрес
зависит от вашей системы. Если браузер не открывается автоматически, можете
запустить его вручную и перейти по указанному адресу (в данном примере http://
localhost:8888/).
Справка и документация
в оболочке IPython
Если вы не читали остальных разделов в данной главе, прочитайте хотя бы этот.
Обсуждаемые здесь утилиты внесли наибольший (из IPython) вклад в мой еже -
дневный процесс разработки.

\1============================================================
СТРАНИЦА 25
============================================================
Справка и документация в оболочке IPython | 2 5
тивно искать неизвестную пока информацию: посредством ли поиска в Интернете
или с помощью других средств.
Одна из самых полезных возможностей IPython/Jupiter заключается в сокраще-
нии разрыва между пользователями и типом документации и поиска, что должно
помочь им эффективнее выполнять свою работу. Хотя поиск в Интернете все еще
играет важную роль в ответе на сложные вопросы, большое количество инфор -
мации можно найти, используя саму оболочку IPython. Вот несколько примеров
вопросов, на которые IPython может помочь ответить буквально с помощью не -
скольких нажатий клавиш.
 Как вызвать эту функцию? Какие аргументы и параметры есть у нее?
 Как выглядит исходный код этого объекта Python?
 Что имеется в импортированном мной пакете? Какие ат рибуты или методы
есть у этого объекта?
Мы обсудим инструменты IPython для быстрого доступа к этой информации,
а именно символ ? для просмотра документации, символы ?? для просмотра ис -
ходного кода и клавишу Tab для автодополнения.

\1
Язык программирования Python и его экосистема для исследования данных яв -
ляются клиентоориентированными, и в значительной степени это проявляется
в доступе к документации. Каждый объект Python содержит ссылку на ст року,
именуемую docstring  (сокращение от documentation string — «строка документа -
ции»), которая в большинстве случаев будет содержать краткое описание объекта
и способ его использования. В языке Python имеется встроенная функция help(),
позволяющая обращаться к этой информации и выводить результат. Например,
чтобы посмотреть документацию по встроенной функции len, можно сделать
следующее:
In [1]: help(len)
Help on built-in function len in module builtins:
len(…)
    len(object) -> integer
    Return the number of items of a sequence or mapping1.

\1емого текста или в отдельном всплывающем окне.

\1n: за пределами обычного Python

\1очень распространенное
и удобное действие, оболочка IPython предоставляет символ ? для быстрого до -
ступа к документации и другой соответствующей информации:
In [2]: len?
Type: | builtin_function_or_method
String form: <built-in function
\1n builtin
Docstring:
len(object) -> integer
Return the number of items of a sequence or mapping1.

\1
In [3]:
\1n [4]: L.insert?
Type: | builtin_function_or_method
String form: <built-in method insert of list object at 0
\1ng: | L.insert(index, object) – insert object before index2
или даже сами объекты с документацией по их типу:
In [5]: L?
Type: | list
String form: [1, 2, 3]
Length: | 3
Docstring:
list() -> new empty list3
list(iterable) -> new list initialized from iterable's items4
Это будет работать даже для созданных пользователем функций и других объектов!
В следующем фрагменте кода мы опишем маленькую функцию с docstring:
In [6]: def square(a):
….: | """Return the square of a."""
….: | return a ** 2
  ….:
Обратите внимание, что при создании docstring для нашей функции мы про -
сто вставили в первую строку строковый литерал. Поскольку docstring обычно
занимает несколько строк, в соответствии с условными соглашениями мы ис -
пользовали для многострочных docstring нотацию языка Python с тройными
кавычками.

\1ndex.

\1n | 2 7
Теперь воспользуемся знаком ? для нахождения этой docstring:
In [7]: square?
Type: | function
String form: <function square at 0
\1nition:  square(a)
Docstring: | Return the square a.
Быстрый доступ к документации через элементы docstring — одна из причин, по
которым желательно приучить себя добавлять подобную встроенную документа-
цию в создаваемый код!

\1
Поскольку текст на языке Python читается очень легко, чтение исходного кода
интересующего вас объекта может обеспечить более глубокое его понимание.
Оболочка IPython предоставляет сокращенную форму обращения к исходному
коду — двойной знак вопроса ( ??):
In [8]: square??
Type: | function
String form: <function square at 0
\1nition:  square(a)
Source:
def square(a):
    "Return the square of a"
    return a ** 2

\1нуть в особенности внутренней реализации.
Поэкспериментировав с ним немного, вы можете обратить внимание, что иногда
добавление в конце ?? не приводит к отображению никакого исходного кода:
обычно это происходит потому, что объект, о котором идет речь, реализован не
на языке Python, а на C или каком-либо другом транслируемом языке расши -
рений. В подобном случае добавление ?? приводит к такому же результату, что
и добавление ?. Вы столкнетесь с этим в отношении многих встроенных объектов
и типов Python, например для упомянутой выше функции len:
In [9]: len??
Type: | builtin_function_or_method
String form: <built-in function
\1n builtin
Docstring:
len(object) -> integer
Return the number of items of a sequence or mapping1.

\1n: за пределами обычного Python

\1простой способ для быстрого поиска информации
о работе любой функции или модуля языка Python.

\1автодополнения
Другой удобный интерфейс оболочки IPython — использование клавиши Tab для
автодополнения и просмотра содержимого объектов, модулей и пространств имен.
В следующих примерах мы будем применять обозначение <
\1n связано множество различных атрибутов и методов.
Аналогично обсуждавшейся выше функции help в языке Python есть встроенная
функция dir, возвращающая их список, но на практике использовать интерфейс
Tab-автодополнения гораздо удобнее. Чтобы просмотреть список всех доступных
атрибутов объекта, необходимо набрать имя объекта с последующим символом
точки ( .) и нажать клавишу Tab:
In [10]: L.<
\1nd | L.copy | L.extend | L.insert | L.remove | L.sort
L.clear | L.count | L.index | L.pop | L.reverse

\1
In [10]: L.
\1nt
In [10]: L.
\1nt

\1нию строки. Например, следующее будет немедленно заменено на L.count:
In [10]: L.
\1n отсутствует четкое разграничение между открытыми/внеш-
ними и закрытыми/внутренними атрибутами, по соглашениям, для обозначения
подобных методов принято использовать знак подчерки вания. Для ясности эти
закрытые методы, а также специальные методы по умолчанию исключаются из
списка, но можно вывести их список, набрав знак подчеркивания:

============================================================
СТРАНИЦА 29
============================================================
Справка и документация в оболочке IPython | 2 9
In [10]: L.
\1n, отмечен -
ные двойным подчеркиванием в названии (на сленге называемые dunder 1 -
методами).

\1
In [10]: from itertools import
\1nations | compress
combinations_with_replacement  count

\1веденного листинга в зависимости от того, какие сторонние сценарии и модули
являются видимыми данному сеансу Python):
In [10]:
\1n)
Crypto | dis | py_compile
Cython | distutils | pyclbr
… | … | …
difflib | pwd | zmq
In [10]: import
\1\1nderscore — «двойное подчеркивание»
и dunderhead — «тупица», «болван».

============================================================
СТРАНИЦА 30
============================================================

\1n: за пределами обычного Python
слова. На этот случай оболочка IPython позволяет искать соответствие названий
по джокерному символу *.
Например, можно использовать следующий код для вывода списка всех объектов
в пространстве имен, заканчивающихся словом Warning:
In [10]: *Warning?
BytesWarning | RuntimeWarning
DeprecationWarning | SyntaxWarning
FutureWarning | UnicodeWarning
ImportWarning | UserWarning
PendingDeprecationWarning | Warning
ResourceWarning

\1звании слово find. Отыскать его можно следующим образом:
In [10]: str.*find*?
Str.find
str.rfind

\1нии после перерыва к уже знакомому.
Сочетания горячих клавиш в командной
оболочке IPython

\1Cmd+ C и Cmd+ V (или Ctrl+ C и Ctrl+ V),
применяемые для копирования и вставки в различных программах и системах.
Опытные пользователи выбирают популярные текстовые редакторы, такие как
Emacs, Vim и другие, позволяющие выполнять множество операций посредством
замысловатых сочетаний клавиш.
В командной оболочке IPython также имеются сочетания горячих клавиш для
быстрой навигации при наборе команд. Эти сочетания горячих клавиш предо -
ставляются не самой оболочкой IPython, а через ее зависимости от библиотеки
GNU Readline: таким образом определенные сочетания горячих клавиш могут раз -
личаться в зависимости от конфигурации вашей системы. Хотя некоторые из них
работают в блокноте для браузера, данный раздел в основном касается сочетаний
горячих клавиш именно в командной оболочке IPython.

============================================================
СТРАНИЦА 31
============================================================
Сочетания горячих клавиш в командной оболочке IPython  3 1

\1пользовать для быстрого выполнения команд без изменения исходного положения
рук на клавиатуре. Если вы пользователь Emacs или имеете опыт работы с Linux-
подобными командными оболочками, некоторые сочетания горячих клавиш пока-
жутся вам знакомыми. Мы сгруппируем их в несколько категорий: навигационные
горячие клавиши , горячие клавиши ввода текста , горячие клавиши для истории
команд и прочие горячие клавиши.

\1нуться. Эта клавиша удаляет только один символ за раз. В оболочке IPython имеется
несколько сочетаний горячих клавиш для удаления различных частей набираемого
текста. Наиболее полезные из них — команды для удаления сразу целых строк текста
(табл. 1.2). Вы поймете, что привыкли к ним, когда поймаете себя на использовании со-
четания Ctrl+ B и Ctrl+ D вместо клавиши Backspace для удаления предыдущего символа!
Таблица 1.2. Горячие клавиши для ввода текста
Комбинация клавиш Действие
Backspace Удаляет предыдущий символ в строке
Ctrl+D Удаляет следующий символ в строке
Ctrl+K Вырезает текст, начиная от курсора и до
конца строки
Ctrl+U Вырезает текст с начала строки до курсора
Ctrl+Y Вставляет предварительно вырезанный текст
Ctrl+T Меняет местами предыдущие два символа

============================================================
СТРАНИЦА 32
============================================================

\1n: за пределами обычного Python

\1сочетания для навигации по истории команд. Данная история команд
распространяется за пределы текущего сеанса оболочки IPython: полная исто -
рия команд хранится в базе данных SQLite в каталоге с профилем IPython.

\1с помощью стрелок «вверх» ( ↑)
и «вниз» ( ↓) для пошагового перемещения по истории, но есть и другие вари -
анты (табл. 1.3).
Таблица 1.3. Горячие клавиши для истории команд
Комбинация клавиш Действие
Ctrl+P (или стрелка «вверх») Доступ к предыдущей команде в истории
Ctrl+N (или стрелка «вниз») Доступ к следующей команде в истории
Ctrl+R Поиск в обратном направлении по истории
команд
Особенно полезным может оказаться поиск в обратном направлении. Как вы
помните, в предыдущем разделе мы описали функцию square. Выполним поиск
в обратном направлении по нашей истории команд Python в новом окне оболочки
IPython и найдем это описание снова. После нажатия Ctrl+ R в терминале IPython
вы должны увидеть следующее приглашение командной строки:
In [1]:
(reverse-i-search)`':
Если начать вводить символы в этом приглашении, IPython автоматически будет
дополнять их, подставляя недавние команды, соответствующие этим символам,
если такие существуют:
In [1]:
(reverse-i-search)`sqa': square??

\1
In [1]:
(reverse-i-search)`sqa': def square(a):
    """Return the square of a"""
    return a ** 2
Найдя искомую команду, нажмите Enter и поиск завершится. После этого можно
использовать найденную команду и продолжить работу в нашем сеансе:

============================================================
СТРАНИЦА 33
============================================================
«Магические» команды IPython  3 3
In [1]: def square(a):
    """Return the square of a"""
    return a ** 2
In [2]: square(2)
Out[2]: 4

\1дущих категорий, но заслуживающих упоминания (табл. 1.4).
Таблица 1.4. Дополнительные горячие клавиши
Комбинация клавиш Действие
Ctrl+L Очистить экран терминала
Ctrl+C (или стрелка «вниз») Прервать выполнение текущей команды Python
Ctrl+D Выйти из сеанса IPython 1

\1мительным, вскоре у вас появится соответствующая мышечная память и вы будете
жалеть, что эти команды недоступны в других программах.
«Магические» команды IPython

\1пользования и изучения языка Python с помощью оболочки IPython. Здесь же мы
начнем обсуждать некоторые предоставляемые IPython расширения обычного
синтаксиса языка Python. В IPython они известны как «магические» команды
(magic commands) и отличаются указанием перед названием команды символа %.

\1пространенных задач стандартного анализа данных.

\1n: за пределами обычного Python

\1немся к более подробному обсуждению некоторых «магических» команд далее
в данной главе.
Вставка блоков кода: %paste и %cpaste
При работе с интерпретатором IPython одна из распространенных проблем за -
ключается в том, что вставка многострочных блоков кода может привести к неожи-
данным ошибкам, особенно при использовании отступов и меток интерпретатора.

\1
>>> def donothing(x):
… | return x
Код отформатирован так, как он будет отображаться в интерпретаторе языка
Python, и если вы скопируете и вставите его непосредственно в оболочку IPython,
то вам будет возвращена ошибка:
In [2]: >>> def donothing(x):
…: | … | return x
   …:
  File "<ipython-input-20-5
\1ne 2
… | return x
               ^
SyntaxError: invalid syntax

\1волы приглашения к вводу. Но не волнуйтесь: «магиче ская» функция оболочки
IPython %paste разработана специально для корректной обработки данного типа
многострочного ввода:
In [3]: %
\1nothing(x):
… | return x
## -- Конец вставленного текста –

\1
In [4]: donothing(10)
Out[4]: 10

============================================================
СТРАНИЦА 35
============================================================
«Магические» команды IPython  3 5

\1
In [5]: %cpaste
Pasting code; enter '–-' alone on the line to stop or use Ctrl-D.
:>>> def donothing(x):
:… | return x
:--
Эти «магические» команды, подобно другим, предоставляют функциональность,
которая была бы сложна или вообще невозможна при использовании обычного
интерпретатора языка Python.
Выполнение внешнего кода: %run
Начав писать более обширный код, вы, вероятно, будете работать как в оболочке
IPython для интерактивного исследования, так и в текстовом редакторе для сохра -
нения кода, который вам хотелось бы использовать неоднократно. Будет удобно
выполнять этот код не в отдельном окне, а непосредственно в сеансе оболочки
IPython. Для этого можно применять «магическую» функцию %run.

\1
    """square a number"""
    return x ** 2
for N in range(1, 4):
    print(N, "squared is", square(N))
Выполнить это в сеансе IPython можно следующим образом:
In [6]: %run myscript.py
1 squared is 1
2 squared is 4
3 squared is 9
Обратите внимание, что после выполнения этого сценария все описанные в нем
функции становятся доступными для использования в данном сеансе оболочки
IPython:
In [7]: square(5)
Out[7]: 25

============================================================
СТРАНИЦА 36
============================================================

\1n: за пределами обычного Python
Существует несколько параметров для точной настройки метода выполнения кода.
Посмотреть относящуюся к этому документацию можно обычным способом, набрав
команду %run? в интерпретаторе IPython.

\1%timeit, автоматически опре-
деляющая время выполнения следующего за ней однострочного оператора языка
Python. Например, если нам нужно определить производительность спискового
включения:
In [8]: %timeit
\1n ** 2 for n in range(1000)]
1000 loops, best of 3: 325 µs per loop

\1
In [9]: %%timeit
   …:
\1n in range(1000):
…: | L.append(n ** 2)
   …:
1000 loops, best of 3: 373 µs per loop
Сразу же видно, что в данном случае списковое включение происходит примерно
на 10 % быстрее, чем эквивалентная конструкция для цикла for. Мы рассмотрим
%timeit и другие подходы к мониторингу скорости выполнения и профилированию
кода в разделе «Профилирование и мониторинг скорости выполнения кода» этой
главы.
Справка по «магическим» функциям: ?, %magic
и %lsmagic
Подобно обычным функциям языка программирования Python, у «магических»
функций оболочки IPython есть свои инструкции (docstring), и к этой докумен -
тации можно обратиться обычным способом. Например, чтобы просмотреть до -
кументацию по «магической» функции %timeit, просто введите следующее:
In [10]: %timeit?

\1

============================================================
СТРАНИЦА 37
============================================================
История ввода и вывода  3 7
In [11]: %magic

\1
In [12]: %lsmagic
При желании можно описать свою собственную «магическую» функцию. Если вас
это интересует, загляните в приведенные в разделе «Дополнительные источники
информации об оболочке IPython» данной главы.
История ввода и вывода
Командная оболочка IPython позволяет получать доступ к предыдущим командам
с помощью стрелок «вверх» ( ↑) и «вниз» ( ↓) или сочетаний клавиш Ctrl+ P/ Ctrl+ N.
Дополнительно как в командной оболочке, так и в блокноте IPython дает возмож -
ность получать вывод предыдущих команд, а также строковые версии самих этих
команд.
Объекты In и Out оболочки IPython
Полагаю, вы уже хорошо знакомы с приглашениями In[1]:/ Out[1]:, используемы -
ми оболочкой IPython. Они представляют собой не просто изящные украшения,
а подсказывают вам способ обратиться к предыдущим вводам и выводам в текущем
сеансе. Допустим, вы начали сеанс, который выглядит следующим образом:
In [1]: import math
In [2]: math.sin(2)
Out[2]: 0.9092974268256817
In [3]: math.cos(2)
Out[3]: -0.4161468365471424

\1ками In/ Out, но за этим кроется нечто большее: оболочка IPython на самом деле
создает переменные языка Python с именами In и Out, автоматически обновляемые
так, что они отражают историю:
In [4]: print(In)
['', 'import math', 'math.sin(2)', 'math.cos(2)', 'print(In)']
In [5]: Out
Out[5]: {2: 0.9092974268256817, 3: -0.4161468365471424}

============================================================
СТРАНИЦА 38
============================================================

\1n: за пределами обычного Python
Объект In представляет собой список, отслеживающий очередность команд
(первый элемент этого списка — «заглушка», чтобы In[1] ссылался на первую
команду):
In [6]: print(In[1])
import math

\1не список, а словарь, связывающий ввод с выводом (если таковой
есть).
Обратите внимание, что не все операции генерируют вывод: например, операторы
import и print на вывод не влияют. Последнее обстоятельство может показаться
странным, но смысл его становится понятен, если знать, что функция print возвра-
щает None; для краткости, возвращающие None команды не вносят вклада в объект
Out.
Это может оказаться полезным при необходимости применять ранее полученные
результаты. Например, вычислим сумму sin(2) ** 2 и cos(2) ** 2, используя най -
денные ранее значения:
In [8]: Out[2] ** 2 + Out[3] ** 2
Out[8]: 1.0

\1ты, вероятно, не было необходимости, но эта возможность может оказаться очень
полезной, когда после выполнения ресурсоемких вычислений следует применить
их результат повторно!
Быстрый доступ к предыдущим выводам с помощью
знака подчеркивания
В обычной командной оболочке Python имеется лишь одна функция быстрого
доступа к предыдущему выводу: значение переменной _ (одиночный символ
подчеркивания) соответствует предыдущему выводу; это работает и в оболочке
IPython:
In [9]: print(_)
1.0
Но IPython несколько более продвинут в этом смысле: можно использовать
двойной символ подчеркивания для доступа к выводу на шаг ранее и тройной —
для предшествовавшего ему (не считая команд, не генерировавших никакого
вывода):
In [10]: print(__)
-0.4161468365471424

============================================================
СТРАНИЦА 39
============================================================
История ввода и вывода  3 9
In [11]: print(___)
0.9092974268256817
На этом оболочка IPython останавливается: более чем три подчеркивания уже не -
много сложно отсчитывать и на этом этапе проще ссылаться на вывод по номеру
строки.

\1
In [12]: Out[2]
Out[12]: 0.9092974268256817
In [13]: _2
Out[13]: 0.9092974268256817

\1
In [14]: math.sin(2) + math.cos(2);

\1
In [15]: 14 in Out
Out[15]: False

\1
In [16]: %history –n 1-4
   1: import math
   2: math.sin(2)
   3: math.cos(2)
   4: print(In)

\1%rerun (выполняющая повторно какую-либо

============================================================
СТРАНИЦА 40
============================================================

\1n: за пределами обычного Python
часть истории команд) и %save (сохраняющая какую-либо часть истории команд
в файле). Для получения дополнительной информации рекомендую изучить их
с помощью справочной функциональности ?, обсуждавшейся в разделе «Справки
и документация в Python» данной главы.
Оболочка IPython и использование системного
командного процессора
При интерактивной работе со стандартным интерпретатором языка Python вы
столкнетесь с досадным неудобством в виде необходимости переключаться между
несколькими окнами для обращения к инструментам Python и системным утили-
там командной строки. Оболочка IPython исправляет э ту ситуацию, предоставляя
пользователям синтаксис для выполнения инструкций системного командного
процессора непосредственно из терминала IPython. Для этого используется вос-
клицательный знак: все, что находится после ! в строке, будет выполняться не
ядром языка Python, а системной командной строкой.
Далее в этом разделе предполагается, что вы работаете в Unix-подобной операци-
онной системе, например Linux или Mac OS X. Некоторые из следующих примеров
не будут работать в операционной системе Windows, использующей по умолчанию
другой тип командного процессора (хотя после анонса в 2016 году нативн ой ко -
мандной оболочки Bash на Windows в ближайшем будущем это может перестать
быть проблемой!). Если инструкции системного командного процессора вам не
знакомы, рекомендую просмотреть руководство по нему ( http://swcarpentry.github.io/
shell-novice/), составленное фондом Software Carpentry.

\1способ текстового взаимодействия с ком -
пьютером. Начиная с середины 1980-х годов, когда корпорации Microsoft и Apple
представили первые версии их графических операционных систем, большинство
пользователей компьютеров взаимодействуют со своей операционной системой
посредством привычных щелчков кнопкой мыши на меню и движений «перета -
скивания». Но операционные системы существовали задолго до этих графических
интерфейсов пользователя и управлялись в основном посредством последователь-
ностей текстового ввода: в приглашении командной строки пользователь вводил
команду, а компьютер выполнял указанное пользователем действие. Эти первые
системы командной строки были предшественниками командных процессоров

============================================================
СТРАНИЦА 41
============================================================
Оболочка IPython и использование системного командного процессора  4 1
и терминалов, используемых до сих пор наиболее деятельными специалистами по
науке о данных.

\1ной системы Linux/OS X, в котором пользователь просматривает, создает и меняет
каталоги и файлы в своей системе ( osx:~ $ представляет собой приглашение ко
вводу, а все после знака $ — набираемая команда; текст, перед которым указан
символ #, представляет собой просто описание, а не действительно вводимый вами
текст):
osx:~ $ echo "hello world" | # echo аналогично функции print
                                        # языка Python
hello world
osx:~ $ pwd | #
\1notebooks  projects
osx:~ $ cd projects/ | #
\1\1n: за пределами обычного Python
myproject.txt

\1настоящему широкие возможности.
Инструкции командного процессора
в оболочке IPython

\1лочке IPython, просто поставив перед ней символ !. Например, команды ls, pwd
и echo можно выполнить следующим образом:
In [1]: !ls
myproject.txt
In [2]: !pwd
/home/jake/projects/myproject
In [3]: !echo "printing from the shell"
printing from the shell
Передача значений
в командный процессор и из него
Инструкции командного процессора можно не только вы полнять из оболочки
IPython, они могут также взаимодействовать с пространством имен IPython. На-
пример, можно сохранить вывод любой инструкции командного процессора с по-
мощью оператора присваивания:
In [4]:
\1n [5]: print(contents)
['myproject.txt']
In [6]:
\1n [7]: print(directory)
['/Users/jakevdp/notebooks/tmp/myproject']

============================================================

\1циальный определенный в IPython для командного процессора тип возвращаемого
значения:
In [8]: type(directory)
IPython.utils.text.Slist
Этот тип выглядит и работает во многом подобно спискам языка Python, но у него
есть и дополнительная функциональность, в частности методы grep и fields, а так -
же свойства s, n и p, позволяющие выполнять поиск, фильтрацию и отображение
результатов удобным способом. Чтобы узнать об этом больше, воспользуйтесь
встроенными справочными возможностями оболочки IPython.

\1передача переменных Python в ко-
мандный процессор — возможна посредством синтаксиса {varname}:
In [9]:
\1n"
In [10]: !echo {message}
Hello from Python

\1
In [11]: !pwd
/home/jake/projects/myproject
In [12]: !cd ..
In [13]: !pwd
/home/jake/projects/myproject
Причина заключается в том, что инструкции командног о процессора в блокноте
оболочки IPython выполняются во временном командном подпроцессоре. Если вам
нужно поменять рабочий каталог на постоянной основе, можно воспользоваться
«магической» командой %cd:
In [14]: %cd ..
/home/jake/
\1\1n: за пределами обычного Python

\1
In [15]: cd myproject
/home/jake/projects/myproject

\1но настраивать с помощью «магической» функции %automagic.
Кроме %cd, доступны и другие «автомагические» функции: %cat, %cp, %env, %ls,
%man, %mkdir, %more, %mv, %pwd, %rm и %rmdir, каждую из которых можно применять
без знака %, если активизировано поведение automagic. При этом командную стро-
ку оболочки IPython можно использовать практически как обычный командный
процессор:
In [16]: mkdir tmp
In [17]: ls
myproject.txt  tmp/
In [18]: cp myproject.txt tmp/
In [19]: ls tmp
myproject.txt
In [20]: rm –r tmp

\1дит сеанс работы с языком Python, означает резкое снижение числа необходимых
переключений между интерпретатором и командным процессором при написании
кода на языке Python.
Ошибки и отладка
Разработка кода и анализ данных всегда требуют некоторого количества проб
и ошибок, и в оболочке IPython есть инструменты для упрощения этого про -
цесса. В данном разделе будут вкратце рассмотрены некоторые возможности
по управлению оповещением об ошибках Python, а также утилиты для отладки
ошибок в коде.
Управление исключениями: %xmode
Почти всегда при сбое сценария на языке Python генерируется исключение. В слу-
чае столкновения интерпретатора с одним из этих исключений, информацию о его
причине можно найти в трассировке (traceback), к которой можно обратиться из
Python. С помощью «магической» функции %xmode оболочка IPython дает вам

============================================================

\1
In[1]: def func1(a, b):
           return a / b
       def func2(x):

\1n func1(a, b)
In[2]: func2(1)
---------------------------------------------------------------------------
ZeroDivisionError | Traceback (most recent call last)
<ipython-input-2-b2e110f6fc8f^gt;
\1nc2(1)
<ipython-input-1-
\1n func2(x)
5 |
\1n func1(a, b)
<ipython-input-1-
\1n func1(a, b)
      1 def func1(a, b):
----> 2 | return a / b
      3
      4 def func2(x):
5 |
\1nError: division by zero
Вызов функции func2 приводит к ошибке, и чтение выведенной трассы позволяет
нам в точности понять, что произошло. По умолчанию эта трасса включает не -
сколько строк, описывающих контекст каждого из приведших к ошибке шагов.
С помощью «магической» функции %xmode (сокращение от exception mode — режим
отображения исключений) мы можем управлять тем, какая информация будет вы -
ведена.
Функция %xmode принимает на входе один аргумент, режим, для которого есть три
значения: Plain (Простой), Context (По контексту) и Verbose (Расширенный).

\1Context, вывод при котором показан выше. Режим Plain
дает более сжатый вывод и меньше информации:
In[3]: %xmode Plain
Exception reporting mode: Plain
In[4]: func2(1)

============================================================
СТРАНИЦА 46
============================================================

\1n: за пределами обычного Python
---------------------------------------------------------------------------
Traceback (most recent call last):
  File "<ipython-input-4-
\1ne 1,
\1nc2(1)
  File "<ipython-input-1-
\1ne 7, in func2
    return func1(a, b)
  File "<ipython-input-1-
\1ne 2, in func1
    return a / b
ZeroDivisionError: division by zero

\1
In[5]: %xmode Verbose
Exception reporting mode: Verbose
In[6]: func2(1)
---------------------------------------------------------------------------
ZeroDivisionError | Traceback (most recent call last)
<ipython-input-6-
\1
\1nc2(1)
        global
\1nction func2 at 0
\1n-input-1-
\1n func2(
\1n func1(a, b)
        global
\1nction func1 at 0
\1n-input-1-
\1n func1(
\1nc1(a, b):
----> 2 | return a / b

\1nc2(x):
5 |
\1nError: division by
\1n для интерактивной отладки называется pdb
(сокращение от Python Debugger — «отладчик Python»). Этот отладчик предо -
ставляет пользователю возможность выполнять код строка за строкой, чтобы вы-
яснить, что могло стать причиной какой-либо замысловатой ошибки. Расширенная
версия этого отладчика оболочки IPython называется ipdb (сокращение от IPython
Debugger — «отладчик IPython»).

\1документации.
Вероятно, наиболее удобный интерфейс для отладки в IPython — «магическая»
команда %debug. Если ее вызвать после столкновения с исключением, она автомати-
чески откроет интерактивную командную строку откладки в точке возникновения
исключения. Командная строка ipdb позволяет изучать текущее состояние стека,
доступные переменные и даже выполнять команды
\1\1
In[7]: %
\1n-input-1-
\1nc1()
      1 def func1(a, b):
----> 2 | return a / b
      3

\1nt(a)
1

\1nt(b)
0

\1\1

============================================================
СТРАНИЦА 48
============================================================

\1n: за пределами обычного Python
In[8]: %
\1n-input-1-
\1nc1()
      1 def func1(a, b):
----> 2 | return a / b
      3

\1n-input-1-
\1nc2()
5 |
\1n func1(a, b)

\1nt(x)
1

\1n-input-6-
\1nc2(1)

\1
\1n-input-1-
\1nc2()
5 |
\1n func1(a, b)

\1\1
In[9]: %xmode Plain
       %pdb on
       func2(1)
Exception reporting mode: Plain
Automatic pdb calling has been turned ON
Traceback (most recent call last):
  File "<ipython-input-9-569
\1ne 3,
\1nc2(1)
  File "<ipython-input-1-
\1ne 7, in func2
    return func1(a, b)

============================================================
СТРАНИЦА 49
============================================================
Профилирование и мониторинг скорости выполнения кода  4 9
  File "<ipython-input-1-
\1ne 2, in func1
    return a / b
ZeroDivisionError: division by
\1n-input-1-
\1nc1()
      1 def func1(a, b):
----> 2 | return a / b
      3

\1nt(b)
0

\1n -d
и использовать команду next для пошагового интерактивного перемещения по
строкам кода.
Неполный список команд отладки. Для интерактивной отладки доступно намного
больше команд, чем мы перечислили (табл. 1.5).
Таблица 1.5. Наиболее часто используемые команды
Команда Описание
list Отображает текущее место в файле
h(elp) Отображает список команд или справку по конкретной команде
q(uit) Выход из отладчика и программы
c(ontinue) Выход из отладчика, продолжение выполнения программы
n(ext) Переход к следующему шагу программы
<
\1nt) Вывод значений переменных
s(tep) Вход в подпрограмму
r(eturn) Возврат из подпрограммы

\1горитма забота о подобных вещах может оказаться контрпродуктивной. Согласно

============================================================
СТРАНИЦА 50
============================================================

\1n: за пределами обычного Python
знаменитому афоризму Дональда Кнута: «Лучше не держать в голове подобные
“малые” вопросы производительности, скажем, 97 % времени: преждевременная
оптимизация — корень всех зол».

\1поко паться в состоящем из
множества строк процессе и выяснить, где находится узкое место каког о-либо
сложного набора операций. Оболочка IPython предоставляет широкий выбор
функциональности для выполнения подобного мониторинга скорости выпол -
нения кода и его профилирования. Здесь мы обсудим следующие «магические»
команды оболочки IPython:
 %time — длительность выполнения отдельного оператора;
 %timeit — длительность выполнения отдельного оператора при неоднократном
повторе, для большей точности;
 %prun — выполнение кода с использованием профилировщика;
 %lprun — пошаговое выполнение кода с применением профилировщика;
 %memit — оценка использования оперативной памяти для отдельного опера -
тора;
 %mprun — пошаговое выполнение кода с применением профилировщика па -
мяти.
Последние четыре команды не включены в пакет IPython — для их использования
необходимо установить расширения line_profiler и memory_profiler, которые мы
обсудим в следующих разделах.

\1гические” команды IPython» данной главы; команду %%timeit можно использовать
для оценки времени многократного выполнения фрагментов кода:
In[1]: %timeit sum(range(100))
1 00000 loops, best of 3: 1.54 µs per loop

\1

============================================================
СТРАНИЦА 51
============================================================
Профилирование и мониторинг скорости выполнения кода  5 1
In[2]: %%timeit

\1n range(1000):
           for j in range(1000):
               total += i * (-1) ** j
1 loops, best of 3: 407 ms per loop

\1
In[3]: import random

\1ndom.random() for i in range(100000)]
       %timeit L.sort()
100 loops, best of 3: 1.9 ms per loop

\1
In[4]: import random

\1ndom.random() for i in range(100000)]
       print("sorting an unsorted list:")
       %time L.sort()
sorting an unsorted list:
CPU times: user 40.6 ms, sys: 896 µs, total: 41.5 ms
Wall time: 41.5 ms
In[5]: print("sorting an already sorted list:")
       %time L.sort()
sorting an already sorted list:
CPU times: user 8.18 ms, sys: 10 µs, total: 8.19 ms
Wall time: 8.24 ms

\1ния неиспользуемых объектов Python (известного как сбор мусора), которое могло
бы повлиять на оценку времени. Именно поэтому выдаваемое функцией %timeit
время обычно заметно короче выдаваемого функцией %time.

============================================================
СТРАНИЦА 52
============================================================

\1n: за пределами обычного Python

\1
In[6]: %%time

\1n range(1000):
           for j in range(1000):
               total += i * (-1) ** j
CPU times: user 504 ms, sys: 979 µs, total: 505 ms
Wall time: 505 ms

\1лочки IPython (то есть наберите %time? в командной строке IPython).
Профилирование сценариев целиком: %prun
Программы состоят из множества отдельных операторов, и иногда оценка времени
их выполнения в контексте важнее, чем по отдельности. В языке Python имеется
встроенный профилировщик кода (о котором можно прочитать в документации
языка Python), но оболочка IPython предоставляет намного более удобный способ
его использования в виде «магической» функции %prun.

\1
In[7]: def sum_of_lists(N):

\1n range(5):

\1n range(N)]
               total += sum(L)
           return total
Теперь мы можем обратиться к «магической» функции %prun с указанием вызова
функции, чтобы увидеть результаты профилирования:
In[8]: %prun sum_of_lists(10 00000)

\1
14 function calls in 0.714 seconds
   Ordered by: internal time
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)

\1n-input-
                                               19>:4(<
\1n method sum}
1 | 0.036 | 0.036 | 0.699 | 0.699 <ipython-input-
                                               19>:1(sum_of_lists)
1 | 0.014 | 0.014 | 0.714 | 0.714 <
\1n method exec}

\1мывать возможные изменения в алгоритме для улучшения производительности.
Для получения дополнительной информации по «магической» функции %prun,
а также доступным для нее параметрам воспользуйтесь справочной функциональ-
ностью оболочки IPython (то есть наберите %prun? В командной строке IPython).
Пошаговое профилирование с помощью %lprun
Профилирование по функциям с помощью функции %prun довольно удобно, но
иногда больше пользы может принести построчный отчет профилировщика. Такая
функциональность не встроена в язык Python или оболочку IPython, но можно
установить пакет line_profiler, обладающий такой возможностью. Начнем с ис-
пользования утилиты языка Python для работы с пакетами, pip, для установки
пакета line_profiler:
$ pip install line_profiler
Далее можно воспользоваться IPython для загрузки расширения line_profiler
оболочки IPython, предоставляемой в виде части указанного пакета:
In[9]: %load_ext line_profiler
Теперь команда %lprun может выполнить построчное профилирование любой
функции. В нашем случае необходимо указать ей явным образом, какие функции
мы хотели быть профилировать:
In[10]: %lprun –f sum_of_lists sum_of_lists(5000)

\1
Timer unit: 1e-06 s
Total time: 0.009382 s File: <ipython-input-19-
\1nction: sum_of_lists at line 1
Line # | Hits | Time  Per Hit | % Time  Line
\1\1n: за пределами обычного Python
3 | 6 | 8 | 1.3 | 0.1 | for i in range(5):
4 | 5 | 9001 | 1800.2 | 95.9 |
\1n total

\1тельности для желаемого сценария использования.
Для получения дополнительной информации о «магической» функции %lprun,
а также о доступных для нее параметрах воспользуйтесь справочной функци -
ональностью оболочки IPython (то есть наберите %lprun? в командной строке
IPython).
Профилирование использования памяти: %memit
и %mprun

\1количество используемой операциями памяти.
Это количество можно оценить с помощью еще одного расширения оболочки
IPython — memory_profiler. Как и в случае с утилитой line_profiler, мы начнем
с установки расширения с помощью утилиты pip:
$ pip install memory_profiler
Затем можно воспользоваться оболочкой IPython для загрузки эт ого расширения:
In[12]: %load_ext memory_profiler

\1ции: %memit (аналог %timeit для измерения количества памяти) и %mprun (аналог
%lprun для измерения количества памяти). Применять функцию %memit несложно:
In[13]: %memit sum_of_lists(1000000)
peak memory: 100.08 MiB, increment: 61.36 MiB
Мы видим, что данная функция использует около 100 Мбайт памяти.
Для построчного описания применения памяти можно использовать «магическую»
функцию %mprun. К сожалению, она работает только для функций, описанных в от-
дельных модулях, а не в самом блокноте, так что начнем с применения «магиче -
ской» функции %%file для создания простого модуля под названием mprun_demo.py,
содержащего нашу функцию sum_of_lists, с одним дополнением, которое немного
прояснит нам результаты профилирования памяти:

============================================================
СТРАНИЦА 55
============================================================
Профилирование и мониторинг скорости выполнения кода  5 5
In[14]: %%file mprun_demo.py
        def sum_of_lists(N):

\1n range(5):

\1n range(N)]
            total += sum(L)
            del L # Удалить ссылку на L
            return total
Overwriting mprun_demo.py

\1
In[15]: from mprun_demo import sum_of_lists
        %mprun –f sum_of_lists sum_of_lists(1000000)

\1
Filename: ./mprun_demo.py
Line # | Mem usage | Increment | Line
\1n range(N)]
Filename: ./mprun_demo.py
Line # | Mem usage | Increment | Line
\1n range(5):
4 | 71.9 MiB | 25.4 MiB |
\1n range(N)]
5 | 71.9 MiB | 0.0 MiB | total += sum(L)
6 | 46.5 MiB | -25.4 MiB | del L # Удалить ссылку на L
7 | 39.1 MiB | -7.4 MiB | return total
Здесь столбец Increment сообщает нам, насколько каждая строка отражается в об-
щем объеме памяти. Обратите внимание, что при создании и удалении списка L,
помимо фонового использования памяти самим интерпретатором языка Python,
использование памяти увеличивается примерно на 25 Мбайт.
Для получения дополнительной информации о «магических» функциях %memit
и %mprun, а также о доступных для них параметрах воспользуйтесь справочной
функциональностью оболочки IPython (то есть наберите %memit? в командной
строке IPython).

============================================================
СТРАНИЦА 56
============================================================

\1n: за пределами обычного Python
Дополнительные источники информации
об оболочке IPython
В данной главе мы захватили лишь верхушку айсберга по использованию языка
Python для задач науки о данных. Гораздо больше информации доступно как в пе-
чатном виде, так и в Интернете. Здесь мы приведем некоторые дополнительные
ресурсы, которые могут вам пригодиться.

\1ресурсы
 Сайт IPython ( http://ipython.org/). Сайт IPython содержит ссылки на документа-
цию, примеры, руководства и множество других ресурсов.
 Сайт nbviewer ( http://nbviewer.ipython.org/). Этот сайт демонстрирует статические
визуализации всех доступных в Интернете блокнотов оболочки IPython. Глав -
ная его страница показывает несколько примеров блокнотов, которые можно
пролистать, чтобы увидеть, для чего другие разработчики используют язык

\1n ( http://github.com/ipython/ipython/
wiki/A-gallery-of-interesting-ipython-Notebooks/ ) . Этот непрерывно растущий список
блокнотов, поддерживаемый nbviewer, демонстрирует глубину и размах чис -
ленного анализа, возможного с помощью оболочки IPython. Он включает все,
начиная от коротких примеров и руководств и заканчивая полноразмерными
курсами и книгами, составленными в формате блокнотов!
 Видеоруководства. В Интернете вы найдете немало видеоруководств по
оболочке IPython. Особенно рекомендую руководства Фернандо Переса
и Брайана Грейнджера — двух основных разработчиков, создавших и под -
держивающих оболочку IPython и проект Jupiter, — с конференций PyCon,
SciPy and PyData.
Книги
 Python for Data Analysis 1  ( http://bit.ly/python-for-data-analysis).  Эта книга Уэса Мак-
кинли включает главу, описывающую использование оболочки IPython с точки
зрения исследователя данных. Хотя многое из ее материала пересекается с тем,
что мы тут обсуждали, вторая точка зрения никогда не помешает.
 Learning IPython for Interactive Computing and Data Visualization ( «Изучаем обо-
лочку IPython для целей интерактивных вычислений и визуализации данных»,
http://bit.ly/2eLCBB7). Эта книга Цириллы Россан предлагает неплохое введение
в использование оболочки IPython для анализа данных.

\1n и анализ данных. — М.: ДМК-Пресс, 2015.

============================================================
СТРАНИЦА 57
============================================================
Дополнительные источники информации об оболочке IPython | 5 7
 IPython Interactive Computing and Visualization Cookbook ( «Справочник по ин -
терактивным вычислениям и визуализации с помощью языка IP ython», http://
bit.ly/2fCEtNE). Данная книга также написана Цириллой Россан и представляет
собой более длинное и продвинутое руководство по использованию оболочки
IPython для науки о данных. Несмотря на название, она посвящена не только
оболочке IPython, в книге рассмотрены некоторые детали широкого спектра
проблем науки о данных.

\1вочная функциональность оболочки IPython (обсуждавшаяся в разделе «Справка
и документация в оболочке Python» этой главы) может оказаться чрезвычайно
полезной, если ее использовать правильно. При работе с примерами из данной
книги и другими применяйте ее для знакомства со всеми утилитами, которые
предоставляет IPython.

============================================================
СТРАНИЦА 58
============================================================

\1n, находящимися в оперативной памяти. Тематика
очень широка, ведь наборы данных могут поступать из разных источников и быть
различных форматов (наборы документов, изображений, аудиоклипов, наборы
численных измерений и др.). Несмотря на столь очевидную разнородность, все эти
данные удобно рассматривать как массивы числовых значений.

\1как одномерные массивы интенсивности звука в соответствующие мо -
менты времени. Текст можно преобразовать в числовое представление различными
путями, например с двоичными числами, представляющими частоту определенных
слов или пар слов. Вне зависимости от характера данных первым шагом к их анали-
зу является преобразование в числовые массивы (мы обсудим некоторые примеры
этого процесса в разделе «Проектирование признаков» главы 5).

\1цесса исследования данных. Мы изучим специализированные инструменты языка
Python, предназначенные для обработки подобных массивов, — пакет NumPy
и пакет Pandas (см. главу 3).
Библиотека NumPy (сокращение от Numerical Python — «числовой Python») обе-
спечивает эффективный интерфейс для хранения и работы с плотными буферами
данных. Массивы библиотеки NumPy похожи на встроенный тип данных языка
Python list, но обеспечивают гораздо более эффективное хранение и операции
с данными при росте размера массивов. Массивы библиотеки NumPy формиру -
ют ядро практически всей экосистемы утилит для исследования данных Python.
Время, проведенное за изучением способов эффективного использования пакета

============================================================
СТРАНИЦА 59
============================================================
Работа с типами данных в языке Python  5 9
NumPy, не будет потрачено впустую вне зависимости от интересующего вас аспек -
та науки о данных.
Если вы последовали приведенному в предисловии совету и установили стек
утилит Anaconda, то пакет NumPy уже готов к использованию. Если же вы от -
носитесь к числу тех, кто любит все делать своими руками, то перейдите на сайт
NumPy и следуйте имеющимся там указаниям. После этого импортируйте NumPy
и тщательно проверьте его версию:
In[1]: import numpy
       numpy.__version__
Out[1]: '1.11.1'
Для используемых здесь частей этого пакета я рекомендовал бы применять NumPy
версии 1.8 или более поздней. По традиции, большинство людей в мире SciPy/
PyData импортируют пакет NumPy, используя в качестве его псевдонима np:
In[2]: import numpy as np
На протяжении книги мы будем именно так импортировать и применять NumPy.
Напоминание о встроенной документации
Читая данную главу, не забывайте, что оболочка IPython позволяет быстро про -
сматривать содержимое пакетов (посредством автодополнения при нажатии кла -
виши Tab), а также документацию по различным функциям (используя символ ?).
Загляните в раздел «Справка и документация в оболочке Python» главы 1, если вам
нужно освежить в памяти эти возможности.
Например, для отображения всего содержимого пространства имен numpy можете
ввести команду:
In [3]: np.<
\1\1
In [4]: np?

\1ции можно найти на сайте http://www.numpy.org.
Работа с типами данных в языке Python
Чтобы достичь эффективности научных вычислений, ориентированных на работу
с данными, следует знать, как хранятся и обрабатываются данные. В этом разделе
описываются и сравниваются способы обработки массивов данных в языке Python,
а также вносимые в этот процесс библиотекой NumPy усовершенствования. Зна -
ние различий очень важно для понимания большей части материала в книге.

============================================================
СТРАНИЦА 60
============================================================

\1n зачастую привлекает его простота, немаловажной
частью которой является динамическая типизация. В то время как в языках со
статической типизацией, таких как C и Java, необходимо явным о бразом объяв -
лять все переменные, языки с динамической типизацией, например Python, этого
не требуют. Например, в языке C можно описать операцию следующим образом:
/* Код на языке C */
int
\1nt
\1n соответствующую операцию можно описать так:
# Код на языке Python

\1n range(100):
    result += i

\1ной объявлены явным образом, а в Python они определяются динамически. Это
значит, что мы можем присвоить любой переменной данные любого типа:
# Код на языке Python

\1\1
/* Код на языке C */
int
\1n и другие языки с динамической типизацией
удобными и простыми в использовании. Понимать, как это работает, очень важно,
чтобы научиться эффективно анализировать данные с помощью языка Python. Од-
нако такая гибкость при работе с типами указывает на то, что переменные Python
представляют собой нечто большее, чем просто значение, они содержат также
дополнительную информацию о типе значения. Мы рассмотрим это подробнее
в следующих разделах.
Целое число в языке Python — больше, чем просто
целое число
Стандартная реализация языка Python написана на языке C. Это значит, что
каждый объект Python — просто искусно замаскированная структура языка C, со -

============================================================
СТРАНИЦА 61
============================================================
Работа с типами данных в языке Python  6 1
держащая не только значение, но и другую информацию. Например, при описании
целочисленной переменной на языке Python, такой как
\1n 3.4, можно узнать, что описание целочисленного типа (типа long) факти-
чески выглядит следующим образом (после разворачивания макросов языка C):
struct _longobject {
    long ob_refcnt;
    PyTypeObject *ob_type;
    size_t ob_size;
    long ob_digit[1];
};
Отдельное целое число в языке Python 3.4 фактически состоит из четырех частей:
 ob_refcnt — счетчика ссылок, с помощью которого Python незаметно выполняет
выделение и освобождение памяти;
 ob_type — кодирующей тип переменной;
 ob_size — задающей размер следующих элементов данных;
 ob_digit — содержащей фактическое целочисленное значение, к оторое пред -
ставляет переменная языка Python.
Это значит, что существует некоторая избыточность при хранении целого числа
в языке Python по сравнению с целым числом в компилируемых языках, таких
как C (рис. 2.1).
Рис. 2.1. Разница между целыми числами в языках C и Python
PyObject_HEAD на этом рисунке — часть структуры, содержащая счетчик ссылок,
код типа и другие упомянутые выше элементы.

\1ное значение. Целое число в Python — указатель на место в памяти, где хранится
вся информация об объекте языка Python, включая байты, содержащие целочис-
ленное значение. Именно эта дополнительная информация в структуре Python для

============================================================
СТРАНИЦА 62
============================================================

\1n с использованием динамической типизации. Однако эта дополнительная
информация в типах Python влечет и накладные расходы, что становится особенно
заметно в структурах, объединяющих значительное количество таких объектов.
Список в языке Python — больше, чем просто список
Теперь рассмотрим, что происходит при использовании структуры языка Python,
содержащей много объектов. Стандартным изменяемым многоэлементным контей-
нером в Python является список. Создать список целочисленных значений можно
следующим образом:
In[1]:
\1nge(10))
       L
Out[1]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
In[2]: type(L[0])
Out[2]: int

\1
In[3]:
\1n L]
       L2
Out[3]: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
In[4]: type(L2[0])
Out[4]: str
В силу динамической типизации языка Python можно создавать даже неоднород-
ные списки:
In[5]:
\1n L3]
Out[5]: [bool, str, float, int] Однако подобная гибкость имеет свою цену: для ис -
пользования гибких типов данных каждый элемент списка должен содержать ин-
формацию о типе, счетчик ссылок и другую информацию, то есть каждый элемент
представляет собой целый объект языка Python. В частном случае совпадения типа
всех переменных большая часть этой информации избыточна: намного рациональ-
нее хранить данные в массиве с фиксированным типом значений. Различие между
списком с динамическим типом значений и списком с фиксированным типом
(в стиле библиотеки NumPy) проиллюстрировано на рис. 2.2.

============================================================
СТРАНИЦА 63
============================================================
Работа с типами данных в языке Python  6 3
PyObject_HEAD 1
2
3
4
5
6
7
8
PyObject_HEAD
0x310718
0x310748
0x310730
0x310760
0x310700
0x3106b8
0x3106d0
0x3106e8
Данные
Измерения
Шаги по индексу
Длина
Элементы
Массив
из библиотеки
NumPy Список языка
Python
Рис. 2.2. Различие между списками в языках C и Python

\1ный блок данных. Список в языке Python же содержит указатель на блок указате-
лей, каждый из которых, в свою очередь, указывает на целый объект языка Python,
например, целое число. Преимущество такого списка состоит в его гибкости: раз
каждый элемент списка — полномасштабная структура, содержащая как данные,
так и информацию о типе, список можно заполнить данными любого требуемого
типа. Массивам с фиксированным типом из библиотеки NumPy недостает этой
гибкости, но они гораздо эффективнее хранят данные и работают с ними.
Массивы с фиксированным типом в языке Python
Язык Python предоставляет несколько возможностей для хранения данных в эф-
фективно работающих буферах с фиксированным типом значений. Встроенный
модуль array (доступен начиная с версии 3.3 языка Python) можно использовать
для создания плотных массивов данных одного типа:
In[6]: import array

\1nge(10))

\1\1ndarray из библиотеки NumPy. В то время как объект
array языка Python обеспечивает эффективное хранение данных в формате мас-
сива, библиотека NumPy добавляет еще и возможность выполнения эффективных
операций над этими данными. Мы рассмотрим такие операции в следующих раз-
делах, а здесь продемонстрируем несколько способов создания NumPy-массива.
Начнем с обычного импорта пакета NumPy под псевдонимом np:
In[7]: import numpy as np
Создание массивов из списков языка Python
Для создания массивов из списков языка Python можно воспользоваться функ -
цией np.array:
In[8]: # массив целочисленных значений:
       np.array([1, 4, 2, 5, 3])
Out[8]: array([1, 4, 2, 5, 3])
В отличие от списков языка Python библиотека NumPy ограничивается массивами,
содержащими элементы одного типа. Если типы элементов не совпадают, NumPy
попытается выполнить повышающее приведение типов (в данном случае цело -
численные значения приводятся к числам с плавающей точкой):
In[9]: np.array([3.14, 4, 2, 3])
Out[9]: array([ 3.14,  4.  ,  2.  ,  3.  ])

\1
In[10]: np.array([1, 2, 3, 4],
\1n массивы библиотеки NumPy могут
явным образом описываться как многомерные. Вот один из способов задания зна-
чений многомерного массива с помощью списка списков:
In[11]: # Вложенные списки преобразуются в многомерный массив
        np.array([range(i, i + 3) for i in [2, 4, 6]])
Out[11]: array([[2, 3, 4],
                [4, 5, 6],
                [6, 7, 8]])
Вложенные списки рассматриваются как строки в итоговом двумерном массиве.

============================================================
СТРАНИЦА 65
============================================================
Работа с типами данных в языке Python  6 5

\1
In[12]: # Создаем массив целых чисел длины 10, заполненный нулями
        np.zeros(10,
\1nt)
Out[12]: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
In[13]: # Создаем массив размером 3 x 5 значений с плавающей точкой,
        # заполненный единицами
        np.ones((3, 5),
\1n[14]: # Создаем массив размером 3 x 5, заполненный значением 3.14
        np.full((3, 5), 3.14)
Out[14]: array([[ 3.14,  3.14,  3.14,  3.14,  3.14],
                [ 3.14,  3.14,  3.14,  3.14,  3.14],
                [ 3.14,  3.14,  3.14,  3.14,  3.14]])
In[15]: # Создаем массив, заполненный линейной последовательностью,
        # начинающейся с 0 и заканчивающейся 20, с шагом 2
        # (аналогично встроенной функции range())
        np.arange(0, 20, 2)
Out[15]: array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])
In[16]: # Создаем массив из пяти значений,
        # равномерно располагающихся между 0 и 1
        np.linspace(0, 1, 5)
Out[16]: array([ 0.  ,  0.25,  0.5 ,  0.75,  1.  ])
In[17]: # Создаем массив размером 3 x 3 равномерно распределенных
        # случайных значения от 0 до 1
        np.random.random((3, 3))
Out[17]: array([[ 0.99844933,  0.52183819,  0.22421193],
                [ 0.08007488,  0.45429293,  0.20941444],
                [ 0.14360941,  0.96910973,  0.946117  ]])
In[18]: # Создаем массив размером 3 x 3 нормально распределенных
        # случайных значения с медианой 0 и стандартным отклонением 1
        np.random.normal(0, 1, (3, 3))
Out[18]: array([[ 1.51772646,  0.39614948, -0.10634696],

============================================================
СТРАНИЦА 66
============================================================

\1n[19]: # Создаем массив размером 3 x 3 случайных целых числа
        # в промежутке [0, 10)
        np.random.randint(0, 10, (3, 3))
Out[19]: array([[2, 3, 4],
                [5, 7, 8],
                [0, 5, 0]])
In[20]: # Создаем единичную матрицу размером 3 x 3
        np.eye(3)
Out[20]: array([[ 1.,  0.,  0.],
                [ 0.,  1.,  0.],
                [ 0.,  0.,  1.]])
In[21]: # Создаем неинициализированный массив из трех целочисленных
        # значений. Значениями будут произвольные, случайно оказавшиеся
        # в соответствующих ячейках памяти данные
        np.empty(3)
Out[21]: array([ 1.,  1.,  1.])

\1писана1  на языке C, эти типы будут знакомы пользователям языков C, Fortran и др.

\1
np.zeros(10,
\1nt16')
или соответствующего объекта из библиотеки NumPy:
Np.zeros(10, dtype=np.int16)
Таблица 2.1. Стандартные типы данных библиотеки NumPy
Тип данных Описание
bool_ Булев тип (True или False), хранящийся в виде 1 байта
int_
Тип целочисленного значения по умолчанию (аналогичен типу long языка C;
обычно int64 или int32)
intc Идентичен типу int языка C (обычно int32 или int64)

\1ntp
Целочисленное значение, используемое для индексов (аналогично типу ssize_t
языка C; обычно int32 или int64)
int8 Байтовый тип (от –128 до 127)
int16 Целое число (от –32 768 до 32 767)
int32 Целое число (от –2 147 483 648 до 2 147 483 647)
int64 Целое число (от –9 223 372 036 854 775 808 до 9 223 372 036 854 775 807)
uint8 Беззнаковое целое число (от 0 до 255)
uint16 Беззнаковое целое число (от 0 до 65 535)
uint32 Беззнаковое целое число (от 0 до 4 294 967 295)
uint64 Беззнаковое целое число (от 0 до 18 446 744 073 709 551 615)
float_ Сокращение для названия типа float64
float16

\1гляните в документацию по пакету NumPy ( http://numpy.org/). Библиотека NumPy
также  поддерживает составные типы данных, которые мы рассмотрим в разделе
«Структурированные данные: структурированные массивы би блиотеки NumPy»
данной главы.
Введение в массивы библиотеки NumPy
Работа с данными на языке Python — практически синоним работы с массивами
библиотеки NumPy: даже более новые утилиты, например библиотека Pandas
(см. главу 3), основаны на массивах NumPy. В этом разделе мы рассмотрим не -
сколько примеров использования массивов библиотеки NumPy для доступа к дан -
ным и подмассивам, а также срезы, изменения формы и объединения массивов.
Хотя демонстрируемые типы операций могут показаться  несколько скучными
и педантичными, они являются своеобразными «кирпичиками» для множества
других примеров из этой книги. Изучите их хорошенько!

============================================================
СТРАНИЦА 68
============================================================

\1n[1]: import numpy as np
       np.random.seed(0)  # начальное значение для целей воспроизводимости

\1np.random.randint(10,
\1np.random.randint(10,
\1np.random.randint(10,
\1ndim (размерность), shape (размер каждого
измерения) и size (общий размер массива):
In[2]: print("x3 ndim: ", x3.ndim)
       print("x3 shape:", x3.shape)
       print("x3 size: ", x3.size)
x3 ndim:  3
x3 shape: (3, 4, 5)
x3 size:  60

\1dtype, тип данных массива (который мы уже ранее
обсуждали в разделе «Работа с типами данных в языке Python» этой главы):
In[3]: print("dtype:", x3.dtype)
dtype: int64

\1мента массива, и nbytes, выводящий полный размер массива (в байтах):
In[4]: print("itemsize:", x3.itemsize, "bytes")

============================================================
СТРАНИЦА 69
============================================================
Введение в массивы библиотеки NumPy  6 9
       print("nbytes:", x3.nbytes, "bytes")
itemsize: 8 bytes
nbytes: 480 bytes
В общем значение атрибута nbytes должно быть равно значению атрибута itemsize,
умноженному на size.
Индексация массива: доступ к отдельным элементам
Если вы знакомы с индексацией стандартных списков языка Python, то индексация
библиотеки NumPy будет для вас привычной. В одномерном массиве обратиться
к i -му (считая с 0) значению можно, указав требуемый индекс в квадратных скоб -
ках, точно так же, как при работе со списками языка Python:
In[5]: x1
Out[5]: array([5, 0, 3, 3, 7, 9])
In[6]: x1[0]
Out[6]: 5
In[7]: x1[4]
Out[7]: 7

\1
In[8]: x1[-1]
Out[8]: 9
In[9]: x1[-2]
Out[9]: 7

\1
In[10]: x2
Out[10]: array([[3, 5, 2, 4],
                [7, 6, 8, 8],
                [1, 6, 7, 7]])
In[11]: x2[0, 0]
Out[11]: 3

============================================================
СТРАНИЦА 70
============================================================

\1n[12]: x2[2, 0]
Out[12]: 1
In[13]: x2[2, -1]
Out[13]: 7
Вы также можете изменить значения, используя любую из перечисленных выше
индексных нотаций.
In[14]: x2[0, 0] = 12
        x2
Out[14]: array([[12, 5, 2, 4],
                [ 7, 6, 8, 8],
                [ 1, 6, 7, 7]])
Не забывайте, что, в отличие от списков языка Python, у массивов NumPy фикси-
рованный тип данных. При попытке вставить в массив целых чисел значение с пла-
вающей точкой это значение будет незаметно усечено. Не попадитесь в ловушку!
In[15]: x1[0] = 3.14159  # это значение будет усечено!
        x1
Out[15]: array([3, 0, 3, 3, 7, 9])

\1ные скобки для доступа к подмассивам с помощью срезов (slicing), обозначаемых
знаком двоеточия ( :). Синтаксис срезов библиотеки NumPy соответствует анало-
гичному синтаксису для стандартных списков языка Python. Для доступа к срезу
массива x используйте синтаксис:
x[начало:конец:шаг]

\1смотрим доступ к массивам в одном и нескольких измерениях.
Одномерные подмассивы
In[16]:
\1np.arange(10)
        x
Out[16]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

============================================================
СТРАНИЦА 71
============================================================
Введение в массивы библиотеки NumPy  7 1
In[17]: x[:5]  # первые пять элементов
Out[17]: array([0, 1, 2, 3, 4])
In[18]: x[5:]  # элементы после индекса = 5
Out[18]: array([5, 6, 7, 8, 9])
In[19]: x[4:7]  # подмассив из середины
Out[19]: array([4, 5, 6])
In[20]: x[::2]  # каждый второй элемент
Out[20]: array([0, 2, 4, 6, 8])
In[21]: x[1::2]  # каждый второй элемент, начиная с индекса 1
Out[21]: array([1, 3, 5, 7, 9])

\1
In[22]: x[::-1]  # все элементы в обратном порядке
Out[22]: array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])
In[23]: x[5::-2]  # каждый второй элемент в обратном порядке,
                  # начиная с индекса 5
Out[23]: array([5, 3, 1])

\1
In[24]: x2
Out[24]: array([[12,  5,  2,  4],
                [ 7,  6,  8,  8],
                [ 1,  6,  7,  7]])
In[25]: x2[:2, :3]  # две строки, три столбца
Out[25]: array([[12,  5,  2],
                [ 7,  6,  8]])
In[26]: x2[:3, ::2]  # все строки, каждый второй столбец

============================================================
СТРАНИЦА 72
============================================================

\1n[27]: x2[::-1, ::-1]
Out[27]: array([[ 7,  7,  6,  1],
                [ 8,  8,  6,  7],
                [ 4,  2,  5, 12]])

\1
In[28]: print(x2[:, 0])  # первый столбец массива x2
[12  7  1]
In[29]: print(x2[0, :])  # первая строка массива x2
[12  5  2  4]

\1
In[30]: print(x2[0])  # эквивалентно x2[0, :]
[12  5  2  4]
Подмассивы как предназначенные только для чтения
представления
Срезы массивов возвращают представления  (views), а не копии (copies) данных
массива. Этим срезы массивов библиотеки NumPy отличаются от срезов списков
языка Python (в списках срезы являются копиями). Рассмотрим уже знакомый
нам двумерный массив:
In[31]: print(x2)
[[12  5  2  4]
 [ 7  6  8  8]
 [ 1  6  7  7]]

============================================================

\1
In[32]:
\1nt(x2_sub)
[[12  5]
 [ 7  6]]

\1
In[33]: x2_sub[0, 0] = 99
        print(x2_sub)
[[99  5]
 [ 7  6]]
In[34]: print(x2)
[[99  5  2  4]
 [ 7  6  8  8]
 [ 1  6  7  7]]

\1
In[35]:
\1nt(x2_sub_copy)
[[99  5]
 [ 7  6]]

\1
In[36]: x2_sub_copy[0, 0] = 42
        print(x2_sub_copy)
[[42  5]
 [ 7  6]]
In[37]: print(x2)
[[99  5  2  4]
 [ 7  6  8  8]
 [ 1  6  7  7]]

============================================================
СТРАНИЦА 74
============================================================

\1n[38]:
\1np.arange(1, 10).reshape((3, 3))
        print(grid)
[[1 2 3]
 [4 5 6]
 [7 8 9]]

\1преобразование одно -
мерного массива в двумерную матрицу-строку или матрицу-столбец. Для этого
можно применить метод reshape, но лучше воспользоваться ключевым словом
newaxis при выполнении операции среза:
In[39]:
\1np.array([1, 2, 3])
        # Преобразование в вектор-строку с помощью reshape
        x.reshape((1, 3))
Out[39]: array([[1, 2, 3]])
In[40]: # Преобразование в вектор-строку посредством newaxis
        x[np.newaxis, :]
Out[40]: array([[1, 2, 3]])
In[41]: # Преобразование в вектор-столбец с помощью reshape
        x.reshape((3, 1))
Out[41]: array([[1],
                [2],
                [3]])
In[42]: # Преобразование в вектор-столбец посредством newaxis
        x[:, np.newaxis]
Out[42]: array([[1],
                [2],
                [3]])

\1массивов. Эти операции мы рассмотрим далее.
Слияние массивов
Слияние, или объединение, двух массивов в библиотеке NumPy выполняется
в основном с помощью методов np.concatenate , np.vstack и np.hstack. Метод
np.concatenate принимает на входе кортеж или список массивов в качестве перво-
го аргумента:
In[43]:
\1np.array([1, 2, 3])

\1np.array([3, 2, 1])
        np.concatenate([x, y])
Out[43]: array([1, 2, 3, 3, 2, 1])

\1
In[44]:
\1nt(np.concatenate([x, y, z]))
[ 1  2  3  3  2  1999999]
Для объединения двумерных массивов можно также использовать np.concatenate:
In[45]:
\1np.array([[1, 2, 3],
                         [4, 5, 6]])
In[46]: # слияние по первой оси координат
        np.concatenate([grid, grid])
Out[46]: array([[1, 2, 3],
                [4, 5, 6],
                [1, 2, 3],
                [4, 5, 6]])
In[47]: # слияние по второй оси координат (с индексом 0)
        np.concatenate([grid, grid],
\1np.vstack (вертикальное объединение) и np.hstack (гори-
зонтальное объединение):

============================================================
СТРАНИЦА 76
============================================================

\1n[48]:
\1np.array([1, 2, 3])

\1np.array([[9, 8, 7],
                         [6, 5, 4]])
        # Объединяет массивы по вертикали
        np.vstack([x, grid])
Out[48]: array([[1, 2, 3],
                [9, 8, 7],
                [6, 5, 4]])
In[49]: # Объединяет массивы по горизонтали

\1np.array([[99],
                      [99]])
        np.hstack([grid, y])
Out[49]: array([[ 9,  8,  7, 99],
                [ 6,  5,  4, 99]])
Функция np.dstack аналогично объединяет массивы по третьей оси.
Разбиение массивов
Противоположностью слияния является разбиение, выполняемое с помощью
функций np.split, np.hsplit и np.vsplit. Каждой из них необходимо передавать
список индексов, задающих точки раздела:
In[50]:
\1np.split(x, [3, 5])
        print(x1, x2, x3)
[1 2 3] [99 99] [3 2 1]

\1ющие функции np.hsplit и np.vsplit действуют аналогично:
In[51]:
\1np.arange(16).reshape((4, 4))
        grid
Out[51]: array([[ 0,  1,  2,  3],
                [ 4,  5,  6,  7],
                [ 8,  9, 10, 11],
                [12, 13, 14, 15]])
In[52]: upper,
\1np.vsplit(grid, [2])
        print(upper)
        print(lower)
[[0 1 2 3]
 [4 5 6 7]]
[[ 8  9 10 11]
 [12 13 14 15]]

============================================================
СТРАНИЦА 77
============================================================
Выполнение вычислений над массивами библиотеки NumPy  7 7
In[53]: left,
\1np.hsplit(grid, [2])
        print(left)
        print(right)
[[ 0  1]
 [ 4  5]
 [ 8  9]
 [12 13]]
[[ 2  3]
 [ 6  7]
 [10 11]
 [14 15]]
Функция np.dsplit аналогично разделяет массивы по третьей оси.

\1использование вектори -
зованных операций, обычно реализуемых посредством универсальных функций
(universal functions, ufuncs) языка Python. В данном разделе будет обосновано,
почему универсальные функции библиотеки NumPy необходимы и почему они
могут намного ускорить выполнение повторяющихся вычислений над элементами
массивов, а также познакомим вас с множеством наиболее распространенных и по-
лезных универсальных арифметических функций из библиотеки NumPy.
Медлительность циклов
Реализация языка Python по умолчанию (известная под названием CPython)
выполняет некоторые операции очень медленно. Частично это происходит из-за
динамической, интерпретируемой природы языка. Гибкость типов означает, что
последовательности операций нельзя скомпилировать в столь же производитель-
ный машинный код, как в случае языков C и Fortran. В последнее время было
предпринято несколько попыток справиться с этой проблемой:
 проект PyPy ( http://pypy.org), реализация языка Python с динамической компи-
ляцией;
 проект Cython ( http://cython.org), преобразующий код на языке Python в компи-
лируемый код на языке C;

============================================================
СТРАНИЦА 78
============================================================

\1numba.pydata.org), преобразующий фрагменты кода на языке
Python в быстрый LLVM-байткод.
У каждого проекта есть свои сильные и слабые стороны, но ни один из них пока не
обошел стандартный механизм CPython по популярности.
Относительная медлительность Python обычно обнаруживается при повторении
множества мелких операций, например при выполнении обработки всех элементов
массива в цикле. Пусть у нас имеется массив значений и необходимо вычислить
обратную величину каждого из них. Очевидное решение могло бы выглядеть сле-
дующим образом:
In[1]: import numpy as np
       np.random.seed(0)
       def compute_reciprocals(values):

\1np.empty(len(values))
           for i in range(len(values)):
               output[i] = 1.0 / values[i]
           return output

\1np.random.randint(1, 10,
\1n (обсуждавшейся в разделе «Профилирование
и мониторинг скорости выполнения кода» главы 1):
In[2]:
\1np.random.randint(1, 100,
\1n при каждом проходе цикла. Всякий раз, когда вычисляется обратная
величина, Python сначала проверяет тип объекта и выполняет динамический по-
иск подходящей для этого типа функции. Если бы мы работали с компилируемым
кодом, сведения о типе были бы известны до выполнения кода, а значит, результат
вычислялся бы намного эффективнее.

============================================================

\1
In[3]: print(compute_reciprocals(values))
       print(1.0 / values)
[ 0.16666667  1. | 0.25 | 0.25 | 0.125 | ]
[ 0.16666667  1. | 0.25 | 0.25 | 0.125 | ]

\1рядков быстрее, чем стандартный цикл Python:
In[4]: %timeit (1.0 / big_array)
100 loops, best of 3: 4.6 ms per loop

\1версальных функций  (ufuncs), главная задача которых состоит в быстром выпол -
нении повторяющихся операций над значениями из массивов библиотеки NumPy .

\1
In[5]: np.arange(5) / np.arange(1, 6)
Out[5]: array([ 0. | ,  0.5 | ,  0.66666667,  0.75 | ,  0.8 | ])

\1
In[6]:
\1np.arange(9).reshape((3, 3))
       2 ** x
Out[6]: array([[  1, | 2, | 4],
               [  8,  16,  32],
               [ 64, 128, 256]])

\1ций практически всегда более эффективны, чем их эквиваленты, реализованные
с помощью циклов Python, особенно при росте размера массивов. Столкнувшись
с подобным циклом в сценарии на языке Python, следует обдумать, не стоит ли
заменить его векторизованным выражением.

============================================================
СТРАНИЦА 80
============================================================

\1n. Можно
выполнять обычные сложение, вычитание, умножение и деление:
In[7]:
\1np.arange(4)
print("x | =", x)
       print("x + 5 =", x + 5)
       print("x - 5 =", x - 5)
       print("x * 2 =", x * 2)
       print("x / 2 =", x / 2)
       print("x // 2 =", x // 2)  # деление с округлением в меньшую сторону
x | = [0 1 2 3]
x + 5 = [5 6 7 8]
x - 5 = [-5 -4 -3 -2]
x * 2 = [0 2 4 6]
x / 2 = [ 0. | 0.5  1. | 1.5]
x // 2 = [0 0 1 1]

\1
In[8]: print("-x | = ", -x)
       print("x ** 2 = ", x ** 2)
       print("x % 2  = ", x % 2)
-x | =  [ 0 -1 -2 -3]
x ** 2 =  [0 1 4 9]
x % 2  =  [0 1 0 1]

\1
In[9]: -(0.5*x + 1) ** 2
Out[9]: array([-1.  , -2.25, -4.  , -6.25])

\1
In[10]: np.add(x, 2)
Out[10]: array([2, 3, 4, 5])

============================================================
СТРАНИЦА 81
============================================================
Выполнение вычислений над массивами библиотеки NumPy  8 1
В табл. 2.2 перечислены реализованные в библиотеке NumPy арифметические
операторы.
Таблица 2.2. Реализованные в библиотеке NumPy арифметические операторы
Оператор Эквивалентная универсальная
функция
Описание
+ np.add Сложение (например, 1 + 1 = 2)
– np.subtract Вычитание (например, 3 – 2 = 1)
– np.negative Унарная операция изменения знака (напри-
мер, –2)
* np.multiply Умножение (например, 2 * 3 = 6)
/ np.divide Деление (например, 3 / 2 = 1.5)
// np.floor_divide Деление с округлением в меньшую сторону
(например, 3 // 2 = 1)
** np.power Возведение в степень (например, 2 ** 3 = 8)
% np.mod Модуль/остаток (например, 9 % 4 = 1)

\1смотрим в разделе «Сравнения, маски и булева логика» данной главы.
Абсолютное значение
Аналогично тому, что библиотека NumPy понимает встроенные арифметические
операторы, она также понимает встроенную функцию абсолютного значения языка
Python:
In[11]:
\1np.array([-2, -1, 0, 1, 2])
        abs(x)
Out[11]: array([2, 1, 0, 1, 2])

\1np.absolute, до -
ступная также под псевдонимом np.abs:
In[12]: np.absolute(x)
Out[12]: array([2, 1, 0, 1, 2])
In[13]: np.abs(x)
Out[13]: array([2, 1, 0, 1, 2])

\1
In[14]:
\1np.array([3 - 4j, 4 - 3j, 2 + 0j, 0 + 1j])

============================================================
СТРАНИЦА 82
============================================================

\1np.abs(x)
Out[14]: array([ 5.,  5.,  2.,  1.])

\1
In[15]:
\1np.linspace(0, np.pi, 3)

\1
In[16]: print("theta | = ", theta)
        print("sin(theta) = ", np.sin(theta))
        print("cos(theta) = ", np.cos(theta))
        print("tan(theta) = ", np.tan(theta))
theta | =  [ 0. | 1.57079633  3.14159265]
sin(theta) =  [  0.00000000e+00 | 1.00000000e+00 | 1.22464680e-16]
cos(theta) =  [  1.00000000e+00 | 6.12323400e-17  -1.00000000e+00]
tan(theta) =  [  0.00000000e+00 | 1.63312394e+16  -1.22464680e-16]

\1
In[17]:
\1nt("x | = ", x)
        print("arcsin(x) = ", np.arcsin(x))
        print("arccos(x) = ", np.arccos(x))
        print("arctan(x) = ", np.arctan(x))
x | =  [-1, 0, 1]
arcsin(x) =  [-1.57079633  0. | 1.57079633]
arccos(x) =  [ 3.14159265  1.57079633  0. | ]
arctan(x) =  [-0.78539816  0. | 0.78539816]

\1
In[18]:
\1nt("x | =", x)
print("e^x | =", np.exp(x))
print("2^x | =", np.exp2(x))
print("3^x | =", np.power(3, x))

============================================================
СТРАНИЦА 83
============================================================
Выполнение вычислений над массивами библиотеки NumPy  8 3
x | = [1, 2, 3]
e^x | = [  2.71828183 | 7.3890561 | 20.08553692]
2^x | = [ 2.  4.  8.]
3^x | = [ 3  9 27]
Функции, обратные к показательным, и логарифмы также имеются в библиотеке.
Простейшая функция np.log возвращает натуральный логарифм числа. Если вам
требуется логарифм по основанию 2 или 10, они также доступны:
In[19]:
\1nt("x | =", x)
print("ln(x) | =", np.log(x))
        print("log2(x)  =", np.log2(x))
        print("log10(x) =", np.log10(x))
x | = [1, 2, 4, 10]
ln(x) | = [ 0. | 0.69314718  1.38629436  2.30258509]
log2(x)  = [ 0. | 1. | 2. | 3.32192809]
log10(x) = [ 0. | 0.30103 | 0.60205999  1. | ]

\1
In[20]:
\1nt("exp(x) - 1 =", np.expm1(x))
        print("log(1 + x) =", np.log1p(x))
exp(x) - 1 = [ 0. | 0.0010005 | 0.01005017  0.10517092]
log(1 + x) = [ 0. | 0.0009995 | 0.00995033  0.09531018]
При очень малых значениях элементов вектора x данные функции возвращают
намного более точные результаты, чем обычные функции np.log и np.exp.

\1

============================================================
СТРАНИЦА 84
============================================================

\1n[21]: from scipy import special
In[22]: # Гамма-функции (обобщенные факториалы) и тому подобные функции

\1nt("gamma(x) | =", special.gamma(x))
        print("ln|gamma(x)| =", special.gammaln(x))
print("beta(x, 2) | =", special.beta(x, 2))
gamma(x) | = [  1.00000000e+00 | 2.40000000e+01 | 3.62880000e+05]
ln|gamma(x)| = [  0. | 3.17805383  12.80182748]
beta(x, 2) | = [ 0.5 | 0.03333333  0.00909091]
In[23]: # Функция ошибок (интеграл от Гауссовой функции),
        # дополнительная и обратная к ней функции

\1np.array([0, 0.3, 0.7, 1.0])
        print("erf(x)  =", special.erf(x))
        print("erfc(x) =", special.erfc(x))
        print("erfinv(x) =", special.erfinv(x))
erf(x)  = [ 0. | 0.32862676  0.67780119  0.84270079]
erfc(x) = [ 1. | 0.67137324  0.32219881  0.15729921]
erfinv(x) = [ 0. | 0.27246271  0.73286908 | inf]

\1
In[24]:
\1np.arange(5)

\1np.empty(5)
        np.multiply(x, 10,
\1nt(y)
[  0.  10.  20.  30.  40.]

\1

============================================================
СТРАНИЦА 85
============================================================
Выполнение вычислений над массивами библиотеки NumPy  8 5
In[25]:
\1np.zeros(10)
        np.power(2, x,
\1nt(y)
[  1. | 0. | 2. | 0. | 4. | 0. | 8. | 0.  16. | 0.]

\1
In[26]:
\1np.arange(1, 6)
        np.add.reduce(x)
Out[26]: 15

\1
In[27]: np.multiply.reduce(x)
Out[27]: 120

\1
In[28]: np.add.accumulate(x)
Out[28]: array([ 1,  3,  6, 10, 15])
In[29]: np.multiply.accumulate(x)
Out[29]: array([  1, | 2, | 6,  24, 120])

\1чений существуют и специализированные функции библиотеки NumPy ( np.sum,

============================================================
СТРАНИЦА 86
============================================================

\1np.prod, np.cumsum, np.cumprod), которые мы рассмотрим в разделе «Агрегирование:
минимум, максимум и все, что посередине» данной главы.

\1
In[30]:
\1np.arange(1, 6)
        np.multiply.outer(x, x)
Out[30]: array([[ 1,  2,  3,  4,  5],
                [ 2,  4,  6,  8, 10],
                [ 3,  6,  9, 12, 15],
                [ 4,  8, 12, 16, 20],
                [ 5, 10, 15, 20, 25]])
Очень удобны методы ufunc.at и ufunc.reduceat, которые мы рассмотрим в разделе
«“Прихотливая” индексация» данной главы.
Универсальные функции дают возможность работать с массивами различных
размеров и форм, используя набор операций под названием транслирование
(broadcasting). Эта тема достаточно важна, так что ей будет посвящен целый раздел
(см. «Операции над массивами. Транслирование» данной главы).
Универсальные функции: дальнейшая информация
На сайтах документации библиотек NumPy и SciPy можно найти дополнительную
информацию об универсальных функциях (включая полный список имеющихся
функций).
Получить доступ к этой информации можно непосредственно из оболочки IPython
путем импорта этих пакетов и использования автодополнения табуляц ией (кла -
виша Tab) и справочной функциональности ( ?), как описано в разделе «Справка
и документация в оболочке Python» главы 1.

\1наиболее распространенные сводные статистические показате-

============================================================

\1сивами. Продемонстрирую некоторые из них.
Суммирование значений из массива
В качестве примера рассмотрим вычисление суммы значений массива. В «чистом»
языке Python это можно сделать с помощью встроенной функции sum:
In[1]: import numpy as np
In[2]:
\1np.random.random(100)
       sum(L)
Out[2]: 55.61209116604941

\1
In[3]: np.sum(L)
Out[3]: 55.612091166049424

\1
In[4]:
\1np.random.rand(1000000)
       %timeit sum(big_array)
       %timeit np.sum(big_array)
10 loops, best of 3: 104 ms per loop
1000 loops, best of 3: 442 µs per loop
Будьте осторожны: функции sum и np.sum не идентичны. Например, смысл их
необязательных аргументов различен и функция np.sum умеет работать с много -
мерными массивами.
Минимум и максимум
В «чистом» языке Python имеются встроенные функции min и max, используемые
для вычисления минимального и максимального значений любого заданного мас-
сива:
In[5]: min(big_array), max(big_array)
Out[5]: (1.1717128136634614e-06, 0.9999976784968716)

============================================================
СТРАНИЦА 88
============================================================

\1n[6]: np.min(big_array), np.max(big_array)
Out[6]: (1.1717128136634614e-06, 0.9999976784968716)
In[7]: %timeit min(big_array)
       %timeit np.min(big_array)
10 loops, best of 3: 82.3 ms per loop
1000 loops, best of 3: 497 µs per loop
Для min, max, sum и еще нескольких функций вычисления сводных показателей
библиотеки NumPy существует сокращенная запись операции путем применения
методов самого объекта массива:
In[8]: print(big_array.min(), big_array.max(), big_array.sum())
1.17171281366e-060.999997678497499911.628197

\1
In[9]:
\1np.random.random((3, 4))
       print(M)
[[ 0.8967576 | 0.03783739  0.75952519  0.06682827]
[ 0.8354065 | 0.99196818  0.19544769  0.43447084]
 [ 0.66859307  0.15038721  0.37911423  0.6687194 ]]

\1
In[10]: M.sum()
Out[10]: 6.0850555667307118

\1
In[11]: M.min(
\1\1
In[12]: M.max(
\1np.sum np.nansum Вычисляет сумму элементов
np.prod np.nanprod Вычисляет произведение элементов
np.mean np.nanmean Вычисляет среднее значение элементов
np.std np.nanstd Вычисляет стандартное отклонение
np.var np.nanvar Вычисляет дисперсию
np.min np.nanmin Вычисляет минимальное значение
np.max np.nanmax Вычисляет максимальное значение
np.argmin np.nanargmin Возвращает индекс минимального значения
np.argmax np.nanargmax Возвращает индекс максимального значения
np.median np.nanmedian Вычисляет медиану элементов
np.percentile np.nanpercentile Вычисляет квантили элементов
np.any N/A Проверяет, существуют ли элементы со значением true
np.all N/A Проверяет, все ли элементы имеют значение true
Мы часто будем встречаться с этими агрегирующими функциями в дальнейшем.

============================================================
СТРАНИЦА 90
============================================================

\1nt_heights.csv ,
представляющем собой простой разделенный запятыми список меток и значений:
In[13]: !head -4 data/president_heights.csv
order,name,height(cm)
1,George Washington,189
2,John Adams,170
3,Thomas Jefferson,189
Мы воспользуемся пакетом Pandas, который изучим более детально в главе 3, для
чтения файла и извлечения данной информации (обратите внимание, что рост
указан в сантиметрах):
In[14]: import pandas as pd

\1nt_heights.csv')

\1np.array(data['height(cm)'])
        print(heights)
[189 170 189 163 183 171 185 168 173 183 173 173 175 178 183 193 178 173
 174 183 183 168 170 178 182 180 183 178 182 188 175 179 183 193 182 183
 177 185 188 188 182 185]

\1
In[15]: print("Mean height: | ", | heights.mean())
        print("Standard deviation:", heights.std())
print("Minimum height: | ", | heights.min())
print("Maximum height: | ",  heights.max())
Mean height: | 179.738095238
Standard deviation: | 6.93184344275
Minimum height: | 163
Maximum height: | 193

\1
In[16]: print("25th percentile: | ", np.percentile(heights, 25))
print("Median: | ", np.median(heights))
print("75th percentile: | ", np.percentile(heights, 75))
25th percentile: | 174.25
Median: | 182.0
75th percentile: | 183.0

============================================================

\1
In[17]: %matplotlib inline
        import matplotlib.pyplot as plt
        import seaborn; seaborn.set()  # задает стиль графика
In[18]: plt.hist(heights)
        plt.title('Height Distribution of US Presidents') # Распределение роста
                                                          # президентов США
plt.xlabel('height (cm)') | # Рост, см
plt.ylabel('number'); | # Количество

\1базовые элементы так называемого разведочного ана-
лиза данных (exploratory data analysis), который мы рассмотрим подробнее в сле-
дующих главах книги.

\1тельно, устранения медленных стандартных циклов языка Python. Еще один спо -
соб применения операций векторизации — использовать имеющиеся в | библиотеке

============================================================
СТРАНИЦА 92
============================================================

\1ng). Транслирование представ -
ляет собой набор правил по применению бинарных универсальных функций
(сложение, вычитание, умножение и т. д.) к массивам различного размера.

\1
In[1]: import numpy as np
In[2]:
\1np.array([0, 1, 2])

\1np.array([5, 5, 5])
       a + b
Out[2]: array([5, 6, 7])

\1
In[3]: a + 5
Out[3]: array([5, 6, 7])

\1
In[4]:
\1np.ones((3, 3))
       M
Out[4]: array([[ 1.,  1.,  1.],
               [ 1.,  1.,  1.],
               [ 1.,  1.,  1.]])
In[5]: M + a
Out[5]: array([[ 1.,  2.,  3.],
               [ 1.,  2.,  3.],
               [ 1.,  2.,  3.]])

\1
In[6]:
\1np.arange(3)

\1np.arange(3)[:, np.newaxis]
       print(a)
       print(b)
[0 1 2]
[[0]
 [1]
 [2]]
In[7]: a + b
Out[7]: array([[0, 1, 2],
               [1, 2, 3],
               [2, 3, 4]])

\1метрия этих примеров наглядно показана на рис. 2.4 1 .
0 1 2 5 6 75
np.arange(3)+5
11 1
11 1
11 1
1
2
3
1
1
2
2
3
3
1 20
np.ones((3,3))+np.arrange(3)
1
1 2
3 4
3
2
20
1
0
2
1 20
np.ones((3,1))+np.arrange(3)
Рис. 2.4. Визуализация транслирования массивов библиотекой NumPy

\1n[8]:
\1np.ones((2, 3))

\1np.arange(3)

\1
In[9]: M + a
Out[9]: array([[ 1.,  2.,  3.],
               [ 1.,  2.,  3.]])

============================================================

\1
In[10]:
\1np.arange(3).reshape((3, 1))

\1np.arange(3)

\1
In[11]: a + b
Out[11]: array([[0, 1, 2],
                [1, 2, 3],
                [2, 3, 4]])

\1
In[12]:
\1np.ones((3, 2))

\1np.arange(3)

\1

============================================================
СТРАНИЦА 96
============================================================

\1n[13]: M + a
---------------------------------------------------------------------------
ValueError | Traceback (most recent call last)
<ipython-input-13-9
\1
\1nds could not be broadcast together with shapes (3,2) (3,)
Обратите внимание на имеющийся потенциальный источник ошибки: можно
было бы сделать массивы a и M совместимыми, скажем путем дополнения формы a
единицами справа, а не слева. Но правила транслирования работают не так!
Если вам хочется применить правостороннее дополнени е, можете сделать это
явным образом, поменяв форму массива (мы воспользуемся ключевым словом
np.newaxis , описанным в разделе «Введение в массивы библиотеки NumPy»
данной главы):
In[14]: a[:, np.newaxis].shape
Out[14]: (3, 1)
In[15]: M + a[:, np.newaxis]
Out[15]: array([[ 1.,  1.],
                [ 2.,  2.],
                [ 3.,  3.]])

\1
In[16]: np.logaddexp(M, a[:, np.newaxis])
Out[16]: array([[ 1.31326169,  1.31326169],
                [ 1.69314718,  1.69314718],
                [ 2.31326169,  2.31326169]])

\1основное ядро множества примеров, приводимых
в дальнейшем в нашей книге. Рассмотрим несколько простых примеров сфер их
возможного применения.

\1ются от необходимости писать явным образом медленно работающие циклы языка
Python. Транслирование расширяет эти возможности. Один из часто встречающихся
примеров — центрирование массива. Пускай у вас есть массив из десяти наблюдений,
каждое состоит из трех значений. Используя стандартные соглашения (см. «Пред-
ставление данных в Scikit-Learn» главы 5), сохраним эти данные в массиве 10 × 3:
In[17]:
\1np.random.random((10, 3))
Вычислить среднее значение каждого признака можно, примененив функцию
агрегирования mean по первому измерению:
In[18]:
\1n(0)
        Xmean
Out[18]: array([ 0.53514715,  0.66567217,  0.44385899])

\1
In[19]:
\1n

\1
In[20]: X_centered.mean(0)
Out[20]: array([  2.22044605e-17,  -7.77156117e-17,  -1.66533454e-17])

\1
In[21]: # Задаем для x и y 50 шагов от 0 до 5

\1np.linspace(0, 5, 50)

\1np.linspace(0, 5, 50)[:, np.newaxis]

\1np.sin(x) ** 10 + np.cos(10 + y * x) * np.cos(x)

============================================================
СТРАНИЦА 98
============================================================

\1n[22]: %matplotlib inline1
       import matplotlib.pyplot as plt
In[23]: plt.imshow(z,
\1
\1\1n, а только в блокноте.

\1ndas (рассматриваемой подробнее в главе 3) мы загрузили еж едневную стати -
стику по осадкам для Сиэтла за 2014 год:
In[1]: import numpy as np
       import pandas as pd
       # Используем Pandas для извлечения количества осадков в дюймах
       # в виде NumPy-массива

\1
\1nfall / 254  # 1/10mm -> inches
       inches.shape
Out[1]: (365,)

\1
In[2]: %matplotlib inline
       import matplotlib.pyplot as plt
       import seaborn; seaborn.set()  # задаем стили
In[3]: plt.hist(inches, 40);

\1
Рис. 2.6. Гистограмма осадков в 2014 году в Сиэтле

============================================================
СТРАНИЦА 100
============================================================

\1ng) для быстрого ответа на подобные вопросы.

\1
In[4]:
\1np.array([1, 2, 3, 4, 5])
In[5]:
\1n[6]:
\1n[7]:
\1n[8]:
\1n[9]:
\1n[10]:
\1\1
In[11]: (2 * x) == (x ** 2)
Out[11]: array([False,  True, False, False, False],
\1np.less(x, 3).
Таблица 2.4. Краткий список операторов сравнения и эквивалентных им универсальных функций
Оператор Эквивалентная универсальная функция
== np.
\1np.
\1np.
\1np.
\1np.
\1np.greater_equal

\1
In[12]:
\1np.random.RandomState(0)

\1ng.randint(10,
\1n[13]:
\1\1np.count_nonzero:
In[15]: # Сколько значений массива меньше 6?
        np.count_nonzero(
\1np.sum. В этом
случае False интерпретируется как 0, а True — как 1:
In[16]: np.sum(
\1np.sum заключается в том, что, подобно другим функциям
агрегирования библиотеки NumPy, это суммирование можно выполнять также по
столбцам или строкам:
In[17]: # Сколько значений меньше 6 содержится в каждой строке?
        np.sum(
\1np.any() и np.all():
In[18]: # Имеются ли в массиве какие-либо значения, превышающие 8?
        np.any(
\1n[19]: # Имеются ли в массиве какие-либо значения меньше 0?
        np.any(
\1n[20]: # Все ли значения меньше 10?
        np.all(
\1n[21]: # Все ли значения равны 6?
        np.all(
\1np.any() и np.all() также можно использовать по конкретным осям.

\1
In[22]: # Все ли значения в каждой строке меньше 8?
        np.all(
\1n имеются
встроенные функции sum(), any() и all(). Их синтаксис отличается от аналогичных
функций библиотеки NumPy. В частности, они будут вы давать ошибку или неожи-
данные результаты при использовании для работы с многомерными массивами.
Убедитесь, что вы применяете для данных примеров функции np.sum(), np.any()
и np.all().
Булевы операторы
Вы уже знаете, как можно подсчитать все дни с осадками менее четырех дюймов
или все дни с осадками более двух дюймов. Но что, если нужна информация обо
всех днях с толщиной слоя осадков менее четырех дюймов и более одного дюйма?
Это можно сделать с помощью побитовых логических операторов  (bitwise logic
operators) языка Python: &, |, ^ и ~. Аналогично обычным арифметическим операто-
рам библиотека NumPy перегружает их как универсальные функции, поэлементно
работающие с (обычно булевыми) массивами.

\1
In[23]: np.sum((
\1
\1\1
\1nches) < 1

\1
In[24]: np.sum(~( (
\1
\1np.bitwise_and
| np.bitwise_or
^ np.bitwise_xor
~ np.bitwise_not

\1
In[25]: print("Number days without rain: ", | np.sum(
\1nt("Number days with rain: ", | np.sum(
\1nt("Days with more than 0.5 inches:",  np.sum(
\1nt("Rainy days
\1nches :", | np.sum((
\1
\1n: | 215
Number days with rain: | 150
Days with more than 0.5 inches: 37
Rainy days
\1nches :  75

\1

============================================================
СТРАНИЦА 105
============================================================
Сравнения, маски и булева логика  105
In[26]: x
Out[26]: array([[5, 0, 3, 3],
                [7, 9, 3, 5],
                [2, 4, 7, 6]])

\1
In[27]:
\1\1
In[28]: x[
\1\1
In[29]:
# создаем маску для всех дождливых дней

\1
\1np.arange(365) - 172 < 90) & (np.arange(365) - 172 > 0)
print("Median precip on rainy days in 2014 (inches): ",
      np.median(inches[rainy]))
print("Median precip on summer days in 2014 (inches): ",
      np.median(inches[summer]))
print("Maximum precip on summer days in 2014 (inches): ",
      np.max(inches[summer]))
print("Median precip on non-summer rainy days (inches):",
      np.median(inches[rainy & ~summer]))
Median precip on rainy days in 2014 (inches): 0.194881889764
Median precip on summer days in 2014 (inches): 0.0
Maximum precip on summer days in 2014 (inches): 0.850393700787
Median precip on non-summer rainy days (inches): 0.200787401575

\1nd/or по сравнению с использованием
операторов &\|

\1

\1ность или ложность всего объекта, операторы & и | оперируют отдельными битами

внутри каждого из объектов.
Использование ключевых слов and и or приводит к тому, что язык Python будет рас-
сматривать объект как одну булеву сущность. В Python все ненулевые целые числа
будут рассматриваться как True. Таким образом:
In[30]: bool(42), bool(0)
Out[30]: (True, False)
In[31]: bool(42 and 0)
Out[31]: False
In[32]: bool(42 or 0)
Out[32]: True
При использовании операторов & и | для работы с целыми числами выражение
оперирует разрядами элемента, фактически применяя операции and и or к состав-
ляющим число отдельным битам:
In[33]: bin(42)
Out[33]: '0b101010'
In[34]: bin(59)
Out[34]: '0b111011'
In[35]: bin(42 & 59)
Out[35]: '0b101010'
In[36]: bin(42 | 59)
Out[36]: '0b111011'

\1
In[37]:
\1np.array([1, 0, 1, 0, 1, 0],
\1np.array([1, 1, 1, 0, 1, 1],
\1\1
In[38]: A or B
------------------------------------------------------------------------
ValueError | Traceback (most recent call last)
<ipython-input-38-5
\1
\1n array with more than one element is...

\1торы & и |, а не операции and или or:
In[39]:
\1np.arange(10)
        (
\1\1
In[40]: (
\1nd (
\1nt call last)
<ipython-input-40-3
\1
\1nd (
\1n array with more than one element is...
Итак, запомните: операции and и or вычисляют единое булево значение для всего
объекта, в то время как операторы & и | вычисляют много булевых значений для со-
держимого (отдельных битов или байтов) объекта. Второй из этих вариантов прак-
тически всегда будет именно той операцией, которая будет вам нужна при работе
с булевыми массивами библиотеки NumPy.

============================================================
СТРАНИЦА 108
============================================================

\1ncy indexing).
«Прихотливая» индексация похожа на уже рассмотренную нами простую индекса-
цию, но вместо скалярных значений передаются массивы индексов. Это дает воз -
можность очень быстрого доступа и модификации сложных подмножеств значений
массива.

\1
In[1]: import numpy as np

\1np.random.RandomState(42)

\1nd.randint(100,
\1nt(x)
[51 92 14 71 60 20 82 86 74 74]

\1
In[2]: [x[3], x[7], x[2]]
Out[2]: [71, 86, 14]

\1
In[3]:
\1nd]
Out[3]: array([71, 86, 60])
В случае «прихотливой» индексации форма результата отражает форму массивов
индексов (index arrays), а не форму индексируемого массива:
In[4]:
\1np.array([[3, 7],
                       [4, 5]])
       x[ind]

============================================================

\1
In[5]:
\1np.arange(12).reshape((3, 4))
       X
Out[5]: array([[ 0,  1,  2,  3],
               [ 4,  5,  6,  7],
               [ 8,  9, 10, 11]])

\1
In[6]:
\1np.array([0, 1, 2])

\1np.array([2, 1, 3])
       X[row, col]
Out[6]: array([ 2,  5, 11])

\1
In[7]: X[row[:, np.newaxis], col]
Out[7]: array([[ 2,  1,  3],
               [ 6,  5,  7],
               [10,  9, 11]])

\1
In[8]: row[:, np.newaxis] * col
Out[8]: array([[0, 0, 0],
               [2, 1, 3],
               [4, 2, 6]])

\1

============================================================
СТРАНИЦА 110
============================================================

\1n[9]: print(X)
[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]]

\1
In[10]: X[2, [2, 0, 1]]
Out[10]: array([10,  8,  9])

\1
In[11]: X[1:, [2, 0, 1]]
Out[11]: array([[ 6,  4,  5],
                [10,  8,  9]])

\1
In[12]:
\1np.array([1, 0, 1, 0],
\1np.newaxis], mask]
Out[12]: array([[ 0,  2],
                [ 4,  6],
                [ 8, 10]])

\1
In[13]:
\1nd.multivariate_normal(mean, cov, 100)
        X.shape
Out[13]: (100, 2)

\1

============================================================
СТРАНИЦА 111
============================================================
«Прихотливая» индексация  111
In[14]: %matplotlib inline
        import matplotlib.pyplot as plt
        import seaborn; seaborn.set()  # for plot styling
        plt.scatter(X[:, 0], X[:, 1]);

\1
In[15]:
\1np.random.choice(X.shape[0], 20,
\1ndices
Out[15]: array([93, 45, 73, 81, 50, 10, 98, 94,  4, 64, 65, 89, 47, 84, 82,
                80, 25, 90, 63, 20])
In[16]:
\1ndices]  # Тут используется «прихотливая» индексация
        selection.shape
Out[16]: (20, 2)

\1мы большие круги в местах расположения выбранных точек (рис. 2.8).
In[17]: plt.scatter(X[:, 0], X[:, 1],
\1n[:, 0], selection[:, 1],

\1none',
\1\1n[18]:
\1np.arange(10)

\1np.array([2, 1, 8, 4])
        x[i] = 99
        print(x)
[ 0 99 99  3 99  5  6  7 99  9]

\1
In[19]: x[i] -= 10
        print(x)
[ 0 89 89  3 89  5  6  7 89  9]

============================================================

\1
In[20]:
\1np.zeros(10)
        x[[0, 0]] = [4, 6]
        print(x)
[ 6.  0.  0.  0.  0.  0.  0.  0.  0.  0.]

\1
In[21]:
\1\1
In[22]:
\1np.zeros(10)
        np.add.at(x, i, 1)
        print(x)
[ 0.  0.  1.  2.  3.  0.  0.  0.  0.  0.]

\1строения гистограммы вручную. Например, пусть у нас есть 1000 значений и нам

============================================================
СТРАНИЦА 114
============================================================

\1n[23]: np.random.seed(42)

\1np.random.randn(100)
        # Рассчитываем гистограмму вручную

\1np.linspace(-5, 5, 20)

\1np.zeros_like(bins)
        # Ищем подходящий интервал для каждого x

\1np.searchsorted(bins, x)
        # Добавляем 1 к каждому из интервалов
        np.add.at(counts, i, 1)

\1
In[24]: # Визуализируем результаты
        plt.plot(bins, counts,
\1\1
plt.hist(x, bins,
\1np.histogram , выполняющую вычисления, очень похожие на сделанные нами.

\1
In[25]: print("NumPy routine:")
        %timeit counts,
\1np.histogram(x, bins)
        print("Custom routine:")
        %timeit np.add.at(counts, np.searchsorted(bins, x), 1)
NumPy routine:
10000 loops, best of 3: 97.6 µs per loop
Custom routine:
10000 loops, best of 3: 19.5 µs per loop
Наш собственный однострочный алгоритм работает в несколько раз быстрее, чем
оптимизированный алгоритм из библиотеки
\1np.histogram (в оболочке IPython это можно
сделать, введя команду np.histogram??), то увидим, что она гораздо сложнее про -
стого поиска-и-подсчета, выполненного нами. Дело в том, что алгоритм из библи -
отеки NumPy более гибок, потому что разработан с ориентацией на более высокую
производительность при значительном увеличении количества точек данных:
In[26]:
\1np.random.randn(1000000)
        print("NumPy routine:")
        %timeit counts,
\1np.histogram(x, bins)
        print("Custom routine:")
        %timeit np.add.at(counts, np.searchsorted(bins, x), 1)
NumPy routine:
10 loops, best of 3: 68.7 ms per loop
Custom routine:
10 loops, best of 3: 135 ms per loop

\1большого”» далее). Но преимущество самостоятельного программирования
этого алгоритма заключается в том, что, получив понимание работы подобных
простых методов, вы сможете «строить» из этих «кирпичиков» очень интересные
варианты пользовательского поведения. Ключ к эффективному использованию
языка Python в приложениях, требующих обработки больших о бъемов данных,
заключается в том, чтобы знать о существовании удобных процедур, таких как
np.histogram, и сферах их использования. Кроме того, нужно знать, как применять
низкоуровневую функциональность при необходимости в узконаправленном по-
ведении.

============================================================
СТРАНИЦА 116
============================================================

\1nsertion sort) многократно находит
минимальное значение из списка и выполняет перестановки до тех пор, пока
список не будет отсортирован. Это можно запрограммировать с помощью всего
нескольких строк кода на языке Python:
In[1]: import numpy as np
       def selection_sort(x):
           for i in range(len(x)):

\1np.argmin(x[i:])
               (x[i], x[swap]) = (x[swap], x[i])
           return x
In[2]:
\1np.array([2, 1, 4, 3, 5])
       selection_sort(x)
Out[2]: array([1, 2, 3, 4, 5])

\1
In[3]: def bogosort(x):
           while np.any(x[:-1] > x[1:]):
               np.random.shuffle(x)
           return
\1n[4]:
\1np.array([2, 1, 4, 3, 5])
       bogosort(x)
Out[4]: array([1, 2, 3, 4, 5])

\1кратно перетасовывает массив случайным образом до т ех пор, пока результат не
окажется отсортированным. При средней сложности порядка O [ N  ×
\1n имеются намного более эффективные встроенные алгоритмы сортировки.
Начнем с изучения встроенных алгоритмов языка Python, после чего рассмотрим ути-
литы, включенные в библиотеку NumPy и оптимизированные под NumPy-массивы.
Быстрая сортировка в библиотеке NumPy: функции
np.sort и np.argsort
Хотя в языке Python имеются встроенные функции sort и sorted для работы со
списками, мы не будем их рассматривать, поскольку функция библиотеки NumPy
np.sort оказывается намного более эффективной и подходящей для наших целей.
По умолчанию функция np.sort  использует имеющий сложность O [ N log N ]:
алгоритм быстрой сортировки  (quicksort), хотя доступны для использования
также алгоритмы сортировки слиянием (mergesort) и пирамидальной сортировки
(heapsort). Для большинства приложений используемой по умолчанию быстрой
сортировки более чем достаточно.
Чтобы получить отсортированную версию входного массива без его изменения,
можно использовать функцию np.sort:
In[5]:
\1np.array([2, 1, 4, 3, 5])
       np.sort(x)
Out[5]: array([1, 2, 3, 4, 5])

\1
In[6]: x.sort()
       print(x)
[1 2 3 4 5]

\1
In[7]:
\1np.array([2, 1, 4, 3, 5])

\1np.argsort(x)
       print(i)
[1 0 3 2 4]

============================================================
СТРАНИЦА 118
============================================================

\1n[8]: x[i]
Out[8]: array([1, 2, 3, 4, 5])

\1
In[9]:
\1np.random.RandomState(42)

\1nd.randint(0, 10, (4, 6))
       print(X)
[[6 3 7 4 6 9]
 [2 6 7 4 3 7]
 [7 2 5 4 1 7]
 [5 1 4 0 9 5]]
In[10]: # Сортируем все столбцы массива X
        np.sort(X,
\1n[11]: # Сортируем все строки массива X
        np.sort(X,
\1np.partition. Функция np.partition принимает на входе массив и число K.

\1

============================================================
СТРАНИЦА 119
============================================================
Сортировка массивов  119
In[12]:
\1np.array([7, 2, 3, 1, 6, 5, 4])
        np.partition(x, 3)
Out[12]: array([2, 1, 3, 4, 6, 5, 7])

\1
In[13]: np.partition(X, 2,
\1np.argsort, вычисляющей индексы для сортировки,
существует функция np.argpartition, вычисляющая индексы для секции. Мы уви-
дим ее в действии в следующем разделе.
Пример: K ближайших соседей
Давайте вкратце рассмотрим, как можно использовать функцию np.argpartition
по нескольким осям для поиска ближайших соседей каждой точки из определен-
ного набора. Начнем с создания случайного набора из десяти точек на двумерной
плоскости. По стандартным соглашениям образуем из них массив 10 × 2:
In[14]:
\1nd.rand(10, 2)

\1
In[15]: %matplotlib inline
        import matplotlib.pyplot as plt
        import seaborn; seaborn.set() # Plot styling
        plt.scatter(X[:, 0], X[:, 1],
\1\1n[16]:
\1np.sum((X[:,np.newaxis,:] - X[np.newaxis,:,:])
                         ** 2,
\1\1
In[17]: # Для каждой пары точек вычисляем разности их координат

\1np.newaxis, :] - X[np.newaxis, :, :]
        differences.shape
Out[17]: (10, 10, 2)
In[18]: # Возводим разности координат в квадрат

\1nces ** 2
        sq_differences.shape
Out[18]: (10, 10, 2)
In[19]: # Суммируем квадраты разностей координат
        # для получения квадрата расстояния

\1nces.sum(-1)
        dist_sq.shape
Out[19]: (10, 10)

\1

============================================================
СТРАНИЦА 121
============================================================
Сортировка массивов  121
In[20]: dist_sq.diagonal()
Out[20]: array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])

\1тыми попарно точками, мы можем воспользоваться функцией np.argsort  для
сортировки по каждой строке. Крайние слева столбцы будут представлять собой
индексы ближайших соседей:
In[21]:
\1np.argsort(dist_sq,
\1nt(nearest)
[[0 3 9 7 1 4 2 5 6 8]
 [1 4 7 9 3 6 8 5 0 2]
 [2 1 4 6 3 0 8 9 7 5]
 [3 9 7 0 1 4 5 8 6 2]
 [4 1 8 5 6 7 9 3 0 2]
 [5 8 6 4 1 7 9 3 2 0]
 [6 8 5 4 1 7 9 3 2 0]
 [7 9 3 1 4 0 5 8 6 2]
 [8 5 6 4 1 7 9 3 2 0]
 [9 7 3 0 1 4 5 8 6 2]]

\1она сама, как и можно было ожидать.

\1совали K ближайших соседей, было достаточно секционировать все строки так,
чтобы сначала шли K+1 минимальных квадратов расстояний, а большие расстояния
заполняли оставшиеся позиции массива. Сделать это можно с помощью функции
np.argpartition:
In[22]:
\1
\1np.argpartition(dist_sq, K + 1,
\1\1
In[23]: plt.scatter(X[:, 0], X[:, 1],
\1n range(X.shape[0]):
            for j in nearest_partition[i, :K+1]:

============================================================
СТРАНИЦА 122
============================================================

\1n весьма эффективен.

\1K-мерное дерево (KD-tree),
реализованное в библиотеке Scikit-Learn.

============================================================

\1средство, позволяющее описывать рост числа опера -
ций, необходимых для выполнения алгоритма, по мере роста объема входных дан-
ных. Для правильного использования нужно немного углубиться в теорию вы -
числительной техники и уметь отличать данную нотацию от родственных нотаций
«о-маленького», « θ-большого», « Ω-большого» и, вероятно, множества их гибридов.

\1тается «порядка N ») выполняется 1 секунду при работе со списком длины
\1\1ndas, которые мы рассмотрим в главе 3.
Пускай у нас имеется несколько категорий данных (например, имя, возраст и вес)
о нескольких людях и мы хотели бы хранить эти значения для использования
в программе на языке Python. Можно сохранить их в отдельных массивах:
In[2]:
\1\1
In[3]:
\1np.zeros(4,
\1nt)

\1
In[4]: # Используем для структурированного массива составной тип данных

\1np.zeros(4,
\1names':('name', 'age', 'weight'),
                                 'formats':('U10', 'i4', 'f8')})
       print(data.dtype)
[('name', '<U10'), ('age', '<i4'), ('weight', '<f8')]
'U10' означает «строку в кодировке Unicode максимальной длины 10», 'i4' —
«4-байтное (то есть 32-битное) целое число», а 'f8' — «8-байтное (то есть 64-бит-
ное) число с плавающей точкой». Мы обсудим другие варианты подобного коди-
рования типов в следующем разделе.

\1
In[5]: data['name'] = name
       data['age'] = age
       data['weight'] = weight
       print(data)
[('Alice', 25, 55.0) ('Bob', 45, 85.5) ('Cathy', 37, 68.0)
 ('Doug', 19, 61.5)]

============================================================

\1
In[6]: # Извлечь все имена
       data['name']
Out[6]: array(['Alice', 'Bob', 'Cathy', 'Doug'],

\1n[7]: # Извлечь первую строку данных
       data[0]
Out[7]: ('Alice', 25, 55.0)
In[8]: # Извлечь имя из последней строки
       data[-1]['name']
Out[8]: 'Doug'

\1
In[9]: # Извлечь имена людей с возрастом менее 30
       data[data['age'] < 30]['name']
Out[9]: array(['Alice', 'Doug'],

\1ndas,
который будет рассмотрен в главе 3. Библиотека Pandas предоставляет объект
DataFrame — основанную на массивах библиотеки NumPy структуру, обладающую
массой полезной функциональности по работе с данными.

\1
In[10]: np.dtype({'names':('name', 'age', 'weight'),
                  'formats':('U10', 'i4', 'f8')})
Out[10]: dtype([('name', '<U10'), ('age', '<i4'), ('weight', '<f8')])
Для ясности можно задавать числовые типы как с прим енением типов данных
языка Python, так и типов dtype библиотеки NumPy:

============================================================
СТРАНИЦА 126
============================================================

\1n[11]: np.dtype({'names':('name', 'age', 'weight'),
                  'formats':((np.str_, 10), int, np.float32)})
Out[11]: dtype([('name', '<U10'), ('age', '<i8'), ('weight', '<f4')])

\1
In[12]: np.dtype([('name', 'S10'), ('age', 'i4'), ('weight', 'f8')])
Out[12]: dtype([('name', 'S10'), ('age', '<i4'), ('weight', '<f8')])

\1
In[13]: np.dtype('S10,i4,f8')
Out[13]: dtype([('f0', 'S10'), ('f1', '<i4'), ('f2', '<f8')])

\1< или >,
означает «число с прямым порядком байтов» или «число с обратным порядком бай-
тов» соответственно и задает порядок значащих битов. Следующий символ задает
тип данных: символы, байтовый тип, целые числа, числа с плавающей точкой и т. д.
(табл. 2.6). Последний символ или символы отражают размер объекта в байтах.
Таблица 2.6. Типы данных библиотеки NumPy
Символ Описание Пример
'b' Байтовый тип np.dtype('b')
'i' Знаковое целое число np.dtype('i4') == np.int32
'u' Беззнаковое целое число np.dtype('u1') == np.uint8
'f ' Число с плавающей точкой np.dtype('f8') == np.int64
'c' Комплексное число с плавающей точкой np.dtype('c16') == np.complex128
'S', 'a' Строка np.dtype('S5')
'U' Строка в кодировке Unicode np.dtype('U') == np.str_
'V' Неформатированные данные (тип void) np.dtype('V') == np.void

\1
In[14]:
\1np.dtype([('id', 'i8'), ('mat', 'f8', (3, 3))])

\1np.zeros(1,
\1nt(X[0])
        print(X['mat'][0])
(0, [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])
[[ 0.  0.  0.]
 [ 0.  0.  0.]
 [ 0.  0.  0.]]

\1чему такой массив может оказаться предпочтительнее, чем простой многомерный
массив или, возможно, словарь языка Python? Дело в том, что dtype библиотеки
NumPy напрямую соответствует описанию структуры из языка C, так что можно
обращаться к содержащему этот массив буферу памяти непосредственно из соот-
ветствующим образом написанной программы на языке C. Если вам понадобится
написать на языке Python интерфейс к уже существующей библиотеке на язы -
ке C или Fortran, которая работает со структурирова нными данными, вероятно,
структурированные массивы будут вам весьма полезны!
Массивы записей: структурированные массивы
с дополнительными возможностями
Библиотека NumPy предоставляет класс np.recarray, практически идентичный
только что описанным структурированным массивам, но с одной дополнительной
возможностью: доступ к полям можно осуществлять как к атрибутам, а не только
как к ключам словаря. Как вы помните, ранее мы обращались к значениям возраста
путем написания следующей строки кода:
In[15]: data['age']
Out[15]: array([25, 45, 37, 19],
\1nt32)

\1
In[16]:
\1np.recarray)
        data_rec.age
Out[16]: array([25, 45, 37, 19],
\1nt32)

\1
In[17]: %timeit data['age']
        %timeit data_rec['age']
        %timeit data_rec.
\1\1ns per loop
100000 loops, best of 3: 4.61 µs per loop
100000 loops, best of 3: 7.27 µs per loop

\1зависит от вашего приложения.
Вперед, к Pandas

\1библиотеки Pandas. В определенных случаях не поме -
шает знать о существовании обсуждавшихся здесь структурированных массивов,
особенно если вам нужно, чтобы массивы библиотеки NumPy соответствовали
двоичным форматам данных в C, Fortran или другом языке программирования.

\1пользовать пакет Pandas, который мы подробно рассмотрим в следующей главе.

============================================================
СТРАНИЦА 129
============================================================

\1ndas
В предыдущей главе мы рассмотрели библиотеку NumPy и ее объект ndarray, обе -
спечивающий эффективное хранение плотных массивов и манипуляции над ними
в Python. В этой главе мы, основываясь на этих знаниях, детально ознакомимся со
структурами данных библиотеки Pandas.
Pandas — более новый пакет, надстройка над библиотекой NumPy, обеспечивающий
эффективную реализацию класса DataFrame. Объекты DataFrame — многомерные
массивы с метками для строк и столбцов, а также зачастую с неоднородным типом
данных и/или пропущенными данными. Помимо удобного интерфейса для хра -
нения маркированных данных, библиотека Pandas реализует множество операций
для работы с данными хорошо знакомых пользователям фреймворков баз данных
и электронных таблиц.
Структура данных ndarray библиотеки NumPy предоставляет все необходимые
возможности для работы с хорошо упорядоченными данными в задачах численных
вычислений. Для этой цели библиотека NumPy отлично подходит, однако имеет
свои ограничения, которые становятся заметными, чуть только нам потребуется не-
много больше гибкости (маркирование данных, работа с пропущенными данными
и т. д.). Эти ограничения проявляются также при попытках выполнения опера -
ций, неподходящих для поэлементного транслирования (группировки, создание
сводных таблиц и т. д.). Такие операции являются важной частью анализа данных
с меньшей степенью структурированности, содержащихся во многих формах
окружающего мира. Библиотека Pandas, особенно ее объекты Series и DataFrame,
основана на структурах массивов библиотеки NumPy и обеспечивает эффективную
работу над подобными задачами «очистки данных».

============================================================
СТРАНИЦА 130
============================================================

\1ndas
В этой главе мы сосредоточимся на стандартных приемах использования объектов
Series, DataFrame и связанных с ними структур. По мере возможности мы будем
применять взятые из реальных наборов данных примеры, но они не являются
нашей целью.
Установка и использование библиотеки Pandas
Для установки пакета Pandas необходимо наличие в вашей системе пакета
NumPy, а если вы выполняете сборку библиотеки из исходного кода, то и соот -
ветствующих утилит для компиляции исходных кодов на языках С и Cython,
из которых состоит Pandas. Подробные инструкции по установке можно найти
в документации пакета Pandas ( http://pandas.pydata.org/).  Если же вы последовали
совету из предисловия и воспользовались стеком Anaconda, то пакет Pandas
у вас уже имеется.
После установки пакета Pandas можно импортировать его и проверить версию:
In[1]: import pandas
       pandas.__version__
Out[1]: '0.18.1'
Аналогично тому, как мы импортировали пакет NumPy под псевдонимом np, пакет
Pandas импортируем под псевдонимом pd:
In[2]: import pandas as pd
Мы будем использовать эти условные обозначения для импорта далее в книге.
Напоминание о встроенной документации
Оболочка IPython предоставляет возможность быстро просматривать содержимое
пакетов (с помощью клавиши Tab), а также документацию по различным функциям
(используя символ ?). Загляните в раздел «Справка и документация в оболочке
Python» главы 1, если вам нужно освежить в памяти эти возможности.
Для отображения всего содержимого пространства имен numpy можете ввести сле-
дующую команду:
In [3]: np.<
\1\1
In [4]: np?

\1ции можно найти на сайте http://pandas.pydata.org.

============================================================
СТРАНИЦА 131
============================================================
Знакомство с объектами библиотеки Pandas  131
Знакомство с объектами библиотеки Pandas
На самом примитивном уровне объекты библиотеки Pandas можно считать рас -
ширенной версией структурированных массивов библиотеки NumPy, в которых
строки и столбцы идентифицируются метками, а не простыми числовыми инде-
ксами. Библиотека Pandas предоставляет множество полезных утилит, методов
и функциональности  в дополнение к базовым структурам данных, но все по -
следующее изложение потребует понимания этих базовых структур. Позвольте
познакомить  вас с тремя фундаментальными структурами данных библиотеки
Pandas: классами Series, DataFrame и Index.
Начнем наш сеанс программирования с обычных импортов библиотек NumPy
и Pandas:
In[1]: import numpy as np
       import pandas as pd
Объект Series библиотеки Pandas
Объект Series библиотеки Pandas — одномерный массив индексированных дан -
ных. Его можно создать из списка или массива следующим образом:
In[2]:
\1ndex. Атрибут values
представляет собой уже знакомый нам массив NumPy:
In[3]: data.values
Out[3]: array([ 0.25,  0.5 ,  0.75,  1.  ])
index — массивоподобный объект типа pd.Index, который мы рассмотрим подробнее
далее:
In[4]: data.index
Out[4]: RangeIndex(
\1\1ndas

\1ветствующему им индексу посредством нотации с использованием квадратных
скобок языка Python:
In[5]: data[1]
Out[5]: 0.5
In[6]: data[1:3]
Out[6]: 1 | 0.50
2 | 0.75
        dtype: float64
Однако объект Series библиотеки Pandas намного универсальнее и гибче, чем
эмулируемый им одномерный массив библиотеки NumPy.

\1целочисленный
и описывается неявно, индекс объекта Series библиотеки Pandas описывается явно
и связывается со значениями.

\1
In[7]:
\1
\1\1
In[8]: data['b']
Out[8]: 0.5

\1

============================================================
СТРАНИЦА 133
============================================================
Знакомство с объектами библиотеки Pandas  133
In[9]:
\1
\1n[10]: data[5]
Out[10]: 0.5
Объект Series как специализированный словарь
Объект Series библиотеки Pandas можно рассматривать как специализированную
разновидность словаря языка Python. Словарь — структура, задающая соответствие
произвольных ключей набору произвольных значений, а объект Series — струк -
тура, задающая соответствие типизированных ключей набору типизированных
значений. Типизация важна: точно так же, как соответствующий типу специали-
зированный код для массива библиотеки NumPy при выполнении определенных
операций делает его эффективнее, чем стандартный список Python, информация
о типе в объекте Series библиотеки Pandas делает его намного более эффективным
для определенных операций, чем словари Python.

\1словарь» еще более наглядной, скон-
струировав объект Series непосредственно из словаря Python:
In[11]:
\1nia': 38332521,
                           'Texas': 26448193,
                           'New York': 19651127,
                           'Florida': 19552860,
                           'Illinois': 12882135}

\1n_dict)
        population
Out[11]: California | 38332521
Florida | 19552860
Illinois | 12882135
New York | 19651127
Texas | 26448193
         dtype: int64

\1
In[12]: population['California']
Out[12]: 383 32521

============================================================
СТРАНИЦА 134
============================================================

\1ndas

\1
In[13]: population['California':'Illinois']
Out[13]: California | 38332521
Florida | 19552860
Illinois | 12882135
         dtype: int64
Мы рассмотрим некоторые нюансы индексации и срезов в библиотеке Pandas в раз -
деле «Индексация и выборка данных» этой главы.
Создание объектов Series
Мы уже изучили несколько способов создания объектов Series библиотеки Pandas
с нуля. Все они представляют собой различные варианты следующего синтаксиса:
>>> pd.Series(data,
\1ndex)
где index — необязательный аргумент, а data может быть одной из множества сущ-
ностей.
Например, аргумент data может быть списком или массивом NumPy. В этом случае
index по умолчанию будет целочисленной последовательностью:
In[14]: pd.Series([2, 4, 6])
Out[14]: 0 | 2
1 | 4
2 | 6
         dtype: int64

\1
In[15]: pd.Series(5,
\1nt64
Аргумент data может быть словарем, в котором index по умолчанию является от-
сортированными ключами этого словаря:
In[16]: pd.Series({2:'a', 1:'b', 3:'c'})
Out[16]: 1 | b
2 | a
3 | c
         dtype:
\1ndas  135

\1
In[17]: pd.Series({2:'a', 1:'b', 3:'c'},
\1ndas
Следующая базовая структура библиотеки Pandas — объ ект DataFrame. Как и объ-
ект Series , обсуждавшийся в предыдущем разделе, объект DataFrame  можно
рассматривать  или как обобщение массива NumPy, или как специализированную
версию словаря Python. Изучим оба варианта.
DataFrame как обобщенный массив NumPy

\1
In[18]:

\1nia': 423967, 'Texas': 695662, 'New York': 141297,
             'Florida': 170312, 'Illinois': 149995}

\1nia | 423967
Florida | 170312
Illinois | 149995
New York | 141297
Texas | 695662
         dtype: int64
Воспользовавшись объектом population класса Series, сконструируем на основе
словаря единый двумерный объект, содержащий всю эту информацию:

============================================================
СТРАНИЦА 136
============================================================

\1ndas
In[19]:
\1n': population,
                               'area': area})
        states
Out[19]: | area | population
California  423967 | 38332521
Florida | 170312 | 19552860
Illinois | 149995 | 12882135
New York | 141297 | 19651127
Texas | 695662 | 26448193
Аналогично объекту Series у объекта DataFrame имеется атрибут index, обеспечи-
вающий доступ к меткам индекса:
In[20]: states.index
Out[20]:
Index(['California', 'Florida', 'Illinois', 'New York', 'Texas'],

\1ns, представляющий собой
содержащий метки столбцов объект Index:
In[21]: states.columns
Out[21]: Index(['area', 'population'],
\1\1
In[22]: states['area']
Out[22]: California | 423967
Florida | 170312
Illinois | 149995
New York | 141297
Texas | 695662
         Name: area, dtype: int64
Обратите внимание на возможный источник путаницы: в двумерном массиве
NumPy data[0] возвращает первую строку. По этой причине объекты
\1ndas  137
лучше рассматривать как обобщенные словари, а не обобщенные массивы, хотя обе
точки зрения имеют право на жизнь. Мы изучим более гибкие средства индексации
объектов DataFrame в разделе «Индексация и выборка данных» этой главы.
Создание объектов DataFrame
Существует множество способов создания объектов DataFrame библиотеки Pandas.

\1
In[23]: pd.DataFrame(population,
\1n'])
Out[23]: | population
California | 38332521
Florida | 19552860
Illinois | 12882135
New York | 19651127
Texas | 26448193

\1
In[24]:
\1n range(3)]
        pd.DataFrame(data)
Out[24]: | a  b
         0  0  0
         1  1  2
         2  2  4
Даже если некоторые ключи в словаре отсутствуют, библиотека Pandas просто за -
полнит их значениями NaN (то есть Not a number — «не является числом»):
In[25]: pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])
Out[25]: | a | b  c
         0  1.0  2  NaN
         1  NaN  3  4.0

\1
In[26]: pd.DataFrame({'population': population,
                      'area': area})
Out[26]: | area | population
California  423967 | 38332521

============================================================
СТРАНИЦА 138
============================================================

\1ndas
Florida | 170312 | 19552860
Illinois | 149995 | 12882135
New York | 141297 | 19651127
Texas | 695662 | 26448193

\1
In[27]: pd.DataFrame(np.random.rand(3, 2),

\1
\1ndas ведет себя во
многом аналогично структурированному массиву и может быть создан непосред-
ственно из него:
In[28]:
\1np.zeros(3,
\1n[29]: pd.DataFrame(A)
Out[29]: | A  B
         0  0  0.0
         1  0  0.0
         2  0  0.0
Объект Index библиотеки Pandas

\1ющий возможность ссылаться на данные и модифицировать их. Объект Index
можно рассматривать или как неизменяемый массив  (immutable array), или как
упорядоченное множество  (ordered set) (формально мультимножество, так как
объекты Index могут содержать повторяющиеся значения). Из этих способов
его представления следуют некоторые интересные возможности операций над
объектами Index. В качестве простого примера создадим Index из списка целых
чисел:

============================================================
СТРАНИЦА 139
============================================================
Знакомство с объектами библиотеки Pandas  139
In[30]:
\1ndex([2, 3, 5, 7, 11])
        ind
Out[30]: Int64Index([2, 3, 5, 7, 11],
\1nt64')
Объект Index как неизменяемый массив
Объект Index во многом ведет себя аналогично массиву. Например, для извлечения
из него значений или срезов можно использовать стандартную нотацию индекса-
ции языка Python:
In[31]: ind[1]
Out[31]: 3
In[32]: ind[::2]
Out[32]: Int64Index([2, 5, 11],
\1nt64')
У объектов Index есть много атрибутов, знакомых нам по массивам NumPy:
In[33]: print(ind.size, ind.shape, ind.ndim, ind.dtype)
5 (5,) 1 int64
Одно из различий между объектами Index и массивами NumPy — неизменяемость
индексов, то есть их нельзя модифицировать стандартными средствами:
In[34]: ind[1] = 0
---------------------------------------------------------------------------
TypeError | Traceback (most recent call last)
<ipython-input-34-40
\1
\1nd[1] = 0
/Users/jakevdp/anaconda/lib/python3.5/site-packages/pandas/indexes/base.py ...
   1243
1244 | def __setitem__(self, key, value):
-> 1245 | raise TypeError("Index does not support mutable operations")
   1246
1247 | def __getitem__(self, key):
TypeError: Index does not support mutable operations

\1тов в виде случайной модификации индекса по неосторожности.

============================================================
СТРАНИЦА 140
============================================================

\1ndas
Index как упорядоченное множество
Объекты библиотеки Pandas спроектированы с прицелом на упрощение таких
операций, как соединения наборов данных, зависящие от многих аспектов ариф -
метики множеств. Объект Index следует большинству соглашений, использу -
емых встроенной структурой данных set языка Python, так что объединения,
пересечения, разности и другие операции над множествами м ожно выполнять
привычным образом:
In[35]:
\1ndex([1, 3, 5, 7, 9])

\1ndex([2, 3, 5, 7, 11])
In[36]: indA & indB  # пересечение
Out[36]: Int64Index([3, 5, 7],
\1nt64')
In[37]: indA | indB  # объединение
Out[37]: Int64Index([1, 2, 3, 5, 7, 9, 11],
\1nt64')
In[38]: indA ^ indB  # симметричная разность
Out[38]: Int64Index([1, 2, 9, 11],
\1nt64')
Эти операции можно выполнять также методами объектов, например indA.in-
tersection(indB).

\1менения значений в массивах библиотеки NumPy: индексацию ( arr[2, 1]), срезы
массивов ( arr[:,1:5]), маскирование ( arr[
\1ndas. Если вы использовали паттерны библиотеки NumPy, то соот-
ветствующие паттерны библиотеки Pandas будут для вас привычны.
Начнем с простого случая одномерного объекта Series, после чего перейдем к более
сложному двумерному объекту DataFrame.
Выборка данных из объекта Series
Объект Series во многом ведет себя подобно одномерному массиву библиотеки
NumPy и стандартному словарю языка Python. Это поможет нам лучше понимать
паттерны индексации и выборки данных из этих массивов.

============================================================

\1
In[1]: import pandas as pd

\1
\1n[2]: data['b']
Out[2]: 0.5

\1вать методы языка Python, аналогичные таковым для словарей:
In[3]: 'a' in data
Out[3]: True
In[4]: data.keys()
Out[4]: Index(['a', 'b', 'c', 'd'],
\1n[5]: list(data.items())
Out[5]: [('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)]

\1
In[6]: data['e'] = 1.25
       data
Out[6]: a | 0.25
b | 0.50
c | 0.75
d | 1.00
e | 1.25
        dtype: float64

\1удобная возможность: библиотека Pandas
сама, незаметно для нас, принимает решения о размещении в памяти и необходимости

============================================================
СТРАНИЦА 142
============================================================

\1ndas
копирования данных. Пользователю, как правило, не приходится заботиться о по -
добных вопросах.

\1
In[7]: # срез посредством явного индекса
       data['a':'c']
Out[7]: a | 0.25
b | 0.50
c | 0.75
        dtype: float64
In[8]: # срез посредством неявного целочисленного индекса
       data[0:2]
Out[8]: a | 0.25
b | 0.50
        dtype: float64
In[9]: # маскирование
       data[(
\1n[10]: # «прихотливая» индексация
        data[['a', 'e']]
Out[10]: a | 0.25
e | 1.25
         dtype: float64

\1неявный индекс в стиле языка Python.
In[11]:
\1
\1n[12]: # Использование явного индекса при индексации
        data[1]
Out[12]: 'a'
In[13]: # Использование неявного индекса при срезе
        data[1:3]
Out[13]: 3 | b
5 | c
         dtype: object

\1теке Pandas предусмотрены специальные атрибуты- индексаторы , позволяющие
явным образом применять определенные схемы индексации. Они являются не
функциональными методами, а именно атрибутами, предоставляющими для дан-
ных из объекта Series определенный интерфейс для выполнения срезов.

\1
In[14]: data.loc[1]
Out[14]: 'a'
In[15]: data.loc[1:3]
Out[15]: 1 | a
3 | b
         dtype: object
Атрибут iloc дает возможность выполнить индексацию и срезы, применяя неявный
индекс в стиле языка Python:
In[16]: data.iloc[1]
Out[16]: 'b'
In[17]: data.iloc[1:3]
Out[17]: 3 |
\1\1ndas
5 | c
         dtype: object

\1смотрим далее.
Один из руководящих принципов написания кода на языке Python — «лучше явно,
чем неявно». То, что атрибуты loc и iloc по своей природе явные, делает их очень
удобными для обеспечения «чистоты» и удобочитаемости кода. Я рекомендую
использовать оба, особенно в случае целочисленных индексов, чтобы сделать код
более простым для чтения и понимания и избежать слу чайных малозаметных
ошибок при обозначении индексации и срезов.

\1
In[18]:
\1nia': 423967, 'Texas': 695662,
                          'New York': 141297, 'Florida': 170312,
                          'Illinois': 149995})

\1nia': 38332521, 'Texas': 26448193,
                         'New York': 19651127, 'Florida': 19552860,
                         'Illinois': 12882135})

\1nia  423967  38332521
Florida | 170312  19552860
Illinois | 149995  12882135
New York | 141297  19651127
Texas | 695662  26448193

\1

============================================================
СТРАНИЦА 145
============================================================
Индексация и выборка данных  145
In[19]: data['area']
Out[19]: California | 423967
Florida | 170312
Illinois | 149995
New York | 141297
Texas | 695662
         Name: area, dtype: int64

\1
In[20]: data.area
Out[20]: California | 423967
Florida | 170312
Illinois | 149995
New York | 141297
Texas | 695662
         Name: area, dtype: int64

\1
In[21]: data.area is data['area']
Out[21]: True

\1
In[22]: data.pop is data['pop']
Out[22]: False

\1
In[23]: data['density'] = data['pop'] / data['area']
        data
Out[23]: | area | pop | density
California  423967  38332521 | 90.413926
Florida | 170312  19552860  114.806121

============================================================
СТРАНИЦА 146
============================================================

\1ndas
Illinois | 149995  12882135 | 85.883763
New York | 141297  19651127  139.076746
Texas | 695662  26448193 | 38.018740

\1ции над данными в библиотеке Pandas» текущей главы.

\1
In[24]: data.values
Out[24]: array([[  4.23967000e+05, | 3.83325210e+07, | 9.04139261e+01],
[  1.70312000e+05, | 1.95528600e+07, | 1.14806121e+02],
[  1.49995000e+05, | 1.28821350e+07, | 8.58837628e+01],
[  1.41297000e+05, | 1.96511270e+07, | 1.39076746e+02],
[  6.95662000e+05, | 2.64481930e+07, | 3.80187404e+01]])

\1
In[25]: data.T
Out[25]:
California | Florida | Illinois | New York | Texas
area | 4.239670e+05  1.703120e+05  1.499950e+05  1.412970e+05  6.956620e+05
pop | 3.833252e+07  1.955286e+07  1.288214e+07  1.965113e+07  2.644819e+07
density  9.041393e+01  1.148061e+02  8.588376e+01  1.390767e+02  3.801874e+01

\1
In[26]: data.values[0]
Out[26]: array([  4.23967000e+05, | 3.83325210e+07, | 9.04139261e+01])
а указание отдельного «индекса» для объекта DataFrame — доступ к столбцу:
In[27]: data['area']
Out[27]: California | 423967
Florida | 170312

============================================================
СТРАНИЦА 147
============================================================
Индексация и выборка данных  147
Illinois | 149995
New York | 141297
Texas | 695662
         Name: area, dtype: int64

\1гичной по стилю индексации массивов. Библиотека Pandas применяет упомянутые
ранее индексаторы loc, iloc и ix. С помощью индексатора iloc можно индексиро-
вать исходный массив, как будто это простой массив NumPy (используя неявный
синтаксис языка Python), но с сохранением в результирующих данных меток объ -
екта DataFrame для индекса и столбцов:
In[28]: data.iloc[:3, :2]
Out[28]: | area | pop
         California  423967  38332521
Florida | 170312  19552860
Illinois | 149995  12882135
In[29]: data.loc[:'Illinois', :'pop']
Out[29]: | area | pop
         California  423967  38332521
Florida | 170312  19552860
Illinois | 149995  12882135

\1
In[30]: data.ix[:3, :'pop']
Out[30]: | area | pop
         California  423967  38332521
Florida | 170312  19552860
Illinois | 149995  12882135

\1
In[31]: data.loc[data.
\1nsity']]
Out[31]: | pop | density
Florida | 19552860  114.806121
         New York  19651127  139.076746

============================================================
СТРАНИЦА 148
============================================================

\1ndas

\1
In[32]: data.iloc[0, 2] = 90
        data
Out[32]: | area | pop | density
California  423967  38332521 | 90.000000
Florida | 170312  19552860  114.806121
Illinois | 149995  12882135 | 85.883763
New York | 141297  19651127  139.076746
Texas | 695662  26448193 | 38.018740
Чтобы достичь уверенности при манипуляции данными с помощью библиотеки
Pandas, рекомендую потратить немного времени на экс перименты над простым
объектом DataFrame и пробы типов индексации, срезов, маскирования и «прихот-
ливой» индексации.

\1
In[33]: data['Florida':'Illinois']
Out[33]: | area | pop | density
Florida | 170312  19552860  114.806121
Illinois  149995  12882135 | 85.883763

\1
In[34]: data[1:3]
Out[34]: | area | pop | density
Florida | 170312  19552860  114.806121
Illinois  149995  12882135 | 85.883763

\1
In[35]: data[data.
\1nsity
Florida | 170312  19552860  114.806121
         New York  141297  19651127  139.076746
Эти два варианта обозначений синтаксически подобны таковым для массивов
библиотеки NumPy, и они, возможно, хоть и не вполне вписываются в шаблоны
синтаксиса библиотеки Pandas, но весьма удобны на практике.

============================================================
СТРАНИЦА 149
============================================================
Операции над данными в библиотеке Pandas  149
Операции над данными в библиотеке Pandas

\1как простейшие арифметические (сложение,
вычитание, умножение и т. д.), так и более сложные (тр игонометрические, по -
казательные и логарифмические функции и т. п.). Библиотека Pandas наследует
от NumPy немалую часть этой функциональности, и ключ к ее использованию —
универсальные функции, с которыми мы познакомились в разделе «Выполне -
ние вычислений над массивами библиотеки NumPy: универсальные функции»
главы 2.
Однако библиотека Pandas включает несколько полезных трюков: для унарных
операций, например изменения знака и тригонометрических функций, при ис -
пользовании ее универсальных функций в выводе будут сохранены индекс и метки
столбцов, а для бинарных операций, например сложения и умножения, библиотека
Pandas будет автоматически выравнивать индексы при передаче объектов универ-
сальной функции. Это значит, что сохранение контекста данных и объединение
данных из различных источников — две задачи, потенциально чреватые ошибками
при работе с исходными массивами библиотеки NumPy, — становятся надежно
защищенными от ошибок благодаря библиотеке Pandas. Кроме того, в библиотеке
заданы операции между одномерными структурами объектов Series и двумерными
структурами объектов DataFrame.
Универсальные функции: сохранение индекса
В силу того что библиотека Pandas предназначена для работы с библиотекой
NumPy, все универсальные функции библиотеки NumPy будут работать с объек-
тами Series и DataFrame библиотеки Pandas. Начнем с описания простых объектов
Series и DataFrame для демонстрации этого:
In[1]: import pandas as pd
       import numpy as np
In[2]:
\1np.random.RandomState(42)

\1ng.randint(0, 10, 4))
       ser
Out[2]: 0 | 6
1 | 3
2 | 7
3 | 4
        dtype: int64
In[3]:
\1ng.randint(0, 10, (3, 4)),

\1\1ndas
Out[3]: | A  B  C  D
        0  6  9  2  6
        1  7  4  3  7
        2  7  2  5  4

\1зультатом будет другой объект библиотеки Pandas с сохранением индексов:
In[4]: np.exp(ser)
Out[4]: 0 | 403.428793
1 | 20.085537
2 | 1096.633158
3 | 54.598150
        dtype: float64

\1
In[5]: np.sin(df * np.pi / 4)
Out[5]: | A | B | C | D
        0 -1.000000  7.071068e-01  1.000000 -1.000000e+00
        1 -0.707107  1.224647e-16  0.707107 -7.071068e-01
        2 -0.707107  1.000000e+00 -0.707107  1.224647e-16

\1пользовать аналогично вышеприведенным.
Универсальные функции: выравнивание индексов
При бинарных операциях над двумя объектами Series или DataFrame библиотека
Pandas будет выравнивать индексы в процессе выполнения операции. Это очень
удобно при работе с неполными данными.

\1
In[6]:
\1nia': 423967},
\1
\1nia': 38332521, 'Texas': 26448193,
                               'New York': 19651127},
\1n')

\1

============================================================
СТРАНИЦА 151
============================================================
Операции над данными в библиотеке Pandas  151
In[7]: population / area
Out[7]: Alaska | NaN
California | 90.413926
New York | NaN
Texas | 38.018740
        dtype: float64

\1сивов, которое можно определить посредством стандартной арифметики множеств
языка Python для этих индексов:
In[8]: area.index | population.index
Out[8]: Index(['Alaska', 'California', 'New York', 'Texas'],
\1ndas отмечает пропущенные
данные (см. дальнейшее обсуждение вопроса отсутствующих данных в разделе «Об-
работка отсутствующих данных» этой главы). Аналогичным образом реализовано
сопоставление индексов для всех встроенных арифметич еских выражений языка
Python: все отсутствующие значения заполняются по умолчанию значением NaN:
In[9]:
\1
\1
\1\1
In[10]: A.add(B,
\1\1

============================================================
СТРАНИЦА 152
============================================================

\1ndas
In[11]:
\1ng.randint(0, 20, (2, 2)),

\1n[12]:
\1ng.randint(0, 10, (3, 3)),

\1n[13]: A + B
Out[13]: | A | B | C
0 | 1.0  15.0 NaN
1  13.0 | 6.0 NaN
2 | NaN | NaN NaN

\1
In[14]:
\1n()
        A.add(B,
\1\1n [45]: A.stack()
 Out[45]:
0  A | 1
B | 11
1  A | 5
B | 1
 dtype: int32
 См. также подраздел «Мультииндекс как дополнительное измерение» раздела «Мульти-
индексированный объект Series» текущей главы.

============================================================
СТРАНИЦА 153
============================================================
Операции над данными в библиотеке Pandas  153
В табл. 3.1 приведен перечень операторов языка Python и эквивалентных им мето -
дов объектов библиотеки Pandas.
Таблица 3.1. Соответствие между операторами языка Python и методами библиотеки Pandas
Оператор языка Python Метод (-ы) библиотеки Pandas
+ add()
– sub(), subtract()
* mul(), multiply()
/ truediv(), div(), divide()
// floordiv()
% mod()
** pow()

\1
In[15]:
\1ng.randint(10,
\1n[16]: A - A[0]
Out[16]: array([[ 0,  0,  0,  0],
                [-1, -2,  2,  4],
                [ 3, -7,  1,  4]])
В соответствии с правилами транслирования библиотеки NumPy («Операции над
массивами. Транслирование» главы 2), вычитание из двумерного массива одной
из его строк выполняется построчно.
В библиотеке Pandas вычитание по умолчанию также происходит построчно:
In[17]:
\1
\1\1ndas
         1 -1 -2  2  4
         2  3 -7  1  4

\1
In[18]: df.subtract(df['R'],
\1\1
In[19]:
\1nt64
In[20]: df - halfrow
Out[20]: | Q | R | S | T
         0  0.0 NaN  0.0 NaN
         1 -1.0 NaN  2.0 NaN
         2  3.0 NaN  1.0 NaN
Подобное сохранение и выравнивание индексов и столбцов означает, что операции
над данными в библиотеке Pandas всегда сохраняют контекст данных, предотвра-
щая возможные ошибки при работе с неоднородными и/или неправильно/неоди-
наково выровненными данными в исходных массивах NumPy.

\1гих интересных наборах данных некоторое количество дан ных отсутствует. Еще
более затрудняет работу то, что в различных источниках данных отсутствующие
данные могут быть помечены различным образом.
В этом разделе мы обсудим общие соображения, касающиеся отсутствующих
данных, обсудим способы представления их библиотекой Pandas и продемонстри -
руем встроенные инструменты библиотеки Pandas для обработки отсутствующих
данных в языке Python. Здесь и далее мы будем называть отсутствующие данные
null , NaN  или NA -значениями.

============================================================

\1индикатора (sentinel value), обозначающего пропущенное
значение.

\1специального значения, включенного
в спецификации IEEE для чисел с плавающей точкой.

\1соединяемый к каждой ячейке.
Отсутствующие данные в библиотеке Pandas
Способ обработки отсутствующих данных библиотекой Pandas определяется тем,
что она основана на пакете NumPy, в котором отсутствует встроенное понятие NA-
значений для всех типов данных, кроме данных с плавающей точкой.
Библиотека Pandas могла бы последовать примеру языка R и задавать комбинации
битов для каждого конкретного типа данных для индикации отсутствия значения,
но этот подход оказывается довольно громоздким. Ведь если в языке R насчитыва -
ется всего четыре базовых типа данных, то NumPy поддерживает намного больше.
Например, в языке R есть только один целочисленный тип, а библиотека
\1\1ndas
поддерживает четырнадцать простых целочисленных типов, с учетом различной
точности, знаковости/беззнаковости и порядка байтов числа. Резервирование спе -
циальной комбинации битов во всех доступных в библиотеке NumPy типах данных
привело бы к громадным накладным расходам в разнообразных частных случаях
операций для различных типов и, вероятно, потребовало бы даже отдельной ветви
пакета NumPy. Кроме того, для небольших типов данных (например, 8-битных
целых чисел) потеря одного бита на маску существенно сузит диапазон доступных
для представления этим типом значений.

\1назначенные для маркирования как «плохих» или «хороших» данных. Библиотека
Pandas могла унаследовать такую возможность, но накладные расходы на хране -
ние, вычисления и поддержку кода сделали этот вариант малопривлекательным.
По этой причине в библиотеке Pandas было решено использовать для отсутству-
ющих данных индикаторы, а также два уже существующих в Python пустых зна-
чения: специальное значение NaN с плавающей точкой и объект None языка Python.
У этого решения есть свои недостатки, но на практике в большинстве случаев оно
представляет собой удачный компромисс.
None: отсутствующие данные в языке Python
Первое из используемых библиотекой Pandas значений-индикаторов — None,
объект-одиночка Python, часто применяемый для обозначения отсутствующих
данных в коде на языке Python. В силу того что None — объект Python, его нельзя
использовать  в произвольных массивах библиотек NumPy/Pandas, а только в мас -
сивах с типом данных 'object' (то есть массивах объектов языка Python):
In[1]: import numpy as np
       import pandas as pd
In[2]:
\1np.array([1, None, 3, 4])
       vals1
Out[2]: array([1, None, 3, 4],
\1n. Хотя такая
разновидность массивов полезна для определенных целей, все операции над ними
будут выполняться на уровне языка Python, с накладными расходами, значительно
превышающими расходы на выполнение быстрых операций над массивами с на-
тивными типами данных:
In[3]: for dtype in ['object', 'int']:

============================================================
СТРАНИЦА 157
============================================================
Обработка отсутствующих данных  157
           print("
\1np.arange(1E6,
\1nt()

\1nt
100 loops, best of 3: 3.06 ms per loop
Использование объектов языка Python в массивах означает также, что при выпол-
нении функций агрегирования по массиву со значениями None, например sum() или
min(), вам будет возвращена ошибка:
In[4]: vals1.sum()
TypeError | Traceback (most recent call last)
<ipython-input-4-749
\1
\1naconda/lib/python3.5/site-packages/numpy/core/_methods.py ...
     30
     31 def _sum(a,
\1ne,
\1ne,
\1ne,
\1n umr_sum(a, axis, dtype, out, keepdims)
     33
     34 def _prod(a,
\1ne,
\1ne,
\1ne,
\1nsupported operand type(s) for +: 'int' and 'NoneType'
Эта ошибка отражает тот факт, что операция между целочисленным значением
и значением None не определена.
NaN: отсутствующие числовые данные

\1
In[5]:
\1np.array([1, np.nan, 3, 4])
       vals2.dtype
Out[5]: dtype('float64')

\1нение скомпилированному коду. Вы должны отдавать себе отчет, что значение

============================================================
СТРАНИЦА 158
============================================================

\1ndas
NaN в чем-то подобно «вирусу данных»: оно «заражает» любой объект, к которому
«прикасается». Вне зависимости от операции результат арифметического действия
с участием NaN будет равен NaN:
In[6]: 1 + np.nan
Out[6]: nan
In[7]: 0 *  np.nan
Out[7]: nan

\1
In[8]: vals2.sum(), vals2.min(), vals2.max()
Out[8]: (nan, nan, nan)

\1
In[9]: np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)
Out[9]: (8.0, 1.0, 4.0)

\1именно значение с плавающей точкой, аналога значения
NaN для целочисленных значений, строковых и других типов не существует.
Значения NaN и None в библиотеке Pandas
Как у значения NaN, так и у None есть свое назначение, и библиотека Pandas делает
их практически взаимозаменяемыми путем преобразования одного в другое в опре-
деленных случаях:
In[10]: pd.Series([1, np.nan, 2, None])
Out[10]: 0 | 1.0
1 | NaN
2 | 2.0
3 | NaN
         dtype: float64
Библиотека Pandas автоматически выполняет преобразование при обнаружении
NA -значений для тех типов, у которых отсутствует значение-индикатор. Напри -
мер, если задать значение элемента целочисленного массива равным np.nan, для
соответствия типу отсутствующего значения будет автоматически выполнено по-
вышающее приведение типа этого массива к типу с плавающей точкой:

============================================================
СТРАНИЦА 159
============================================================
Обработка отсутствующих данных  159
In[11]:
\1nge(2),
\1nt)
        x
Out[11]: 0 | 0
1 | 1
         dtype: int64
In[12]: x[0] = None
        x
Out[12]: 0 | NaN
1 | 1.0
         dtype: float64

\1сиву значений с плавающей точкой, библиотека Pandas автоматически преобразует
значение None в NaN. Замечу, что существует план по внесению в будущем нативного
целочисленного NA в библиотеку Pandas, но на момент написания данной книги
оно еще не было включено.

\1теки Pandas может показаться несколько вычурным по сравнению с более уни -
фицированным подходом к NA -значениям в таких предметно-ориентированных
языках, как R, на практике он прекрасно работает и на моей памяти лишь изредка
вызывал проблемы.
В табл. 3.2 перечислены правила повышающего приведения типов  в библиотеке
Pandas в случае наличия NA -значений.
Таблица 3.2. Правила повышающего приведения типов в библиотеке Pandas

\1индикатор NA
С плавающей точкой Без изменений np.nan
Объект (object) Без изменений None  или np.nan
Целое число Приводится к float64 np.nan
Булево значение Приводится к object None  или np.nan
Имейте в виду, что строковые данные в библиотеке Pandas всегда хранятся с типом
данных (dtype) object.
Операции над пустыми значениями
Библиотека Pandas рассматривает значения None и NaN как взаимозаменяемые
средства указания на отсутствующие или пустые значения. Существует несколько
удобных методов для обнаружения, удаления и замены пустых значений в струк-
турах данных библиотеки Pandas, призванных упростить работу с ними.

============================================================
СТРАНИЦА 160
============================================================

\1ndas
 isnull() — генерирует булеву маску для отсутствующих значений.
 notnull() — противоположность метода isnull().
 dropna() — возвращает отфильтрованный вариант данных.
 fillna() — возвращает копию данных, в которой пропущенные значения за -
полнены или восстановлены.
Завершим раздел кратким рассмотрением и демонстрацией этих методов.
Выявление пустых значений
У структур данных библиотеки Pandas имеются два удо бных метода для выявления
пустых значений: isnull() и notnull(). Каждый из них возвращает булеву маску
для данных. Например:
In[13]:
\1np.nan, 'hello', None])
In[14]: data.isnull()
Out[14]: 0 | False
1 | True
2 | False
3 | True
         dtype: bool

\1
In[15]: data[data.notnull()]
Out[15]: 0 | 1
2 | hello
         dtype: object
Аналогичные булевы результаты дает использование методов isnull() и notnull()
для объектов DataFrame.

\1тоды: dropna() (отбрасывающий NA -значения) и fillna() (заполняющий NA -
значения). Для объекта Series результат вполне однозначен:
In[16]: data.dropna()
Out[16]: 0 | 1
2 | hello
         dtype:
\1\1
In[17]:
\1np.nan, 2],
[2, | 3, | 5],
[np.nan, 4, | 6]])
        df
Out[17]: | 0 | 1  2
         0  1.0  NaN  2
         1  2.0  3.0  5
         2  NaN  4.0  6
Нельзя выбросить из DataFrame отдельные значения, только целые строки или
столбцы. В зависимости от приложения может понадобиться тот или иной вариант,
так что функция dropna() предоставляет для случая объектов DataFrame несколько
параметров.
По умолчанию dropna() отбрасывает все строки, в которых присутствует хотя бы
одно пустое значение:
In[18]: df.dropna()
Out[18]: | 0 | 1  2
         1  2.0  3.0  5

\1
In[19]: df.dropna(
\1ns')
Out[19]: | 2
         0  2
         1  5
         2  6

\1раметров how и thresh, обеспечивающих точный контроль допустимого количества
пустых значений.
По умолчанию
\1ny', то есть отбрасываются все строки или столбцы (в зави-
симости от ключевого слова axis), содержащие хоть одно пустое значение. Можно
также указать значение
\1n[20]: df[3] = np.nan
        df
Out[20]: | 0 | 1  2 | 3

============================================================
СТРАНИЦА 162
============================================================

\1ndas
         0  1.0  NaN  2 NaN
         1  2.0  3.0  5 NaN
         2  NaN  4.0  6 NaN
In[21]: df.dropna(
\1ns',
\1\1
In[22]: df.dropna(
\1null() в качестве маски. Но это настолько распро-
страненная операция, что библиотека Pandas предоставляет метод fillna(), воз-
вращающий копию массива с замененными пустыми значениями.

\1
In[23]:
\1np.nan, 2, None, 3],
\1\1
In[24]: data.fillna(0)
Out[24]: a | 1.0

============================================================

\1
In[25]: # заполнение по направлению «вперед»
        data.fillna(
\1\1
In[26]: # заполнение по направлению «назад»
        data.fillna(
\1\1
In[27]: df
Out[27]: | 0 | 1  2 | 3
         0  1.0  NaN  2 NaN
         1  2.0  3.0  5 NaN
         2  NaN  4.0  6 NaN
In[28]: df.fillna(
\1\1ndas
Иерархическая индексация
До сих пор мы рассматривали главным образом одномерные и двумерные данные,
находящиеся в объектах Series и DataFrame библиотеки Pandas. Часто бывает удобно
выйти за пределы двух измерений и хранить многомерные данные, то есть данные,
индексированные по более чем двум ключам. Хотя библиотека Pandas предоставля -
ет объекты Panel и Panel4D, позволяющие нативным образом хранить трехмерные
и четырехмерные данные (см. врезку «Данные объектов Panel» на с. 178), на прак -
тике намного чаще используется иерархическая индексация (hierarchical indexing),
или мультииндексация (multi-indexing), для включения в один индекс нескольких
уровней. При этом многомерные данные могут быть компактно представлены в уже
привычных нам одномерных объектах Series и двумерных объектах DataFrame.
В этом разделе мы рассмотрим создание объектов MultiIndex напрямую, приведем
соображения относительно индексации, срезов и вычисления статистических по-
казателей по мультииндексированным данным, а также полезные методы для пре-
образования между простым и иерархически индексированным представлением
данных.

\1
In[1]: import pandas as pd
       import numpy as np
Мультииндексированный объект Series
Рассмотрим, как можно представить двумерные данные в одномерном объекте
Series. Для конкретики изучим ряд данных, в котором у каждой точки имеются
символьный и числовой ключи.
Плохой способ
Пускай нам требуется проанализировать данные о штатах за два разных года. Вам
может показаться соблазнительным, воспользовавшись утилитами библиотеки
Pandas, применить в качестве ключей кортежи языка Python:
In[2]:
\1nia', 2000), ('California', 2010),
                ('New York', 2000), ('New York', 2010),
                ('Texas', 2000), ('Texas', 2010)]

\1ns,
\1ndex)
       pop
Out[2]: (California, 2000) | 33871648

============================================================
СТРАНИЦА 165
============================================================
Иерархическая индексация  165
(California, 2010) | 37253956
(New York, 2000) | 18976457
(New York, 2010) | 19378102
(Texas, 2000) | 20851820
(Texas, 2010) | 25145561
        dtype: int64

\1
In[3]: pop[('California', 2010):('Texas', 2000)]
Out[3]: (California, 2010) | 37253956
(New York, 2000) | 18976457
(New York, 2010) | 19378102
(Texas, 2000) | 20851820
        dtype: int64

\1
In[4]: pop[[i for i in pop.index if i[1] == 2010]]
Out[4]: (California, 2010) | 37253956
(New York, 2010) | 19378102
(Texas, 2010) | 25145561
        dtype: int64
Это хоть и приводит к желаемому результату, но гораздо менее изящно (и далеко
не так эффективно), как использование синтаксиса срезов, столь полюбившегося
нам в библиотеке Pandas.
Лучший способ
В библиотеке Pandas есть лучший способ выполнения таких операций. Наша ин-
дексация, основанная на кортежах, по сути, является примитивным мультииндек -
сом, и тип MultiIndex библиотеки Pandas как раз обеспечивает необходимые нам
операции. Создать мультииндекс из кортежей можно следующим образом:
In[5]:
\1ndex.from_tuples(index)
       index
Out[5]: MultiIndex(
\1nia', 'New York', 'Texas'], [2000, 2010]],

\1ndex содержит несколько уровней (levels) индекса-
ции. В данном случае названия штатов и годы, а также несколько кодирующих эти
уровни меток (labels) для каждой точки данных.

============================================================
СТРАНИЦА 166
============================================================

\1ndas
Проиндексировав заново наши ряды данных с помощью MultiIndex, мы увидим
иерархическое представление данных:
In[6]:
\1ndex(index)
       pop
Out[6]: California  2000 | 33871648
2010 | 37253956
New York | 2000 | 18976457
2010 | 19378102
Texas | 2000 | 20851820
2010 | 25145561
        dtype: int64

\1данные. Обратите внимание, что в первом столбце
отсутствуют некоторые элементы: в этом мультииндексном представлении все
пропущенные элементы означают то же значение, что и строкой выше.
Теперь для выбора всех данных, второй индекс которых равен 2010, можно просто
воспользоваться синтаксисом срезов библиотеки Pandas:
In[7]: pop[:, 2010]
Out[7]: California | 37253956
New York | 19378102
Texas | 25145561
        dtype: int64

\1сации над иерархически индексированными данными.
Мультииндекс как дополнительное измерение
Мы могли с легкостью хранить те же самые данные с помощью простого объекта
DataFrame с индексом и метками столбцов. На самом деле библиотека Pandas создана
с учетом этой равнозначности. Метод unstack() может быстро преобразовать муль-
тииндексный объект Series в индексированный обычным образом объект DataFrame:
In[8]:
\1nstack()
       pop_df
Out[8]: | 2000 | 2010
        California  33871648  37253956
New York | 18976457  19378102
Texas | 20851820  25145561

============================================================

\1
In[9]: pop_df.stack()
Out[9]:  California  2000 | 33871648
2010 | 37253956
New York | 2000 | 18976457
2010 | 19378102
Texas | 2000 | 20851820
2010 | 25145561
         dtype: int64

\1биться добавить в демографические данные по каждому штату за каждый год еще
один столбец (допустим, количество населения младше 18 лет). Благодаря типу
MultiIndex это сводится к добавлению еще одного столбца в объект DataFrame:
In[10]:
\1nder18': [9267089, 9284094,
                                           4687374, 4318033,
                                           5906301, 6879014]})
        pop_df
Out[10]: | total  under18
         California 2000  33871648  9267089
                    2010  37253956  9284094
New York | 2000  18976457  4687374
                    2010  19378102  4318033
Texas | 2000  20851820  5906301
                    2010  25145561  6879014

\1суждавшаяся в разделе «Операции над данными в библиотеке Pandas» этой главы,
также прекрасно работают с иерархическими индексами. В следующем фрагменте
кода мы вычисляем по годам долю населения младше 18 лет на основе вышепри-
веденных данных:
In[11]:
\1nder18'] / pop_df['total']
        f_u18.unstack()
Out[11]: | 2000 | 2010
         California  0.273594  0.249211
New York | 0.247010  0.222831
Texas | 0.283251  0.273568

============================================================
СТРАНИЦА 168
============================================================

\1ndas

\1
In[12]:
\1np.random.rand(4, 2),

\1
\1ndas автоматически распознает такой синтаксис и будет по умолчанию
использовать мультииндекс:
In[13]:
\1nia', 2000): 33871648,
                ('California', 2010): 37253956,
                ('Texas', 2000): 20851820,
                ('Texas', 2010): 25145561,
                ('New York', 2000): 18976457,
                ('New York', 2010): 19378102}
        pd.Series(data)
Out[13]: California  2000 | 33871648
2010 | 37253956
New York | 2000 | 18976457
2010 | 19378102
Texas | 2000 | 20851820
2010 | 25145561
         dtype: int64
Тем не менее иногда бывает удобно создавать объекты MultiIndex явным образом.
Далее мы рассмотрим несколько методов для этого.
Явные конструкторы для объектов MultiIndex

\1ющимися в классе pd.MultiIndex конструкторами-методами класса. Например,
можно сформировать объект MultiIndex из простого списка массивов, задающих
значения индекса в каждом из уровней:

============================================================
СТРАНИЦА 169
============================================================
Иерархическая индексация  169
In[14]: pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]])
Out[14]: MultiIndex(
\1\1
In[15]: pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])
Out[15]: MultiIndex(
\1\1
In[16]: pd.MultiIndex.from_product([['a', 'b'], [1, 2]])
Out[16]: MultiIndex(
\1ndex непосредственно с помощью его внутрен-
него представления, передав в конструктор levels (список списков, содержащих
имеющиеся значения индекса для каждого уровня) и labels (список списков меток):
In[17]: pd.MultiIndex(
\1ndex(
\1ndex при
создании объектов Series или DataFrame или методу reindex уже существующих
объектов Series или DataFrame.
Названия уровней мультииндексов
Иногда бывает удобно задать названия для уровней объекта MultiIndex. Сделать
это можно, передав аргумент names любому из вышеперечисленных конструкторов
класса MultiIndex или задав значения атрибута names постфактум:
In[18]: pop.index.
\1nia  2000 | 33871648
2010 | 37253956
New York | 2000 | 18976457
2010 | 19378102
Texas | 2000 | 20851820
2010 | 25145561
         dtype:
\1\1ndas

\1
In[19]:
# Иерархические индексы и столбцы

\1ndex.from_product([[2013, 2014], [1, 2]],

\1
\1ndex.from_product([['Bob', 'Guido', 'Sue'],
                                      ['HR', 'Temp']],

\1np.round(np.random.randn(4, 6), 1)
data[:, ::2] *= 10
data += 37
# Создаем объект DataFrame

\1
\1ndex,
\1ns)
health_data
Out[19]: subject | Bob | Guido | Sue
type | HR  Temp | HR  Temp | HR  Temp
         year visit
2013 1 | 31.0  38.7  32.0  36.7  35.0  37.2
2 | 44.0  37.7  50.0  35.0  29.0  36.7
2014 1 | 30.0  37.4  39.0  37.8  61.0  36.9
2 | 47.0  37.8  48.0  37.3  51.0  36.5

\1
In[20]: health_data['Guido']
Out[20]: type | HR  Temp
         year visit
2013 1 | 32.0  36.7
2 | 50.0  35.0
2014 1 | 39.0  37.8
2 | 48.0  37.3

\1ndex спроектирован так, чтобы индексация и срезы по мул ьтиин-
дексу были интуитивно понятны, особенно если думать об индексах как о допол-
нительных измерениях. Изучим сначала индексацию мультииндекс ированного
объекта Series, а затем мультииндексированного объекта DataFrame.

\1
In[21]: pop
Out[21]: state | year
California  2000 | 33871648
2010 | 37253956
New York | 2000 | 18976457
2010 | 19378102
Texas | 2000 | 20851820
2010 | 25145561
         dtype: int64

\1
In[22]: pop['California', 2000]
Out[22]: 33871648
Объект MultiIndex поддерживает также частичную индексацию (partial indexing),
то есть индексацию только по одному из уровней индекса. Результат — тоже объект
Series, с более низкоуровневыми индексами:
In[23]: pop['California']
Out[23]: year
2000 | 33871648
2010 | 37253956
         dtype: int64

\1

============================================================
СТРАНИЦА 172
============================================================

\1ndas
In[24]: pop.loc['California':'New York']
Out[24]: state | year
California  2000 | 33871648
2010 | 37253956
New York | 2000 | 18976457
2010 | 19378102
         dtype: int64

\1
In[25]: pop[:, 2000]
Out[25]: state
California | 33871648
New York | 18976457
Texas | 20851820
         dtype: int64

\1
In[26]: pop[
\1nia  2000 | 33871648
2010 | 37253956
Texas | 2010 | 25145561
         dtype: int64

\1
In[27]: pop[['California', 'Texas']]
Out[27]: state | year
California  2000 | 33871648
2010 | 37253956
Texas | 2000 | 20851820
2010 | 25145561
         dtype: int64

\1
In[28]: health_data
Out[28]: subject | Bob | Guido | Sue
type | HR  Temp | HR  Temp | HR  Temp
         year
\1\1
In[29]: health_data['Guido', 'HR']
Out[29]: year  visit
2013  1 | 32.0
2 | 50.0
2014  1 | 39.0
2 | 48.0
         Name: (Guido, HR), dtype: float64

\1
In[30]: health_data.iloc[:2, :2]
Out[30]: subject | Bob
type | HR  Temp
         year visit
2013 1 | 31.0  38.7
2 | 44.0  37.7

\1
In[31]: health_data.loc[:, ('Bob', 'HR')]
Out[31]: year  visit
2013  1 | 31.0
2 | 44.0
2014  1 | 30.0
2 | 47.0
         Name: (Bob, HR), dtype: float64

\1
In[32]: health_data.loc[(:, 1), (:, 'HR')]
  File "<IPython-input-32-8
\1ne 1
    health_data.loc[(:, 1), (:, 'HR')]
                     ^
SyntaxError: invalid
\1\1ndas

\1ной функции Python slice() , но лучше в данном случае использовать объект
IndexSlice, предназначенный библиотекой Pandas как раз для подобной ситуации.

\1
In[33]:
\1ndexSlice
        health_data.loc[idx[:, 1], idx[:, 'HR']]
Out[33]: subject | Bob Guido | Sue
type | HR | HR | HR
         year visit
2013 1 | 31.0  32.0  35.0
2014 1 | 30.0  39.0  61.0

\1умение эффективно преобразовывать данные. Существует немало операций, сохра-
няющих всю информацию из набора данных, но преобразующих ее ради удобства
проведения различных вычислений. Мы рассмотрели небольшой пример этого
с методами stack() и unstack(), но есть гораздо больше способов точного контроля
над перегруппировкой данных между иерархическими индексами и столбцами.

\1
In[34]:
\1ndex.from_product([['a', 'c', 'b'], [1, 2]])

\1np.random.rand(6),
\1ndex)
        data.index.
\1nt']
        data
Out[34]: char  int
a | 1 | 0.003001
2 | 0.164974
c | 1 | 0.741650
2 | 0.569264
b | 1 | 0.001693
2 | 0.526226
         dtype:
\1\1
In[35]: try:
            data['a':'b']
        except KeyError as e:
            print(type(e))
            print(e)
<class 'KeyError'>
'Key length (1) was greater than MultiIndex lexsort depth (0)'

\1тому что объект MultiIndex не отсортирован. По различным причинам частичные
срезы и другие подобные операции требуют, чтобы уровни мультииндекса были
отсортированы (лексикографически упорядочены). Библиотека Pandas предостав -
ляет множество удобных инструментов для выполнения подобной сортировки.
В качестве примеров можем указать методы sort_index() и sortlevel() объекта
DataFrame. Мы воспользуемся простейшим из них — методом sort_index():
In[36]:
\1ndex()
        data
Out[36]: char  int
a | 1 | 0.003001
2 | 0.164974
b | 1 | 0.001693
2 | 0.526226
c | 1 | 0.741650
2 | 0.569264
         dtype: float64

\1
In[37]: data['a':'b']
Out[37]: char  int
a | 1 | 0.003001
2 | 0.164974
b | 1 | 0.001693
2 | 0.526226
         dtype: float64
Выполнение над индексами операций stack и unstack

\1
1 «Длина ключа была больше, чем глубина лексикографической сортировки объекта Multi -
Index».

============================================================
СТРАНИЦА 176
============================================================

\1ndas
In[38]: pop.unstack(
\1nia | New York | Texas
          year
2000 | 33871648 | 18976457  20851820
2010 | 37253956 | 19378102  25145561
In[39]: pop.unstack(
\1nia  33871648  37253956
New York | 18976457  19378102
Texas | 20851820  25145561
Методу unstack() противоположен по действию метод stack(), которым можно
воспользоваться, чтобы получить обратно исходный ряд данных:
In[40]: pop.unstack().stack()
Out[40]: state | year
California  2000 | 33871648
2010 | 37253956
New York | 2000 | 18976457
2010 | 19378102
Texas | 2000 | 20851820
2010 | 25145561
         dtype: int64

\1преобразовать мет -
ки индекса в столбцы с помощью метода reset_index. Результатом вызова этого
метода для нашего ассоциативного словаря населения будет объект DataFrame со
столбцами state  (штат) и year  (год), содержащими информацию, ранее находи-
вшуюся в индексе. Для большей ясности можно при желании задать название для
представленных в виде столбцов данных:
In[41]:
\1ndex(
\1n')
        pop_flat
Out[41]: | state  year  population
0  California  2000 | 33871648
1  California  2010 | 37253956
2 | New York  2000 | 18976457
3 | New York  2010 | 19378102
4 | Texas  2000 | 20851820
5 | Texas  2010 | 25145561

\1дят подобным образом, поэтому удобно создать объект MultiIndex из значений

============================================================
СТРАНИЦА 177
============================================================
Иерархическая индексация  177
столбцов. Это можно сделать с помощью метода set_index объекта DataFrame, воз -
вращающего мультииндексированный объект DataFrame:
In[42]: pop_flat.set_index(['state', 'year'])
Out[42]: | population
state | year
California 2000 | 33871648
2010 | 37253956
New York | 2000 | 18976457
2010 | 19378102
Texas | 2000 | 20851820
2010 | 25145561

\1один из самых удобных
паттернов при работе с реальными наборами данных.
Агрегирование по мультииндексам
В библиотеке Pandas имеются встроенные методы для агрегирования данных, на-
пример mean(), sum() и max(). В случае иерархически индексированных данных им
можно передать параметр level для указания подмножества данных, на котором
будет вычисляться сводный показатель.

\1
In[43]: health_data
Out[43]:  subject | Bob | Guido | Sue
type | HR  Temp | HR  Temp | HR  Temp
          year visit
2013 1 | 31.0  38.7  32.0  36.7  35.0  37.2
2 | 44.0  37.7  50.0  35.0  29.0  36.7
2014 1 | 30.0  37.4  39.0  37.8  61.0  36.9
2 | 47.0  37.8  48.0  37.3  51.0  36.5

\1
In[44]:
\1n(
\1n
Out[44]: subject | Bob | Guido | Sue
type | HR  Temp | HR | Temp | HR | Temp
         year
2013 | 37.5  38.2  41.0  35.85  32.0  36.95
2014 | 38.5  37.6  43.5  37.55  56.0  36.70

============================================================
СТРАНИЦА 178
============================================================

\1ndas

\1
In[45]: data_mean.mean(
\1nel
В библиотеке Pandas есть еще несколько пока не охваченных нами структур данных,
а именно объекты pd.Panel и pd.Panel4D. Их можно рассматривать как соответ -
ственно трехмерное и четырехмерное обобщение (одномерной структуры) объекта
Series и (двумерной структуры) объекта DataFrame. Раз вы уже знакомы с индекса-
цией данных в объектах Series и DataFrame и манипуляциями над ними, то исполь-
зование объектов Panel и Panel4D не должно вызвать у вас затруднений. В частности,
возможности индексаторов loc, iloc и ix, обсуждавшихся в разделе «Индексация
и выборка данных» текущей главы, с легкостью распространяются на эти структуры
более высоких размерностей.

\1разреженное представление данных. По мере
увеличения размерности плотное представление становится все менее эффектив -
ным для большинства реальных наборов данных. В некоторых специализированных
приложениях, однако, эти структуры данных могут быть полезны. Если вы захотите
узнать больше о структурах Panel и Panel4D, загляните в ссылки, приведенные в раз-
деле «Дополнительные источники информации» данной главы.

\1ющих все возможные частичные совпадения наборов. Объекты Series и DataFrame
созданы в расчете на подобные операции, и библиотека Pandas содержит функции
и методы для быстрого и удобного выполнения таких манипуляций.
Мы рассмотрим простую конкатенацию объектов Series и DataFrame с помощью
функции pd.concat, углубимся в реализованные в библиотеке Pandas более запу-
танные слияния и соединения, выполняемые в оперативной памяти.

\1
In[1]: import pandas as pd
       import numpy as np

\1
In[2]: def make_df(cols, ind):
           """Быстро создаем объект DataFrame"""

\1n ind]
                   for c in cols}
           return pd.DataFrame(data, ind)
       # Экземпляр DataFrame
       make_df('ABC', range(3))
Out[2]: | A | B | C
        0  A0  B0  C0
        1  A1  B1  C1
        2  A2  B2  C2

\1сивов библиотеки NumPy, которую можно осуществить посредством функции
np.concatenate , обсуждавшейся в разделе «Введение в массивы библиотеки
NumPy» главы 2. Напомним, что таким образом можно объединять содержимое
двух или более массивов в один:
In[4]:
\1np.concatenate([x, y, z])
Out[4]: array([1, 2, 3, 4, 5, 6, 7, 8, 9])

\1

============================================================
СТРАНИЦА 180
============================================================

\1ndas
In[5]:
\1np.concatenate([x, x],
\1ncat
В библиотеке Pandas имеется функция, pd.concat(), синтаксис которой аналогичен
функции np.concatenate, однако она содержит множество параметров, которые мы
вскоре обсудим:
# Сигнатура функции pd.concat в библиотеке Pandas v0.18
pd.concat(objs,
\1
\1
\1ne,
\1ne,
\1ne,
\1ne,
\1ncat  можно использовать для простой конкатенации объектов
Series или DataFrame аналогично тому, как функцию np.concatenate()  можно
применять для простой конкатенации массивов:
In[6]:
\1
\1
\1ncat([ser1, ser2])
Out[6]: 1 | A
2 | B
3 | C
4 | D
5 | E
6 | F
        dtype: object

\1
In[7]:
\1nt(df1); print(df2); print(pd.concat([df1, df2]))
df1 | df2 | pd.concat([df1, df2])
A | B | A | B | A | B
1  A1  B1 | 3  A3  B3 | 1  A1  B1
2  A2  B2 | 4  A4  B4 | 2  A2  B2
                               3  A3  B3
                               4  A4  B4
По умолчанию конкатенация происходит в объекте DataFrame построчно, то есть

\1np.concatenate() функция pd.concat() позволяет
указывать ось, по которой будет выполняться конкатенация. Рассмотрим следу -
ющий пример:

============================================================
СТРАНИЦА 181
============================================================
Объединение наборов данных: конкатенация и добавление в конец  181
In[8]:
\1nt(df3); print(df4); print(pd.concat([df3, df4],
\1ncat([df3, df4],
\1np.concatenate()  и pd.concat()  состоит
в том, что конкатенация из библиотеки Pandas сохраняет индексы , даже если
в результате некоторые индексы будут дублироваться. Рассмотрим следующий
пример:
In[9]:
\1
\1ndex  # Дублируем индексы!
       print(x); print(y); print(pd.concat([x, y]))
x | y | pd.concat([x, y])
A | B | A | B | A | B
0  A0  B0 | 0  A2  B2 | 0  A0  B0
1  A1  B1 | 1  A3  B3 | 1  A1  B1
                           0  A2  B2
                           1  A3  B3

\1пустимо, подобный результат часто может быть нежелателен. Функция pd.concat()
предоставляет нам несколько способов решения этой проблемы.

\1ксы в возвращаемом функцией pd.concat() результате не перекрываются, можно
задать флаг verify_integrity. В случае равного True значения этого флага конка-
тенация приведет к генерации ошибки при наличии дублирующихся индексов. Вот
пример, в котором мы для большей ясности перехватываем и выводим в консоль
сообщение об ошибке:

\1ndas допустимы следующие варианты синта-
ксиса:

\1ns'
 или

\1ndas, используемой автором книги, документированный синтаксис для этой функции
допускает только применение числовых значений для параметра axis.

============================================================
СТРАНИЦА 182
============================================================

\1ndas
In[10]: try:
            pd.concat([x, y],
\1nt("ValueError:", e)
ValueError: Indexes have overlapping values: [0, 1]
Игнорирование индекса.  Иногда индекс сам по себе не имеет значения и лучше
его просто проигнорировать. Для этого достаточно установить флаг ignore_index.

\1
In[11]: print(x); print(y); print(pd.concat([x, y],
\1ncat([x, y],
\1\1
In[12]: print(x); print(y); print(pd.concat([x, y],
\1ncat([x, y],
\1ncat()  имеется несколько опций. Изучим объединение следующих двух
объектов DataFrame, у которых столбцы (но не все!) называются одинаково:
In[13]:
\1nt(df5); print(df6); print(pd.concat([df5, df6]))

============================================================
СТРАНИЦА 183
============================================================
Объединение наборов данных: конкатенация и добавление в конец  183
df5 | df6 | pd.concat([df5, df6])
A | B | C | B | C | D | A | B | C | D
1  A1  B1  C1 | 3  B3  C3  D3 | 1 | A1  B1  C1  NaN
2  A2  B2  C2 | 4  B4  C4  D4 | 2 | A2  B2  C2  NaN
3  NaN  B3  C3 | D3
4  NaN  B4  C4 | D4

\1значе ниями. Чтобы поменять это поведение, можно указать одну из нескольких
опций для параметров join и join_axes  функции конкатенации. По умолча -
нию соединение — объединение входных столбцов (
\1
\1nner':
In[14]: print(df5); print(df6);
        print(pd.concat([df5, df6],
\1nner'))
df5 | df6 | pd.concat([df5, df6],
\1nner')
A | B | C | B | C | D | B | C
1  A1  B1  C1 | 3  B3  C3  D3 | 1  B1  C1
2  A2  B2  C2 | 4  B4  C4  D4 | 2  B2  C2
                                   3  B3  C3
                                   4  B4  C4
Еще одна опция предназначена для указания явным образом индекса оставшихся
столбцов с помощью аргумента join_axes, которому присваивается список объек -
тов индекса. В данном случае мы указываем, что возвращаемые столбцы должны
совпадать со столбцами первого из конкатенируемых объектов DataFrame:
In[15]: print(df5); print(df6);
        print(pd.concat([df5, df6],
\1ns]))
df5 | df6 | pd.concat([df5, df6],
\1ns])
A | B | C | B | C | D | A | B | C
1  A1  B1  C1 | 3  B3  C3  D3 | 1 | A1  B1  C1
2  A2  B2  C2 | 4  B4  C4  D4 | 2 | A2  B2  C2
                                   3  NaN  B3  C3
                                   4  NaN  B4  C4
Различные сочетания опций функции pd.concat() обеспечивают широкий диа -
пазон возможных поведений при соединении двух наборов данных. Не забывайте
об этом при использовании ее для ваших собственных данных.
Метод append()

\1екты Series и DataFrame был включен метод append(), позволяющий выполнить то
же самое с меньшими усилиями. Например, вместо вызова pd.concat([df1, df2])
можно вызвать df1.append(df2):

============================================================
СТРАНИЦА 184
============================================================

\1ndas
In[16]: print(df1); print(df2); print(df1.append(df2))
df1 | df2 | df1.append(df2)
A | B | A | B | A | B
1  A1  B1 | 3  A3  B3 | 1 | A1  B1
2  A2  B2 | 4  A4  B4 | 2 | A2  B2
3 | A3  B3
4 | A4  B4
Не забывайте, что, в отличие от методов append()  и extend()  списков языка
Python, метод append() в библиотеке Pandas не изменяет исходный объект. Вместо
этого он создает новый объект с объединенными данными, что делает этот метод
не слишком эффективным, поскольку означает создание нового индекса и буфера
данных. Следовательно, если вам необходимо выполнить несколько операций
append, лучше создать список объектов DataFrame и передать их все сразу функции
concat().

\1полнение слияний/объединений в стиле баз данных с помощью функции pd.merge.
Для получения дополнительной информации о методах concat(), append() и от -
носящейся к ним функциональности см. раздел Merge, Join and Concatenate («Сли -
яние, соединение и конкатенация», http://pandas.pydata.org/pandas-docs/stable/merging.
html) документации библиотеки Pandas.
Объединение наборов данных: слияние
и соединение
Одно из важных свойств библиотеки Pandas — ее высокопроизводительные, вы -
полняемые в оперативной памяти операции соединения и слияния. Если вы когда-
либо работали с базами данных, вам должен быть знаком такой вид взаимодействия
с данными. Основной интерфейс для них — функция pd.merge. Несколько приме-
ров ее работы на практике мы рассмотрим далее.
Реляционная алгебра
Реализованное в методе pd.merge поведение представляет собой подмножество
того, что известно под названием «реляционная алгебра»  (relational algebra). Ре -
ляционная алгебра — формальный набор правил манипуляции реляционными
данными, формирующий теоретические основания для имеющихся в большинстве
баз данных операций. Сила реляционного подхода состоит в предоставлении им
нескольких простейших операций — своеобразных «кирпичиков» для построения
более сложных операций над любым набором данных. При наличии эффективно

============================================================

\1раций.
Библиотека Pandas реализует несколько из этих базовых «кирпичиков» в функции
pd.merge() и родственном ей методе join() объектов Series и DataFrame. Они обе-
спечивают возможность эффективно связывать данные из различных источников.

\1
In[2]:

\1nting', 'Engineering', 'Engineering',
                              'HR']})

\1nt(df1); print(df2)
df1 | df2
employee | group | employee  hire_date
0 | Bob | Accounting | 0 | Lisa | 2004
1 | Jake  Engineering | 1 | Bob | 2008
2 | Lisa  Engineering | 2 | Jake | 2012
3 | Sue | HR | 3 | Sue | 2014

\1
In[3]:
\1nting | 2008

============================================================
СТРАНИЦА 186
============================================================

\1ndas
1 | Jake  Engineering | 2012
2 | Lisa  Engineering | 2004
3 | Sue | HR | 2014
Функция pd.merge() распознает, что в обоих объектах DataFrame имеется столбец
employee, и автоматически выполняет соединение, используя этот столбец в качестве
ключа. Результатом слияния становится новый объект DataFrame, объединяющий
информацию из двух входных объектов. Обратите внимание, что порядок записей
в столбцах не обязательно сохраняется: в данном случае сортировка столбца employee
различна в объектах df1 и df2 и функция pd.merge() обрабатывает эту ситуацию
корректным образом. Кроме того, не забывайте, что слияние игнорирует индекс,
за исключением особого случая слияния по индексу (см. пункт «Ключевые слова
left_index и right_index» подраздела «Задание ключа слияния» данного раздела).

\1
In[4]:
\1nting', 'Engineering', 'HR'],
                           'supervisor': ['Carly', 'Guido', 'Steve']})
       print(df3); print(df4); print(pd.merge(df3, df4))
df3 | df4
employee | group  hire_date | group supervisor
0 | Bob | Accounting | 2008 | 0 | Accounting | Carly
1 | Jake  Engineering | 2012 | 1  Engineering | Guido
2 | Lisa  Engineering | 2004 | 2 | HR | Steve
3 | Sue | HR | 2014
pd.merge(df3, df4)
employee | group  hire_date supervisor
0 | Bob | Accounting | 2008 | Carly
1 | Jake  Engineering | 2012 | Guido
2 | Lisa  Engineering | 2004 | Guido
3 | Sue | HR | 2014 | Steve

\1
In[5]:
\1nting', 'Accounting',
                                     'Engineering', 'Engineering',
                                     'HR', 'HR'],
                           'skills': ['math', 'spreadsheets', 'coding',
                                      'linux',
                                      'spreadsheets', 'organization']})
print(df1); print(df5); print(pd.merge(df1, df5))
df1 | df5
employee | group | group | skills
0 | Bob | Accounting | 0 | Accounting | math
1 | Jake  Engineering | 1 | Accounting  spreadsheets
2 | Lisa  Engineering | 2  Engineering | coding
3 | Sue | HR | 3  Engineering | linux
4 | HR  spreadsheets
5 | HR  organization
pd.merge(df1, df5)
employee | group | skills
0 | Bob | Accounting | math
1 | Bob | Accounting  spreadsheets
2 | Jake  Engineering | coding
3 | Jake  Engineering | linux
4 | Lisa  Engineering | coding
5 | Lisa  Engineering | linux
6 | Sue | HR  spreadsheets
7 | Sue | HR  organization
Эти три типа соединений можно использовать и в других инструментах библиотеки
Pandas, что дает возможность реализовать широкий диапазон функциональности.

\1но точно, и в методе pd.merge() имеется немало параметров для такой ситуации.

============================================================
СТРАНИЦА 188
============================================================

\1ndas
Ключевое слово on
Проще всего указать название ключевого столбца с помощью ключевого слова on,
в котором указывается название или список названий столбцов:
In[6]: print(df1); print(df2); print(pd.merge(df1, df2,
\1nting | 0 | Lisa | 2004
1 | Jake  Engineering | 1 | Bob | 2008
2 | Lisa  Engineering | 2 | Jake | 2012
3 | Sue | HR | 3 | Sue | 2014
pd.merge(df1, df2,
\1nting | 2008
1 | Jake  Engineering | 2012
2 | Lisa  Engineering | 2004
3 | Sue | HR | 2014
Этот параметр работает только в том случае, когда в левом и правом объектах
DataFrame имеется указанное название столбца.
Ключевые слова left_on и right_on

\1чевыми словами left_on и right_on для указания названий двух нужных столбцов:
In[7]:

\1name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'salary': [70000, 80000, 120000, 90000]})
print(df1); print(df3);
print(pd.merge(df1, df3,
\1
\1name"))
df1 | df3
employee | group | name  salary
0 | Bob | Accounting | 0 | Bob | 70000
1 | Jake  Engineering | 1  Jake | 80000
2 | Lisa  Engineering | 2  Lisa  120000
3 | Sue | HR | 3 | Sue | 90000
pd.merge(df1, df3,
\1
\1name")
employee | group  name  salary
0 | Bob | Accounting | Bob | 70000
1 | Jake  Engineering  Jake | 80000
2 | Lisa  Engineering  Lisa  120000
3 | Sue | HR | Sue | 90000

============================================================

\1
In[8]:
pd.merge(df1, df3,
\1
\1name").drop('name',
\1nting | 70000
1 | Jake  Engineering | 80000
2 | Lisa  Engineering  120000
3 | Sue | HR | 90000
Ключевые слова left_index и right_index

\1
In[9]:
\1ndex('employee')

\1ndex('employee')
       print(df1a); print(df2a)
df1a | df2a
group | hire_date
employee | employee
Bob | Accounting | Lisa | 2004
Jake | Engineering | Bob | 2008
Lisa | Engineering | Jake | 2012
Sue | HR | Sue | 2014
Можно использовать индекс в качестве ключа слияния путем указания в методе
pd.merge() флагов left_index и/или right_index:
In[10]:
print(df1a); print(df2a);
print(pd.merge(df1a, df2a,
\1
\1nting | Lisa | 2004
Jake | Engineering | Bob | 2008
Lisa | Engineering | Jake | 2012
Sue | HR | Sue | 2014
pd.merge(df1a, df2a,
\1
\1ngineering | 2004
Bob | Accounting | 2008
Jake | Engineering | 2012
Sue | HR | 2014

============================================================
СТРАНИЦА 190
============================================================

\1ndas
Для удобства в объектах DataFrame реализован метод join(), выполняющий по
умолчанию слияние по индексам:
In[11]: print(df1a); print(df2a); print(df1a.join(df2a))
df1a | df2a
group | hire_date
employee | employee
Bob | Accounting | Lisa | 2004
Jake | Engineering | Bob | 2008
Lisa | Engineering | Jake | 2012
Sue | HR | Sue | 2014
df1a.join(df2a)
                group  hire_date
employee
Bob | Accounting | 2008
Jake | Engineering | 2012
Lisa | Engineering | 2004
Sue | HR | 2014

\1жения нужного поведения воспользоваться сочетанием флага left_index с пара-
метром right_on или параметра left_on с флагом right_index:
In[12]:
print(df1a); print(df3);
print(pd.merge(df1a, df3,
\1
\1name'))
df1a | df3
                group
employee | name  salary
Bob | Accounting | 0 | Bob | 70000
Jake | Engineering | 1  Jake | 80000
Lisa | Engineering | 2  Lisa  120000
Sue | HR | 3 | Sue | 90000
pd.merge(df1a, df3,
\1
\1name')
          group  name  salary
0 | Accounting | Bob | 70000
1  Engineering  Jake | 80000
2  Engineering  Lisa  120000
3 | HR | Sue | 90000
Все эти параметры работают и в случае нескольких индексов и/или столбцов,
синтаксис для этого интуитивно понятен. Более подробную информацию по
этому вопросу см. в разделе Merge, Join and Concatenate («Слияние, соединение
и конкатенация», http://pandas.pydata.org/pandas-docs/stable/merging.html) документации
библиотеки Pandas.

============================================================

\1
In[13]:
\1name': ['Peter', 'Paul', 'Mary'],
                            'food': ['fish', 'beans', 'bread']},

\1name', 'food'])

\1name': ['Mary', 'Joseph'],
                            'drink': ['wine', 'beer']},

\1name', 'drink'])
        print(df6); print(df7); print(pd.merge(df6, df7))
df6 | df7 | pd.merge(df6, df7)
name | food | name drink | name | food  drink
0  Peter | fish | 0 | Mary  wine | 0 | Mary  bread | wine
1 | Paul  beans | 1  Joseph  beer
2 | Mary  bread

\1пись name: Mary. По умолчанию результат будет содержать пересечение двух вход-
ных множеств — внутреннее соединение (inner join). Можно указать это явным об -
разом, с помощью ключевого слова how, имеющего по умолчанию значение 'inner':
In[14]: pd.merge(df6, df7,
\1nner')
Out[14]: | name | food drink
         0  Mary  bread  wine

\1нее соединение (outer join) означает соединение по объединению входных столбцов
и заполняет значениями NA  все пропуски значений:
In[15]: print(df6); print(df7); print(pd.merge(df6, df7,
\1name | food | name drink | name | food drink
0  Peter | fish | 0 | Mary  wine | 0 | Peter | fish | NaN
1 | Paul  beans | 1  Joseph  beer | 1 | Paul  beans | NaN
2 | Mary  bread | 2 | Mary  bread  wine
3  Joseph | NaN  beer
Левое соединение (left join) и правое соединение (right join) выполняют соединение
по записям слева и справа соответственно. Например:
In[16]: print(df6); print(df7); print(pd.merge(df6, df7,
\1name | food | name drink | name | food
\1\1ndas
0 | Peter | fish | 0 | Mary  wine | 0 | Peter | fish | NaN
1 | Paul  beans | 1  Joseph  beer | 1 | Paul  beans | NaN
2 | Mary  bread | 2 | Mary  bread  wine

\1
In[17]:
\1name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                            'rank': [1, 2, 3, 4]})

\1name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                            'rank': [3, 1, 4, 2]})
        print(df8); print(df9); print(pd.merge(df8, df9,
\1name"))
df8 | df9 | pd.merge(df8, df9,
\1name")
name  rank | name  rank | name  rank_x  rank_y
0 | Bob | 1 | 0 | Bob | 3 | 0 | Bob | 1 | 3
1  Jake | 2 | 1  Jake | 1 | 1  Jake | 2 | 1
2  Lisa | 3 | 2  Lisa | 4 | 2  Lisa | 3 | 4
3 | Sue | 4 | 3 | Sue | 2 | 3 | Sue | 4 | 2

\1
In[18]:
print(df8); print(df9);
print(pd.merge(df8, df9,
\1name",
\1name  rank | name  rank
0 | Bob | 1 | 0 | Bob | 3
1  Jake | 2 | 1  Jake | 1
2  Lisa | 3 | 2  Lisa | 4
3 | Sue | 4 | 3 | Sue | 2
pd.merge(df8, df9,
\1name",
\1name  rank_L  rank_R
0 | Bob | 1 | 3

============================================================

\1ную алгебру, а также в раздел Merge, Join and Concatenate («Слияние, соединение
и конкатенация», http://pandas.pydata.org/pandas-docs/stable/merging.html) документации
библиотеки Pandas.

\1
In[19]:
# Инструкции системного командного процессора для скачивания данных:
# !curl -O https://raw.githubusercontent.com/jakevdp/
# | data-USstates/master/state-population.csv
# !curl -O https://raw.githubusercontent.com/jakevdp/
# | data-USstates/master/state-areas.csv
# !curl -O https://raw.githubusercontent.com/jakevdp/
# | data-USstates/master/state-abbrevs.csv
Посмотрим на эти наборы данных с помощью функции read_csv() библиотеки
Pandas:
In[20]:
\1n.csv')

\1nt(pop.head()); print(areas.head()); print(abbrevs.head())
pop.head() | areas.head()
state/region | ages  year  population | state  area (sq. mi)
0 | AL  under18  2012 | 1117489.0 | 0 | Alabama | 52423
1 | AL | total  2012 | 4817528.0 | 1 | Alaska | 656425
2 | AL  under18  2010 | 1130966.0 | 2 | Arizona | 114006
3 | AL | total  2010 | 4785570.0 | 3 | Arkansas | 53182
4 | AL  under18  2011 | 1125763.0 | 3 | Arkansas | 53182
4  California | 163707
abbrevs.head()
        state abbreviation
0 | Alabama |
\1\1ndas
1 | Alaska | AK
2 | Arizona | AZ
3 | Arkansas | AR
4  California | CA1

\1многим», которое позволит получить полное имя
штата в объекте DataFrame для населения. Выполнить слияние нужно на основе
столбца state/region объекта pop и столбца abbreviation объекта abbrevs. Мы вос-
пользуемся опцией
\1n[21]:
\1
\1n',
\1n')

\1n', 1) # Удаляем дублирующуюся
                                                # информацию
        merged.head()
Out[21]: | state/region | ages  year  population | state
0 | AL  under18  2012 | 1117489.0  Alabama
1 | AL | total  2012 | 4817528.0  Alabama
2 | AL  under18  2010 | 1130966.0  Alabama
3 | AL | total  2010 | 4785570.0  Alabama
4 | AL  under18  2011 | 1125763.0  Alabama

\1
In[22]: merged.isnull().any()
Out[22]: state/region | False
ages | False
year | False
population | True
state | True
         dtype: bool

\1
In[23]: merged[merged['population'].isnull()].head()
Out[23]: | state/region | ages  year  population state
2448 | PR  under18  1990 | NaN | NaN
2449 | PR | total  1990 | NaN | NaN
2450 | PR | total  1991 | NaN | NaN
2451 | PR  under18  1991 | NaN | NaN
2452 | PR | total  1993 | NaN | NaN

\1n строк набора данных. По умолчанию
\1\1
In[24]: merged.loc[merged['state'].isnull(), 'state/region'].unique()
Out[24]: array(['PR', 'USA'],
\1\1
In[25]: merged.loc[merged['state/region'] == 'PR', 'state'] = 'Puerto Rico'
        merged.loc[merged['state/region'] == 'USA', 'state'] = 'United States'
        merged.isnull().any()
Out[25]: state/region | False
ages | False
year | False
population | True
state | False
         dtype: bool

\1
In[26]:
\1
\1nal.head()
Out[26]: | state/region | ages  year  population | state  area (sq. mi)
0 | AL  under18  2012 | 1117489.0  Alabama | 52423.0
1 | AL | total  2012 | 4817528.0  Alabama | 52423.0
2 | AL  under18  2010 | 1130966.0  Alabama | 52423.0
3 | AL | total  2010 | 4785570.0  Alabama | 52423.0
4 | AL  under18  2011 | 1125763.0  Alabama | 52423.0

\1
In[27]: final.isnull().any()
Out[27]: state/region | False
ages | False
year |
\1\1ndas
population | True
state | False
area (sq. mi) | True
         dtype: bool

\1
In[28]: final['state'][final['area (sq. mi)'].isnull()].unique()
Out[28]: array(['United States'],
\1\1
In[29]: final.dropna(
\1nal.head()
Out[29]: | state/region | ages  year  population | state  area (sq. mi)
0 | AL  under18  2012 | 1117489.0  Alabama | 52423.0
1 | AL | total  2012 | 4817528.0  Alabama | 52423.0
2 | AL  under18  2010 | 1130966.0  Alabama | 52423.0
3 | AL | total  2010 | 4785570.0  Alabama | 52423.0
4 | AL  under18  2011 | 1125763.0  Alabama | 52423.0

\1селению. Воспользуемся функцией query() (для этого должен быть установлен
пакет numexpr, см. раздел «Увеличение производительности библиотеки Pandas:
eval() и query()» данной главы):
In[30]:
\1nal.query("
\1n | ages  year  population | state  area (sq. mi)
3 | AL  total  2010 | 4785570.0 | Alabama | 52423.0
91 | AK  total  2010 | 713868.0 | Alaska | 656425.0
101 | AZ  total  2010 | 6408790.0 | Arizona | 114006.0
189 | AR  total  2010 | 2922280.0 | Arkansas | 53182.0
197 | CA  total  2010  37333601.0  California | 163707.0

\1
In[31]: data2010.set_index('state',
\1
\1n'] / data2010['area (sq. mi)']

============================================================
СТРАНИЦА 197
============================================================
Агрегирование и группировка  197
In[32]: density.sort_values(
\1
\1nsity.head()
Out[32]: state
District of Columbia | 8898.897059
Puerto Rico | 1058.665149
New Jersey | 1009.253268
Rhode Island | 681.339159
Connecticut | 645.600649
         dtype: float64

\1
In[33]: density.tail()
Out[33]: state
South Dakota | 10.583512
North Dakota | 9.537565
Montana | 6.736171
Wyoming | 5.768079
Alaska | 1.087509
         dtype: float64

\1их эффективное обобщение: вычисление
сводных показателей, например sum(), mean(), median(), min() и max(), в которых
одно число позволяет понять природу, возможно, огромного набора данных. В этом
разделе мы займемся изучением сводных показателей в библиотеке Pandas, на -
чиная с простых операций, подобных тем, с которыми мы уже имели дело при
работе с массивами NumPy, и заканчивая более сложными операциями на основе
понятия groupby.

============================================================
СТРАНИЦА 198
============================================================

\1ndas
Данные о планетах
Воспользуемся набором данных «Планеты» (Planets), доступным через пакет
Seaborn (см. раздел «Визуализация с помощью библиотеки Seaborn» главы 4).

\1круг других звезд, известных под названием внесолнечных планет или экзопланет
(exoplanets). Скачать его можно с помощью команды пакета Seaborn:
In[2]: import seaborn as sns

\1ns.load_dataset('planets')
       planets.shape
Out[2]: (1035, 6)
In[3]: planets.head()
Out[3]: | method | number  orbital_period  mass | distance  year
0  Radial Velocity  1 | 269.300 | 7.10 | 77.40 | 2006
1  Radial Velocity  1 | 874.774 | 2.21 | 56.95 | 2008
2  Radial Velocity  1 | 763.000 | 2.60 | 19.84 | 2011
3  Radial Velocity  1 | 326.030 | 19.40  110.62 | 2007
4  Radial Velocity  1 | 516.220 | 10.50  119.47 | 2009

\1планет, открытых до 2014 года.
Простое агрегирование в библиотеке Pandas
Ранее мы рассмотрели некоторые доступные для массивов NumPy возможности по
агрегированию данных (см. раздел «Агрегирование: минимум, максимум и все, что
посередине» главы 2). Как и в случае одномерных массивов библиотеки NumPy,
для объектов Series библиотеки Pandas агрегирующие функции возвращают ска-
лярное значение:
In[4]:
\1np.random.RandomState(42)

\1ng.rand(5))
       ser
Out[4]: 0 | 0.374540
1 | 0.950714
2 | 0.731994
3 | 0.598658
4 | 0.156019
        dtype: float64
In[5]: ser.sum()
Out[5]: 2.8119254917081569

============================================================
СТРАНИЦА 199
============================================================
Агрегирование и группировка  199
In[6]: ser.mean()
Out[6]: 0.56238509834163142

\1
In[7]:
\1ng.rand(5),
                          'B': rng.rand(5)})
       df
Out[7]: | A | B
        0  0.155995  0.020584
        1  0.058084  0.969910
        2  0.866176  0.832443
        3  0.601115  0.212339
        4  0.708073  0.181825
In[8]: df.mean()
Out[8]: A | 0.477888
B | 0.443420
        dtype: float64

\1
In[9]: df.mean(
\1ns')
Out[9]: 0 | 0.088290
1 | 0.513997
2 | 0.849309
3 | 0.406727
4 | 0.444949
        dtype: float64
Объекты Series и DataFrame библиотеки Pandas содержат методы, соответству-
ющие всем упомянутым в разделе «Агрегирование: минимум, максимум и все,
что посередине» главы 2 распространенным агрегирующим функциям. В них есть
удобный метод describe() , вычисляющий сразу несколько самых распростра -
ненных сводных показателей для каждого столбца и возвращающий результат.

\1
In[10]: planets.dropna().describe()
Out[10]: | number  orbital_period | mass | distance | year
count  498.00000 | 498.000000  498.000000  498.000000 | 498.000000
mean | 1.73494 | 835.778671 | 2.509320 | 52.068213  2007.377510
std | 1.17572 | 1469.128259 | 3.636274 | 46.596041 | 4.167284
min | 1.00000 | 1.328300 | 0.003600 | 1.350000  1989.000000
25% | 1.00000 | 38.272250 | 0.212500 | 24.497500  2005.000000

============================================================
СТРАНИЦА 200
============================================================

\1ndas
50% | 1.00000 | 357.000000 | 1.245000 | 39.940000  2009.000000
75% | 2.00000 | 999.600000 | 2.867500 | 59.332500  2011.000000
max | 6.00000 | 17337.500000 | 25.000000  354.000000  2014.000000

\1работанный для поиска затмений от планет, вращающих вокруг других звезд.
В табл. 3.3 перечислены основные встроенные агрегирующие методы библиотеки
Pandas.
Таблица 3.3. Список агрегирующих методов библиотеки Pandas
Агрегирующая функция Описание
count() Общее количество элементов
first(), last() Первый и последний элементы
mean(), median() Среднее значение и медиана
min(), max() Минимум и максимум
std(), var() Стандартное отклонение и дисперсия
mad() Среднее абсолютное отклонение
prod() Произведение всех элементов
sum() Сумма всех элементов

\1обобщающее агрегирование, показан на рис. 3.1.

\1тания описанных выше команд маскирования, агрегирования и слияния, важно
понимать, что не обязательно создавать объекты для промежуточных разбиений .
Операция GroupBy может проделать все это за один проход по данным, вычисляя
сумму, среднее значение, количество, минимум и другие сводные показатели для

============================================================
СТРАНИЦА 202
============================================================

\1ndas
каждой группы. Мощь операции GroupBy состоит в абстрагировании этих шагов:
пользователю не нужно заботиться о том, как фактически выполняются вычисле-
ния, а можно вместо этого думать об операции в целом.
В качестве примера рассмотрим использование библиотеки Pandas для выполнения
показанных на рис. 3.1 вычислений. Начнем с создания входного объекта DataFrame:
In[11]:
\1nge(6)},
\1\1
In[12]: df.groupby('key')
Out[12]: <pandas.core.groupby.DataFrameGroupBy object at 0
\1\1
In[13]: df.groupby('key').sum()
Out[13]: | data
         key
A | 3
B | 5
C | 7

\1лишь один из возможных вариантов в этой команде. Здесь можно
использовать практически любую распространенную агрегирующую функцию
библиотек Pandas или NumPy, равно как и практически любую корректную опе-
рацию объекта DataFrame.

============================================================

\1
In[14]: planets.groupby('method')
Out[14]: <pandas.core.groupby.DataFrameGroupBy object at 0
\1n[15]: planets.groupby('method')['orbital_period']
Out[15]: <pandas.core.groupby.SeriesGroupBy object at 0
\1\1
In[16]: planets.groupby('method')['orbital_period'].median()
Out[16]: method
Astrometry | 631.180000
Eclipse Timing Variations | 4343.500000
Imaging | 27500.000000
Microlensing | 3300.000000
Orbital Brightness Modulation | 0.342887
Pulsar Timing | 66.541900
Pulsation Timing Variations | 1170.000000
Radial Velocity | 360.200000
Transit | 5.714932
Transit Timing Variations | 57.011000
         Name: orbital_period, dtype: float64

\1

============================================================
СТРАНИЦА 204
============================================================

\1ndas
In[17]: for (method, group) in planets.groupby('method'):
            print("{0:30s}
\1ng Variations |
\1ng |
\1nsing |
\1ness Modulation
\1ng |
\1n Timing Variations |
\1nsit |
\1nsit Timing Variations |
\1n все
методы, не реализованные явным образом объектом GroupBy, будут передаваться далее
и выполняться для групп, вне зависимости от того, являются ли они объектами Series
или DataFrame. Например, можно использовать метод describe() объекта DataFrame
для вычисления набора сводных показателей, описывающих каждую группу в данных:
In[18]: planets.groupby('method')['year'].describe().unstack()
Out[18]:
count | mean | std | min | 25%
\\
method
Astrometry | 2.0  2011.500000  2.121320  2010.0  2010.75
Eclipse Timing Variations | 9.0  2010.000000  1.414214  2008.0  2009.00
Imaging | 38.0  2009.131579  2.781901  2004.0  2008.00
Microlensing | 23.0  2009.782609  2.859697  2004.0  2008.00
Orbital Brightness Modulation | 3.0  2011.666667  1.154701  2011.0  2011.00
Pulsar Timing | 5.0  1998.400000  8.384510  1992.0  1992.00
Pulsation Timing Variations | 1.0  2007.000000 | NaN  2007.0  2007.00
Radial Velocity | 553.0  2007.518987  4.249052  1989.0  2005.00
Transit | 397.0  2011.236776  2.077867  2002.0  2010.00
Transit Timing Variations | 4.0  2012.500000  1.290994  2011.0  2011.75
50% | 75% | max
method
Astrometry | 2011.5  2012.25  2013.0
Eclipse Timing Variations | 2010.0  2011.00  2012.0
Imaging | 2009.0  2011.00  2013.0
Microlensing | 2010.0  2012.00  2013.0
Orbital Brightness Modulation 2011.0  2012.00  2013.0
Pulsar Timing | 1994.0  2003.00  2011.0
Pulsation Timing Variations | 2007.0  2007.00  2007.0
Radial Velocity | 2009.0  2011.00  2014.0
Transit | 2012.0  2013.00  2014.0
Transit Timing Variations | 2012.5  2013.25  2014.0

============================================================

\1мер, большинство планет было открыто методом измерения лучевой скорости
(radial velocity method) и транзитным методом (transit method), хотя последний
стал распространенным благодаря новым более точным телескопам только в по-
следнее десятилетие. Похоже, что новейшими методами являются метод вариации
времени транзитов (transit timing variation method) и метод модуляции орбиталь-
ной яркости (orbital brightness modulation method),  которые до 2011 года не ис -
пользовались для открытия новых планет.

\1единения, но доступны и другие возможности. В частности, у объектов GroupBy
имеются методы aggregate(), filter(), transform() и apply(), эффективно выпол -
няющие множество полезных операций до объединения сгруппированных данных.

\1
In[19]:
\1np.random.RandomState(0)

\1nge(6),
                           'data2': rng.randint(0, 10, 6)},

\1n() и т. п., но метод aggregate() обе-
спечивает еще большую гибкость. Он может принимать на входе строку, функцию
или список и вычислять все сводные показатели сразу. Вот пример, включающий
все вышеупомянутое:
In[20]: df.groupby('key').aggregate(['min', np.median, max])
Out[20]: | data1 |
\1\1ndas
min median max | min median max
         key
A | 0 | 1.5 | 3 | 3 | 4.0 | 5
B | 1 | 2.5 | 4 | 0 | 3.5 | 7
C | 2 | 3.5 | 5 | 3 | 6.0 | 9

\1
In[21]: df.groupby('key').aggregate({'data1': 'min',
                                     'data2': 'max'})
Out[21]: | data1  data2
         key
A | 0 | 5
B | 1 | 7
C | 2 | 9

\1
In[22]:
def filter_func(x):
    return x['data2'].std() > 4
print(df); print(df.groupby('key').std());
print(df.groupby('key').filter(filter_func))
df | df.groupby('key').std()
key  data1  data2 | key | data1 | data2
0 | A | 0 | 5 | A | 2.12132  1.414214
1 | B | 1 | 0 | B | 2.12132  4.949747
2 | C | 2 | 3 | C | 2.12132  4.242641
3 | A | 3 | 3
4 | B | 4 | 7
5 | C | 5 | 9
df.groupby('key').filter(filter_func)
  key  data1  data2
1 | B | 1 | 0
2 | C | 2 | 3
4 | B | 4 | 7
5 | C | 5 | 9

\1
In[23]: df.groupby('key').transform(lambda x: x - x.mean())
Out[23]: | data1  data2
0 | -1.5 | 1.0
1 | -1.5 | -3.5
2 | -1.5 | -3.0
3 | 1.5 | -1.0
4 | 1.5 | 3.5
5 | 1.5 | 3.0

\1ект DataFrame, а возвращать или объект библиотеки Pandas (например, DataFrame,
Series), или скалярное значение, в зависимости от возвращаемого значения будет
вызвана соответствующая операция объединения.

\1
In[24]: def norm_by_data2(x):
            # x – объект DataFrame сгруппированных значений
            x['data1'] /= x['data2'].sum()
            return x
        print(df); print(df.groupby('key').apply(norm_by_data2))
df | df.groupby('key').apply(norm_by_data2)
key  data1  data2 | key | data1  data2
0 | A | 0 | 5 | 0 | A  0.000000 | 5
1 | B | 1 | 0 | 1 | B  0.142857 | 0
2 | C | 2 | 3 | 2 | C  0.166667 | 3
3 | A | 3 | 3 | 3 | A  0.375000 | 3
4 | B | 4 | 7 | 4 | B  0.571429 | 7
5 | C | 5 | 9 | 5 | C  0.416667 | 9
Функция apply() в GroupBy достаточно гибка. Единственное требование, чтобы она
принимала на входе объект DataFrame и возвращала объект библиотеки Pandas или
скалярное значение; что вы делаете внутри, остается на ваше усмотрение!

\1вания групп, и мы сейчас рассмотрим некоторые другие возможности.

============================================================
СТРАНИЦА 208
============================================================

\1ndas

\1
In[25]:
\1nt(df); print(df.groupby(L).sum())
df | df.groupby(L).sum()
key  data1  data2 | data1  data2
0 | A | 0 | 5 | 0 | 7 | 17
1 | B | 1 | 0 | 1 | 4 | 3
2 | C | 2 | 3 | 2 | 4 | 7
3 | A | 3 | 3
4 | B | 4 | 7
5 | C | 5 | 9

\1
In[26]: print(df); print(df.groupby(df['key']).sum())
df | df.groupby(df['key']).sum()
key  data1  data2 | data1  data2
0 | A | 0 | 5 | A | 3 | 8
1 | B | 1 | 0 | B | 5 | 7
2 | C | 2 | 3 | C | 7 | 12
3 | A | 3 | 3
4 | B | 4 | 7
5 | C | 5 | 9

\1
In[27]:
\1ndex('key')

\1nsonant', 'C': 'consonant'}
        print(df2); print(df2.groupby(mapping).sum())
df2 | df2.groupby(mapping).sum()
key  data1  data2 | data1  data2
A | 0 | 5 | consonant | 12 | 19
B | 1 | 0 | vowel | 3 | 8
C | 2 | 3
A | 3 | 3
B | 4 | 7
C | 5 | 9
Любая функция языка Python.  Аналогично заданию соответствия можно пере -
дать функции groupby любую функцию, принимающую на входе значение индекса
и возвращающую группу:

============================================================
СТРАНИЦА 209
============================================================
Агрегирование и группировка  209
In[28]: print(df2); print(df2.groupby(str.lower).mean())
df2 | df2.groupby(str.lower).mean()
key  data1  data2 | data1  data2
A | 0 | 5 | a | 1.5 | 4.0
B | 1 | 0 | b | 2.5 | 3.5
C | 2 | 3 | c | 3.5 | 6.0
A | 3 | 3
B | 4 | 7
C | 5 | 9

\1
In[29]: df2.groupby([str.lower, mapping]).mean()
Out[29]: | data1  data2
a vowel | 1.5 | 4.0
b consonant | 2.5 | 3.5
c consonant | 3.5 | 6.0
Пример группировки
В качестве примера соберем все это вместе в нескольких строках кода на языке
Python и подсчитаем количество открытых планет по методу открытия и десяти-
летию:
In[30]:
\1nets['year'] // 10)

\1
\1nets.groupby(['method', decade])
                        ['number'].sum().unstack().fillna(0)
Out[30]: decade | 1980s  1990s  2000s  2010s
         method
Astrometry | 0.0 | 0.0 | 0.0 | 2.0
Eclipse Timing Variations | 0.0 | 0.0 | 5.0 | 10.0
Imaging | 0.0 | 0.0 | 29.0 | 21.0
Microlensing | 0.0 | 0.0 | 12.0 | 15.0
Orbital Brightness Modulation | 0.0 | 0.0 | 0.0 | 5.0
Pulsar Timing | 0.0 | 9.0 | 1.0 | 1.0
Pulsation Timing Variations | 0.0 | 0.0 | 1.0 | 0.0
Radial Velocity | 1.0 | 52.0  475.0  424.0
Transit | 0.0 | 0.0 | 64.0  712.0
Transit Timing Variations | 0.0 | 0.0 | 0.0 | 9.0
Это демонстрирует возможности комбинирования нескольких из вышеописанных
операций применительно к реальным наборам данных. Мы мгновенно получили
представление о том, когда и как открывались экзопланеты в последние несколько
десятилетий!

============================================================
СТРАНИЦА 210
============================================================

\1ndas

\1схожая
операция, часто встречающаяся в электронных таблицах и других программах,
работающих с табличными данными. Сводная таблица получает на входе простые
данные в виде столбцов и группирует записи в двумерную таблицу, обеспечива-
ющую многомерное представление данных. Различие между сводными табли -
цами и операцией GroupBy иногда неочевидно. Лично мне помогает представлять
сводные таблицы как многомерную версию агрегирующей функции GroupBy .

\1хода «Титаник», доступной через библиотеку Seaborn (см. раздел «Визуализация
с помощью библиотеки Seaborn» главы 4):
In[1]: import numpy as np
       import pandas as pd
       import seaborn as sns

\1ns.load_dataset('titanic')
In[2]: titanic.head()
Out[2]:
survived  pclass | sex | age  sibsp  parch | fare embarked  class  \\
0 | 0 | 3 | male  22.0 | 1 | 0 | 7.2500 | S  Third
1 | 1 | 1  female  38.0 | 1 | 0  71.2833 | C  First
2 | 1 | 3  female  26.0 | 0 | 0 | 7.9250 | S  Third
3 | 1 | 1  female  35.0 | 1 | 0  53.1000 | S  First
4 | 0 | 3 | male  35.0 | 0 | 0 | 8.0500 | S  Third
     who adult_male deck  embark_town alive  alone
0 | man | True  NaN  Southampton | no  False
1  woman | False | C | Cherbourg | yes  False
2  woman | False  NaN  Southampton | yes |
\1n | False | C  Southampton | yes  False
4 | man | True  NaN  Southampton | no | True

\1
In[3]: titanic.groupby('sex')[['survived']].mean()
Out[3]: | survived
        sex
        female  0.742038
male | 0.188908

\1ющую функцию среднего значения, объединить получившиеся группы, после чего
выполнить операцию unstack  иерархического индекса, чтобы обнажить скрытую
многомерность. В виде кода:
In[4]: titanic.groupby(['sex', 'class'])
                       ['survived'].aggregate('mean').unstack()
Out[4]: class | First | Second | Third
        sex
        female  0.968085  0.921053  0.500000
male | 0.368852  0.157407  0.135447

\1вания. Двумерный GroupBy встречается настолько часто, что в состав библиотеки
Pandas был включен удобный метод, pivot_table, позволяющий описывать более
кратко данную разновидность многомерного агрегирования.

============================================================
СТРАНИЦА 212
============================================================

\1ndas

\1
In[5]: titanic.pivot_table('survived',
\1
\1nd | Third
        sex
        female  0.968085  0.921053  0.500000
male | 0.368852  0.157407  0.135447

\1
In[6]:
\1nic['age'], [0, 18, 80])
       titanic.pivot_table('survived', ['sex', age], 'class')
Out[6]: | class | First | Second | Third
sex | age
female (0, 18] | 0.909091  1.000000  0.511628
                 (18, 80]  0.972973  0.900000  0.423729
male | (0, 18] | 0.800000  0.600000  0.215686
                 (18, 80]  0.375000  0.071429  0.133663

\1
In[7]:
\1nic['fare'], 2)
       titanic.pivot_table('survived', ['sex', age], [fare, 'class'])
Out[7]:
fare | [0, 14.454]
class | First | Second | Third | \\
sex | age
female (0, 18] | NaN  1.000000  0.714286
(18, 80] | NaN  0.880000  0.444444
male | (0, 18] | NaN  0.000000  0.260870
(18, 80] | 0.0  0.098039  0.125000

============================================================
СТРАНИЦА 213
============================================================
Сводные таблицы  213
fare | (14.454, 512.329]
class | First | Second | Third
sex | age
female (0, 18] | 0.909091  1.000000  0.318182
(18, 80] | 0.972973  0.914286  0.391304
male | (0, 18] | 0.800000  0.818182  0.178571
(18, 80] | 0.391304  0.030303  0.192308

\1
# сигнатура вызова в версии 0.181 библиотеки Pandas
DataFrame.pivot_table(data,
\1ne,
\1ne,
\1ne,

\1n',
\1ne,
\1
\1
\1na, относятся к про -
пущенным значениям и интуитивно понятны, примеры их использования мы
приводить не будем.
Ключевое слово aggfunc управляет тем, какой тип агрегирования применяется,
по умолчанию это среднее значение. Как и в GroupBy, спецификация агрегирую -
щей функции может быть строкой с одним из нескольких обычных вариантов
( 'sum', 'mean', 'count', 'min', 'max' и т. д.) или функцией, реализующей агреги -
рование ( np.sum(), min(), sum() и т. п.). Кроме того, агрегирование может быть
задано в виде словаря, связывающего столбец с любым из вышеперечисленных
вариантов:
In[8]: titanic.pivot_table(
\1
\1
\1n'})
Out[8]: | fare | survived
class | First | Second | Third | First Second Third
        sex
female  106.125798  21.970121  16.118810 | 91.0 | 70.0  72.0
male | 67.226127  19.741782  12.661633 | 45.0 | 17.0  47.0
Обратите внимание, что мы опустили ключевое слово values, при задании aggfunc
происходит автоматическое определение.

\1ndas
Иногда бывает полезно вычислять итоги по каждой группе. Это можно сделать
с помощью ключевого слова margins:
In[9]: titanic.pivot_table('survived',
\1
\1
\1nd | Third | All
        sex
        female  0.968085  0.921053  0.500000  0.742038
male | 0.368852  0.157407  0.135447  0.188908
All | 0.629630  0.472826  0.242363  0.383838

\1мости от класса, коэффициенте выживаемости по классу вне зависимости от пола
и общем коэффициенте выживаемости 38 %. Метки для этих итогов можно задать
с помощью ключевого слова margins_name, по умолчанию имеющего значение "All".

\1емости в США, предоставляемые центрами по контролю заболеваний (Centers for
Disease Control, CDC). Данные можно найти по адресу https://raw.githubusercontent.
com/jakevdp/data-CDCbirths/master/births.csv  (этот набор данных довольно широко
 исследовался Эндрю Гелманом и его группой (см., например, сообщение в блоге
http://bit.ly/2fZzW8K)):
In[10]:
# Инструкция системного командного процессора для скачивания данных:
# !curl -O https://raw.githubusercontent.com/jakevdp/data-CDCbirths/
# master/births.csv
In[11]:
\1\1
In[12]: births.head()
Out[12]: | year  month day gender  births
0  1969 | 1 | 1 | F | 4046
1  1969 | 1 | 1 | M | 4440
2  1969 | 1 | 2 | F | 4454
3  1969 | 1 | 2 | M | 4548
4  1969 | 1 | 3 | F | 4548

\1

============================================================
СТРАНИЦА 215
============================================================
Сводные таблицы  215
In[13]:
births['decade'] = 10 * (births['year'] // 10)
births.pivot_table('births',
\1
\1nder',
\1nder | F | M | decade
1960 | 1753634 | 1846572
1970 | 16263075  17121550
1980 | 18310351  19243452
1990 | 19479454  20420553
2000 | 18229309  19106428

\1вочек. Воспользуемся встроенными средствами построения графиков библиотеки
Pandas для визуализации общего количества новорожденных в зависимости от года
(рис. 3.2; см. обсуждение построения графиков с помощью библиотеки Matplotlib
в главе 4):
In[14]:
%matplotlib inline
import matplotlib.pyplot as plt
sns.set()  # Используем стили библиотеки Seaborn
births.pivot_table('births',
\1
\1nder',

\1ndas. Нам придется начать с небольшой очистки данных, удалив
аномальные значения, возникшие из-за неправильно набранных дат (например,
31 июня) или отсутствующих значений (например, 99 июня). Простой способ
убрать сразу их все — отсечь аномальные значения. Мы сделаем это с помощью
надежного алгоритма сигма-отсечения (sigma-clipping) 1 :
In[15]:
\1np.percentile(births['births'], [25, 50, 75])

\1\1ning and Machine Learning in Astronomy: A Practical Python Guide for the Analysis
of Survey Data (Princeton University Press, 2014).

============================================================
СТРАНИЦА 216
============================================================

\1ndas

\1межквартильный размах Гауссового распределения. Теперь можно
воспользоваться методом query() (обсуждаемым далее в разделе «Увеличение про-
изводительности библиотеки Pandas: eval() и query()» этой главы) для фильтрации
строк, в которых количество новорожденных выходит за пределы этих значений:
In[16]:

\1null':
In[17]: # делаем тип столбца 'day' целочисленным;
# изначально он был строчным из-за пустых значений
        births['day'] = births['day'].astype(int)

\1
In[18]: # создаем индекс для даты из года, месяца и дня
        births.
\1nth +
                                      births.day,
\1ndex.dayofweek  # День недели

============================================================

\1
In[19]:
import matplotlib.pyplot as plt
import matplotlib as mpl
births.pivot_table('births',
\1
\1
\1n').plot()
plt.gca().set_xticklabels(['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat',
'Sun'])
plt.ylabel('mean births by day'); # среднее количество новорожденных в день

\1
In[20]:

\1ndex.month, births.index.day])
births_by_date.head()
Out[20]: 1  1 | 4009.225
2 | 4247.400

============================================================
СТРАНИЦА 218
============================================================

\1ndas
3 | 4500.900
4 | 4571.350
5 | 4603.625
         Name: births, dtype: float64
Результат представляет собой мультииндекс по месяцам и дням. Чтобы упростить
построение графика, преобразуем эти месяцы и дни в даты путем связывания их
с фиктивным годом (обязательно выберите високосный год, чтобы обработать
29 февраля корректным образом!)
In[21]: births_by_date.
\1nth, day)
                                for (month, day) in births_by_date.index]
        births_by_date.head()
Out[21]: 2012-01-01 | 4009.225
2012-01-02 | 4247.400
2012-01-03 | 4500.900
2012-01-04 | 4571.350
2012-01-05 | 4603.625
         Name: births, dtype: float64

\1
In[22]: # Строим график результатов
        fig,
\1n и библиотеки Pandas можно комбинировать
между собой и использовать, чтобы почерпнуть полезную информацию из мно -
жества наборов данных. Более сложные манипуляции над данными мы увидим
в следующих разделах!
Векторизованные операции над строками
Одна из сильных сторон языка Python — относительное удобство работы в нем со
строковыми данными и манипуляций ими. Библиотека Pandas вносит в это свою
лепту и предоставляет набор векторизованных операций над строками , ставших
существенной частью очистки данных, необходимой при работе с реальными
данными. В этом разделе мы изучим некоторые строковые операции библиотеки
Pandas, после чего рассмотрим их использование для частичной очистки очень за -
шумленного набора данных рецептов, собранных в Интернете.
Знакомство со строковыми операциями
библиотеки Pandas

\1кие инструменты, как библиотека NumPy и библиотека Pandas, позволяя легко
и быстро выполнять одну и ту же операцию над множеством элементов массива.

\1
In[1]: import numpy as np

\1np.array([2, 3, 5, 7, 11, 13])
       x * 2
Out[1]: array([ 4,  6, 10, 14, 22, 26])

\1

============================================================
СТРАНИЦА 220
============================================================

\1ndas
In[2]:
\1n data]
Out[2]: ['Peter', 'Paul', 'Mary', 'Guido']

\1
In[3]:
\1ne, 'MARY', 'gUIDO']
       [s.capitalize() for s in data]
---------------------------------------------------------------------------
---------------------------------------------------------------------------
AttributeError | Traceback (most recent call last)
<ipython-input-3-
\1
\1ne, 'MARY', 'gUIDO']
----> 2 [s.capitalize() for s in data]
<ipython-input-3-
\1
\1ne, 'MARY', 'gUIDO']
----> 2 [s.capitalize() for s in data]
AttributeError: 'NoneType' object has no attribute 'capitalize'
---------------------------------------------------------------------------
Библиотека Pandas включает средства как для работы с векторизованными стро-
ковыми операциями, так и для корректной обработки отсутствующих значений
посредством атрибута str объектов Series библиотеки Pandas и содержащих стро -
ки объектов Index. Так, допустим, мы создали объект Series библиотеки Pandas
с теми же данными:
In[4]: import pandas as pd

\1names
Out[4]: 0 | peter
1 | Paul
2 | None
3 | MARY
4 | gUIDO
        dtype: object

\1
In[5]: names.str.capitalize()

============================================================
СТРАНИЦА 221
============================================================
Векторизованные операции над строками  221
Out[5]: 0 | Peter
1 | Paul
2 | None
3 | Mary
4 | Guido
        dtype: object

\1автодополнения для этого атрибута str можно получить список
всех векторизованных строковых методов, доступных в библиотеке Pandas.
Таблицы методов работы со строками
библиотеки Pandas
Если вы хорошо разбираетесь в манипуляции строковыми д анными в языке
Python, львиная доля синтаксиса работы со строками библиотеки Pandas будет вам
интуитивно понятна настолько, что достаточно, наверное, просто привести таблицу
имеющихся методов. С этого и начнем, прежде чем углубимся в некоторые нюансы.

\1
In[6]:
\1n', 'John Cleese', 'Terry Gilliam',
                          'Eric Idle', 'Terry Jones', 'Michael Palin'])
Методы, аналогичные строковым методам языка Python
Практически для всех встроенных строковых методов Python есть соответ -
ствующий векторизованный строковый метод библиотеки Pandas. Вот список
методов атрибута str библиотеки Pandas, дублирующий строковые методы
языка Python:
len() | lower() | translate() | islower()
ljust() | upper() | startswith() | isupper()
rjust() | find() | endswith() | isnumeric()
center() | rfind() | isalnum() | isdecimal()
zfill() | index() | isalpha() | split()
strip() | rindex() | isdigit() | rsplit()
rstrip() | capitalize() | isspace() | partition()
lstrip() | swapcase() | istitle() | rpartition()

\1
In[7]: monte.str.lower()
Out[7]: 0 | graham chapman
1 | john cleese
2 | terry
\1\1ndas
3 | eric idle
4 | terry jones
5 | michael palin
        dtype: object

\1
In[8]: monte.str.len()
Out[8]: 0 | 14
1 | 11
2 | 13
3 | 9
4 | 11
5 | 13
        dtype: int64

\1
In[9]: monte.str.startswith('T')
Out[9]: 0 | False
1 | False
2 | True
3 | False
4 | True
5 | False
        dtype: bool

\1
In[10]: monte.str.split()
Out[10]: 0 | [Graham, Chapman]
1 | [John, Cleese]
2 | [Terry, Gilliam]
3 | [Eric, Idle]
4 | [Terry, Jones]
5 | [Michael, Palin]
         dtype: object

\1лярные выражения для проверки содержимого каждого из строковых  элементов
и следующих некоторым соглашениям по API встроенного модуля re языка Python
(табл. 3.4).

============================================================
СТРАНИЦА 223
============================================================
Векторизованные операции над строками  223
Таблица 3.4. Соответствие между методами библиотеки Pandas и функциями модуля re языка
Python

\1щие группы в виде строк
findall() Вызывает функцию re.findall() для каждого элемента
replace() Заменяет вхождения шаблона какой-либо другой строкой
contains() Вызывает функцию re.search() для каждого элемента, возвращая булево
значение
count() Подсчитывает вхождения шаблона
split() Эквивалент функции str.split(), но принимающий на входе регулярные
выражения
rsplit() Эквивалент функции str.rsplit(), но принимающий на входе регулярные
выражения

\1
In[11]: monte.str.extract('([A-Za-z]+)')
Out[11]: 0 | Graham
1 | John
2 | Terry
3 | Eric
4 | Terry
5 | Michael
         dtype: object

\1
In[12]: monte.str.findall(r'^[^AEIOU].*[^aeiou]$')
Out[12]: 0 | [Graham Chapman]
1 | []
2 | [Terry Gilliam]
3 | []
4 | [Terry Jones]
5 | [Michael Palin]
         dtype: object
Такой сжатый синтаксис регулярных выражений для запи сей объектов Series
и DataFrame открывает массу возможностей для анализа и очистки данных.

============================================================
СТРАНИЦА 224
============================================================

\1ndas
Прочие методы
Наконец, существуют и прочие методы, пригодные для разных удобных операций
(табл. 3.5).
Таблица 3.5. Прочие методы для работы со строками библиотеки Pandas
Метод Описание
get() Индексирует все элементы
slice() Вырезает подстроку из каждого элемента
slice_replace() Заменяет в каждом элементе вырезанную подстроку заданным значением
cat() Конкатенация строк
repeat() Повторяет значения (указанное число раз)
normalize() Возвращает версию строки в кодировке Unicode
pad() Добавляет пробелы слева, справа или с обеих сторон строки
wrap() Разбивает длинные строковые значения на строки длины, не превыша-
ющей заданную
join() 1 Объединяет строки из всех элементов с использованием заданного раз-
делителя
get_dummies() Извлекает значения переменных-индикаторов в виде объекта DataFrame

\1сиса индексации языка Python, например, df.str.slice(0, 3)  эквивалентно
df.str[0:3]:
In[13]: monte.str[0:3]
Out[13]: 0 | Gra
1 | Joh
2 | Ter
3 | Eri
4 | Ter
5 | Mic
         dtype: object
Индексация посредством df.str.get(i) и df.str[i] происходит аналогично.

\1n(monte)
 где ; — разделитель.

============================================================

\1
In[14]: monte.str.split().str.get(-1)
Out[14]: 0 | Chapman
1 | Cleese
2 | Gilliam
3 | Idle
4 | Jones
5 | Palin
         dtype: object

\1
In[15]:

\1name': monte,
                           'info': ['B|C|D', 'B|D', 'A|C', 'B|D', 'B|C',
                           'B|C|D']})
full_monte
Out[15]: | info | name
         0  B|C|D  Graham Chapman
1 | B|D | John Cleese
2 | A|C | Terry Gilliam
3 | B|D | Eric Idle
4 | B|C | Terry Jones
5  B|C|D | Michael Palin

\1
In[16]: full_monte['info'].str.get_dummies('|')
Out[16]: | A  B  C  D
         0  0  1  1  1
         1  0  1  0  1
         2  1  0  1  0
         3  0  1  0  1
         4  0  1  1  0
         5  0  1  1  1
Используя эти операции как «строительные блоки», можно создать бесчисленное
множество обрабатывающих строки процедур для очистки данных.
Мы не будем углубляться в эти методы, но я рекомендую прочитать раздел Working
with Text Data («Работа с текстовыми данными») из онлай н-документации

============================================================
СТРАНИЦА 226
============================================================

\1ndas
библиотеки  Pandas ( http://pandas.pydata.org/pandas-docs/stable/text.html) или заглянуть
в раздел «Дополнительные источники информации» данной главы.

\1разбор рецептов на спи-
ски ингредиентов, чтобы можно было быстро найти рецепт, исходя из име ющихся
в распоряжении ингредиентов.
Используемые для компиляции сценарии можно найти по адресу https://github.com/
fictivekin/openrecipes, как и ссылку на актуальную версию базы.

\1
In[17]: # !curl -O
        # http://openrecipes.s3.amazonaws.com/20131812-recipeitems.json.gz
        # !gunzip 20131812-recipeitems.json.gz

\1ваться функцией pd.read_json для ее чтения:
In[18]: try:

\1n('recipeitems-latest.json')
        except ValueError as e:
            print("ValueError:", e)
ValueError: Trailing data

\1
In[19]: with open('recipeitems-latest.json') as f:

\1ne()
        pd.read_json(line).shape
Out[19]: (2, 12)

\1фактически сформировать строковое
представление, содержащее все записи JSON, после чего загрузить все с помощью
pd.read_json:

============================================================
СТРАНИЦА 227
============================================================
Векторизованные операции над строками  227
In[20]: # Читаем весь файл в массив Python
        with open('recipeitems-latest.json', 'r') as f:
            # Извлекаем каждую строку

\1ne.strip() for line in f)
            # Преобразуем так, чтобы каждая строка была элементом списка

\1n(data))
        # Читаем результат в виде JSON

\1n(data_json)
In[21]: recipes.shape
Out[21]: (173278, 17)

\1
In[22]: recipes.iloc[0]
Out[22]:
_id | {'$oid': '5160756b96cc62079cc2db15'}
cookTime | PT30M
creator | NaN
dateModified | NaN
datePublished | 2013-03-11
description | Late Saturday afternoon, after Marlboro Man ha...
image | http://static.thepioneerwoman.com/cooking/file...
ingredients | Biscuits\n3 cups All-purpose Flour\n2 Tablespo...
name | Drop Biscuits and Sausage Gravy
prepTime | PT10M
recipeCategory | NaN
recipeInstructions | NaN
recipeYield | 12
source | thepioneerwoman
totalTime | NaN
ts | {'$date': 1365276011104}
url | http://thepioneerwoman.com/cooking/2013/03/dro...
Name: 0, dtype: object

\1
In[23]: recipes.ingredients.str.len().describe()
Out[23]: count | 173278.000000
mean | 244.617926
std | 146.705285
min | 0.000000
25% | 147.000000

============================================================
СТРАНИЦА 228
============================================================

\1ndas
50% | 221.000000
75% | 314.000000
max | 9067.000000
         Name: ingredients, dtype: float64

\1
In[24]: recipes.name[np.argmax(recipes.ingredients.str.len())]
Out[24]: 'Carrot Pineapple Spice &amp; Brownie Layer Cake with Whipped Cream
&amp; Cream Cheese Frosting and Marzipan Carrots'

\1
In[33]: recipes.description.str.contains('[Bb]reakfast').sum()
Out[33]: 3524
Или сколько рецептов содержат корицу (cinnamon) в списке ингредиентов:
In[34]: recipes.ingredients.str.contains('[Cc]innamon').sum()
Out[34]: 10526
Можно даже посмотреть, есть ли рецепты, в которых название этого ингредиента
написано с орфографической ошибкой, как cinamon:
In[27]: recipes.ingredients.str.contains('[Cc]inamon').sum()
Out[27]: 11

\1на благодаря инструментам по работе со строками библиотеки Pandas. Именно
в сфере такой очистки данных Python действительно силен.

\1
In[28]:
\1no', 'sage', 'parsley',
                      'rosemary', 'tarragon', 'thyme', 'paprika', 'cumin']

\1
In[29]:
import re

\1ngredients.str.contains(spice, re.IGNORECASE))
                                              for spice in spice_list))
spice_df.head()
Out[29]:
cumin oregano paprika parsley pepper rosemary | sage | salt tarragon  thyme
0  False | False | False | False  False | False | True  False | False  False
1  False | False | False | False  False | False  False  False | False  False
2 | True | False | False | False | True | False  False | True | False  False
3  False | False | False | False  False | False  False  False | False  False
4  False | False | False | False  False | False  False  False | False  False

\1пользуются петрушка (parsley), паприка (paprika) и эстрагон (tarragon). Это мож-
но сделать очень быстро, воспользовавшись методом query() объекта DataFrame,
который мы обсудим подробнее в разделе «Увеличение производительности би-
блиотеки Pandas: eval() и query()» данной главы:
In[30]:
\1n')
        len(selection)
Out[30]: 10

\1
In[31]: recipes.name[selection.index]
Out[31]: 2069 | All cremat with a Little Gem, dandelion and wa...
74964 | Lobster with Thermidor butter
93768 | Burton's Southern Fried Chicken with White Gravy
113926 | Mijo's Slow Cooker Shredded Beef
137686 | Asparagus Soup with Poached Eggs
140530 | Fried Oyster Po'boys
158475 | Lamb shank tagine with herb tabbouleh
158486 | Southern fried chicken in buttermilk
163175 | Fried Chicken Sliders with Pickles + Slaw
165243 | Bar Tartine Cauliflower Salad
         Name: name, dtype:
\1\1ndas

\1бам!)
операции по очистке данных, которые хорошо выполняются с помощью строковых
методов библиотеки Pandas. Создание надежной рекомендательной системы для
рецептов потребовало бы намного больше труда! Важной частью этой задачи было
бы извлечение из каждого рецепта полного списка ингредиентов. К сожалению,
разнообразие используемых форматов делает этот процесс весьма трудоемким.

\1ших из реального мира данных часто представляет собой основную часть работы,
и библиотека Pandas предоставляет инструменты для эффективного выполнения
этой задачи.
Работа с временными рядами
Библиотека Pandas была разработана в расчете на построение финансовых мо -
делей, так что, как вы могли и ожидать, она содержит весьма широкий набор
инструментов для работы с датой, временем и индекси рованными по времени
данными. Данные о дате и времени могут находиться в нескольких видах, которые
мы сейчас обсудим.
 Метки даты/времени  ссылаются на конкретные моменты времени (например,
4 июля 2015 года в 07:00 утра).
 Временные интервалы и периоды ссылаются на отрезки времени между конкрет-
ными начальной и конечной точками (например, 2015 год). Периоды обычно
представляют собой особый случай интервалов, с непересекающимися интер-
валами одинаковой длительности (например, 24-часовые пе риоды времени,
составляющие сутки).
 Временная дельта (она же продолжительность) относится к отрезку времени
конкретной длительности (например, 22,56 с).
В данном разделе мы расскажем, как работать с каждым из этих типов временных
данных в библиотеке Pandas. Короткий раздел никоим образом не претендует на
звание исчерпывающего руководства по имеющимся в Python или библиотеке
Pandas инструментам работы с временными рядами. Он представляет собой обзор
работы с временными рядами в общих чертах. Мы начнем с краткого обсуждения
инструментов для работы с датой и временем в языке Python, прежде чем перейти
непосредственно к обсуждению инструментов библиотеки Pandas. После перечис -
ления источников углубленной информации мы рассмотрим несколько кратких
примеров работы с данными временных рядов в библиотеке Pandas.

============================================================
СТРАНИЦА 231
============================================================
Работа с временными рядами  231
Дата и время в языке Python
В мире языка Python существует немало представлений дат, времени, временных
дельт и интервалов времени. Хотя для приложений науки о данных наиболее удоб-
ны инструменты работы с временными рядами библиотеки Pandas, не помешает
посмотреть на другие используемые в Python пакеты.
Нативные даты и время языка Python: пакеты datetime и dateutil
Базовые объекты Python для работы с датами и временем располагаются во встро-
енном пакете datetime. Его, вместе со сторонним модулем dateutil, можно исполь -
зовать для быстрого выполнения множества удобных операций над датами и вре-
менем. Например, можно вручную сформировать дату с помощью типа datetime:
In[1]: from datetime import datetime
       datetime(
\1
\1\1
In[2]: from dateutil import parser

\1\1
In[3]: date.strftime('%A')
Out[3]: 'Saturday'

\1матирования строк ( "%A"), о котором можно прочитать в разделе strftime ( https://
docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior ) документации по
пакету datetime ( https://docs.python.org/3/library/datetime.html) языка Python. Докумен -
тацию по другим полезным утилитам для работы с датой и временем можно найти
в онлайн-документации пакета dateutil ( http://labix.org/python-dateutil). Не помешает
также быть в курсе связанного с ними пакета pytz ( http://pytz.sourceforge.net), содер -
жащего инструменты для работы с частью данных временных рядов — часовыми
поясами.

\1сисе: эти объекты и их встроенные методы можно использовать для выполнения
практически любой интересующей вас операции. Единственное, в чем они работают
плохо, это работа с большими массивами дат и времени: подобно спискам числовых

============================================================
СТРАНИЦА 232
============================================================

\1ndas
переменных языка Python, работающим неоптимально по сравнению с типизиро-
ванными числовыми массивами в стиле библиотеки NumPy, списки объектов даты/
времени Python работают с меньшей производительностью, чем типизированные
массивы кодированных дат.
Типизированные массивы значений времени: тип datetime64
библиотеки NumPy
Указанная слабая сторона формата даты/времени языка Python побудила команду
разработчиков библиотеки NumPy добавить набор нативных типов данных вре -
менных рядов. Тип (dtype) datetime64 кодирует даты как 64-битные целые числа,
так что представление массивов дат оказывается очень компактным. Для типа
datetime64 требуется очень точно заданный формат входных данных:
In[4]: import numpy as np

\1np.array('2015-07-04', dtype=np.datetime64)
       date
Out[4]: array(datetime.date(2015, 7, 4),
\1\1
In[5]: date + np.arange(12)
Out[5]:
array(['2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07',
       '2015-07-08', '2015-07-09', '2015-07-10', '2015-07-11',
       '2015-07-12', '2015-07-13', '2015-07-14', '2015-07-15'],

\1n, особенно если речь идет о больших
массивах (мы рассматривали эту разновидность векторизации в разделе «Выпол -
нение вычислений над массивами библиотеки NumPy: универсальные функции»
главы 2).
Важный нюанс относительно объектов datetime64 и timedelta64: они основаны
на базовой единице времени (fundamental time unit). Поскольку объект datetime64
ограничен точностью 64 бита, кодируемый им диапазон времени составляет эту
базовую единицу, умноженную на 2 64 . Другими словами, datetime64 навязывает
компромисс между разрешающей способностью по времени  и максимальным про-
межутком времени.

\1
In[6]: np.datetime64('2015-07-04')
Out[6]: numpy.datetime64('2015-07-04')

\1
In[7]: np.datetime64('2015-07-0412:00')
Out[7]: numpy.datetime64('2015-07-04T12:00')

\1
In[8]: np.datetime64('2015-07-0412:59:59.50', 'ns')
Out[8]: numpy.datetime64('2015-07-04T12:59:59.500000000')

\1ные и абсолютные промежутки времени, которые можно кодировать с их помощью.
Таблица 3.6. Описание кодов форматирования даты и времени
Код Значение Промежуток времени
(относительный)
Промежуток времени (абсолютный)
Y Год ±9.2e18 лет [9.2e18 до н. э., 9.2e18 н. э.]
M Месяц ±7.6e17 лет [7.6e17 до н. э., 7.6e17 н. э.]
W Неделя ±1.7e17 лет [1.7e17 до н. э., 1.7e17 н. э.]
D День ±2.5e16 лет [2.5e16 до н. э., 2.5e16 н. э.]
h Час ±1.0e15 лет [1.0e15 до н. э., 1.0e15 н. э.]
m Минута ±1.7e13 лет [1.7e13 до н. э., 1.7e13 н. э.]
s Секунда ±2.9e12 лет [2.9e9 до н. э., 2.9e9 н. э.]
ms Миллисекунда ±2.9e9 лет [2.9e6 до н. э., 2.9e6 н. э.]
us Микросекунда ±2.9e6 лет [2 90301 до н. э., 2 94241 н. э.]
ns Наносекунда ±292 лет [1678 до н. э., 2262 н. э.]
ps Пикосекунда ±106 дней [1969 до н. э., 1970 н. э.]
fs Фемтосекунда ±2.6 часов [1969 до н. э., 1970 н. э.]
as Аттосекунда ±9.2 секунды [1969 до н. э., 1970 н. э.]

============================================================
СТРАНИЦА 234
============================================================

\1ndas

\1datetime64[ns], позволяющее кодировать достаточный диапазон совре -
менных дат с высокой точностью.
Наконец, отметим, что, хотя тип данных datetime64 лишен некоторых недостатков
встроенного типа данных datetime языка Python, ему недостает многих предо -
ставляемых datetime и особенно dateutil удобных методов и функций. Больше
информации можно найти в документации по типу datetime64 библиотеки NumPy
( http://docs.scipy.org/doc/numpy/reference/arrays.datetime.html).
Даты и время в библиотеке Pandas: избранное из лучшего
Библиотека Pandas предоставляет, основываясь на всех только что обсужда-
вшихся инструментах, объект Timestamp, сочетающий удобство использования
datetime и dateutil с эффективным хранением и векторизованным интерфей -
сом типа numpy.datetime64 . Библиотека Pandas умеет создавать из нескольких
таких объектов Timestamp  объект класса DatetimeIndex , который можно ис -
пользовать для индексации данных в объектах Series или DataFrame. Можно
применить инструменты библиотеки Pandas для воспроизведения вышепри -
веденной наглядной демонстрации. Можно выполнить синтаксический разбор
строки с датой в гибком формате и воспользоваться кодами форматирования,
чтобы вывести день недели:
In[9]: import pandas as pd

\1n[10]: date.strftime('%A')
Out[10]: 'Saturday'

\1
In[11]: date + pd.to_timedelta(np.arange(12), 'D')
Out[11]: DatetimeIndex(['2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07',
                        '2015-07-08', '2015-07-09', '2015-07-10', '2015-07-11',
                        '2015-07-12', '2015-07-13', '2015-07-14', '2015-07-15'],

\1ns]',
\1ne)

\1менных рядов с помощью предоставляемых библиотекой Pandas инструментов.

============================================================
СТРАНИЦА 235
============================================================
Работа с временными рядами  235
Временные ряды библиотеки Pandas: индексация
по времени
Инструменты для работы с временными рядами библиотеки Pandas особенно удоб -
ны при необходимости индексации данных по меткам даты/времени . Например,
создадим объект Series с индексированными по времени данными:
In[12]:
\1ndex(['2014-07-04', '2014-08-04',
                                  '2015-07-04', '2015-08-04'])

\1
\1ndex)
        data
Out[12]: 2014-07-04 | 0
2014-08-04 | 1
2015-07-04 | 2
2015-08-04 | 3
         dtype: int64

\1
In[13]: data['2014-07-04':'2015-07-04']
Out[13]: 2014-07-04 | 0
2014-08-04 | 1
2015-07-04 | 2
         dtype: int64

\1
In[14]: data['2015']
Out[14]: 2015-07-04 | 2
2015-08-04 | 3
         dtype: int64
Позднее мы рассмотрим еще примеры удобства индексации по датам. Но сначала
изучим имеющиеся структуры данных для временных рядов.
Структуры данных для временных рядов
библиотеки Pandas
В этом разделе мы рассмотрим основные структуры данных, предназначенные для
работы с временными рядами.

============================================================
СТРАНИЦА 236
============================================================

\1ndas
 Для меток даты/времени библиотека Pandas предоставляет тип данных Timestamp.
Этот тип является заменой для нативного типа данных datetime  языка Python, он
основан на более эффективном типе данных numpy.datetime64. Соответствующая
индексная конструкция — DatetimeIndex.
 Для периодов времени  библиотека Pandas предоставляет тип данных Period.
Этот тип на основе типа данных numpy.datetime64 кодирует интервал времени
фиксированной периодичности. Соответствующая индексная конструкция —
PeriodIndex.
 Для временных дельт ( продолжительностей) библиотека Pandas предоставляет
тип данных Timedelta. Timedelta — основанная на типе numpy.timedelta64 более
эффективная замена нативного типа данных datetime.timedelta языка Python.

\1TimedeltaIndex.

\1объекты Timestamp и Datetime-
Index. Хотя к ним и можно обращаться непосредственно, чаще используют функ -
цию pd.to_datetime() , умеющую выполнять синтаксический разбор широкого
диапазона форматов. При передаче в функцию pd.to_datetime()  отдельной
даты она возвращает Timestamp, при передаче ряда дат по умолчанию возвращает
DatetimeIndex:
In[15]:
\1ndex(['2015-07-03', '2015-07-04', '2015-07-06', '2015-07-07',
                        '2015-07-08'],

\1ns]',
\1ne)
Любой объект DatetimeIndex можно с помощью функции to_period() преобра -
зовать в объект PeriodIndex, указав код для периодичности интервала. В данном
случае мы использовали код 'D', означающий, что периодичность интервала —
один день:
In[16]: dates.to_period('D')
Out[16]: PeriodIndex(['2015-07-03', '2015-07-04', '2015-07-06', '2015-07-07',
                      '2015-07-08'],

\1nt64',
\1ndex  создается, например, при вычитании одной даты из
другой:
In[17]: dates - dates[0]
Out[17]:
TimedeltaIndex(['0 days', '1 days', '3 days', '4 days', '5 days'],

\1ns]',
\1ne)

============================================================
СТРАНИЦА 237
============================================================
Работа с временными рядами  237
Регулярные последовательности: функция pd.date_range().  Чтобы облегчить
создание регулярных последовательностей, библиотека Pandas предоставляет
несколько функций: pd.date_range()  — для меток даты/времени, pd.period_
range() — для периодов времени и pd.timedelta_range() — для временных дельт.
Мы уже видели, что функции range() языка Python и np.arange() библиотеки
NumPy преобразуют начальную точку, конечную точку и (необязательную)
величину шага в последовательность. Аналогично функция pd.date_range()
создает регулярную последовательность дат, принимая на входе начальную дату,
конечную дату и необязательный код периодичности. По умолчанию период
равен одному дню:
In[18]: pd.date_range('2015-07-03', '2015-07-10')
Out[18]: DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06',
                        '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'],

\1ns]',
\1\1
In[19]: pd.date_range('2015-07-03',
\1ndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06',
                        '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'],

\1ns]',
\1\1
In[20]: pd.date_range('2015-07-03',
\1ndex(['2015-07-0300:00:00', '2015-07-0301:00:00',
                        '2015-07-0302:00:00', '2015-07-0303:00:00',
                        '2015-07-0304:00:00', '2015-07-0305:00:00',
                        '2015-07-0306:00:00', '2015-07-0307:00:00'],

\1ns]',
\1nge()  и pd.timedelta_
range(), напоминающими функцию date_range(). Вот несколько периодов времени
длительностью в месяц:
In[21]: pd.period_range('2015-07',
\1ndex(['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12',
             '2016-01', '2016-02'],

\1nt64',
\1\1ndas

\1
In[22]: pd.timedelta_range(0,
\1ndex(['00:00:00', '01:00:00', '02:00:00', '03:00:00', '04:00:00',
                '05:00:00', '06:00:00', '07:00:00', '08:00:00', '09:00:00'],

\1ns]',
\1ndas, необходимых для работы с временными рядами. Аналогично уже
продемонстрированным кодам D (день) и H (час) можно использовать коды для
задания любой требуемой периодичности. В табл. 3.7 описаны основные суще -
ствующие коды.
Таблица 3.7. Список кодов периодичности библиотеки Pandas

\1
In[23]: pd.timedelta_range(0,
\1ndex(['00:00:00', '02:30:00', '05:00:00', '07:30:00', '10:00:00',
                '12:30:00', '15:00:00', '17:30:00', '20:00:00'],

\1ns]',
\1ndas, которые можно найти в мо -
дуле pd.tseries.offsets. Например, можно непосредственно создать смещение
в один рабочий день следующим образом:
In[24]: from pandas.tseries.offsets import BDay
        pd.date_range('2015-07-01',
\1ndex(['2015-07-01', '2015-07-02', '2015-07-03',
                        '2015-07-06','2015-07-07'],

\1ns]',
\1ndas.pydata.org/pandas-docs/stable/timeseri-
es.html#dateoffset-objects ) онлайн-документации библиотеки Pandas.

\1немаловажная часть инструментария

============================================================
СТРАНИЦА 240
============================================================

\1ndas
библиотеки Pandas по работе с временными рядами. При этом сохраняются общие
преимущества использования индексированных данных (автоматическое выравни-
вание во время операций, интуитивно понятные срезы и доступ к данным и т. д.), но
библиотека Pandas предоставляет еще несколько дополнительных операций специ -
ально для временных рядов.

\1ми по курсам акций. Библиотека Pandas, будучи разработанной в значительной
степени для работы с финансовыми данными, имеет для этой цели несколько
весьма специфических инструментов. Например, сопутствующий Pandas пакет
pandas-datareader (который можно установить с помощью команды conda install
pandas-datareader) умеет импортировать финансовые данные из множества ис -
точников, включая
\1nance, Google Finance и другие.

\1
In[25]: from pandas_datareader import data

\1
\1n | High | Low  Close  Volume
         Date
2004-08-19  49.96  51.98  47.93  50.12 | NaN
2004-08-20  50.69  54.49  50.20  54.10 | NaN
2004-08-23  55.32  56.68  54.47  54.65 | NaN
2004-08-24  55.56  55.74  51.73  52.38 | NaN
2004-08-25  52.43  53.95  51.89  52.95 | NaN

\1
In[26]:
\1\1
In[27]: %matplotlib inline
        import matplotlib.pyplot as plt
        import seaborn; seaborn.set()
In[28]: goog.plot();

\1выборку данных.
Рассмотрим, что возвращают эти два метода для данных по ценам закрытия Google
при понижающей дискретизации данных. Здесь мы выполняем передискретизацию
данных на конец финансового года (рис. 3.6).
Рис. 3.5. График изменения цен акций Google с течением времени
Рис. 3.6. Передискретизация цен акций Google
In[29]: goog.plot(
\1n().plot(
\1\1ndas
        goog.asfreq('BA').plot(
\1nd(['input', 'resample', 'asfreq'],

\1na() метод asfreq() принимает
аргумент method, определяющий, откуда будут браться значения для таких точек.

\1
In[30]: fig,
\1nd(["back-fill", "forward-fill"]);

\1fill interpolation)
и назад (backward-fill interpolation)

============================================================

\1fill interpolation) и интерполяцией назад (back-fill interpolation).

\1сдвиг данных во
времени. В библиотеке Pandas есть два родственных метода для подобных вычис-
лений: shift() и tshift(). Разница между ними заключается в том, что shift()
выполняет сдвиг данных, а tshift() — сдвиг индекса. В обоих случаях сдвиг задается
кратным периоду.

\1
In[31]: fig,
\1nd(['input'],
\1ne(local_max,
\1nd(['shift(900)'],
\1ne(local_max + offset,
\1nd(['tshift(900)'],
\1ne(local_max + offset,
\1\1

\1ndas
In[32]:
\1n on Investment'); # Прибыль от вложений
Рис. 3.8. Сравнение shift и tshift
Рис. 3.9. Прибыль на текущий день от вложений в акции
\1ndas разновидностей операций, предназначенных для временных рядов. Рабо-
тать с ними можно с помощью атрибута rolling() объектов Series и DataFrame,
возвращающего представление, подобное тому, с которым мы сталкивались при
выполнении операции groupby (см. раздел «Агрегирование и группировка» данной
главы). Это скользящее представление предоставляет по умолчанию несколько
операций агрегирования.

\1
In[33]:
\1ng(365,
\1nput': goog,
                             'one-year rolling_mean': rolling.mean(),
                             'one-year rolling_std': rolling.std()})

\1nes[0].set_alpha(0.3)
Рис. 3.10. Скользящие статистические показатели для цен на акции Google
Как и в случае операций groupby , можно использовать методы aggregate()
и apply() для вычисления пользовательских скользящих показателей.

============================================================
СТРАНИЦА 246
============================================================

\1ndas
Где найти дополнительную информацию
В данном разделе приведена лишь краткая сводка некоторых наиболее важных
возможностей инструментов для работы с временными рядами библиотеки Pandas.

\1документации библиотеки Pandas ( http://pan-
das.pydata.org/pandas-docs/stable/timeseries.html).

\1руководство Python for Data
Analysis  издательства O’Reilly ( http://shop.oreilly.com/product/063 69200 23784.do). Хотя
ему уже несколько лет, это бесценный источник информации по использованию
библиотеки Pandas. В частности, в книге сделан особый акцент на применении
инструментов временных рядов в контексте бизнеса и финансов и уделено боль-
ше внимания конкретным деталям бизнес-календаря, работе с часовыми поясами
и связанным с этим вопросам.
Вы также можете воспользоваться справочной функциональностью оболочки
IPython для изучения и экспериментов с другими параметрами, имеющимися
у обсуждавшихся здесь функций и методов. Я считаю, что это оптимальный способ
изучения какого-либо нового инструмента языка Python.

\1le.gov/Transportation/Fremont-Bridge-Hourly-Bicycle-Counts-by-Month-Octo/65db-xm6k .

\1
In[34]:
# !curl -o FremontBridge.csv
# https://data.seattle.gov/api/views/65db-xm6k/rows.csv?
\1ndas для
чтения CSV-файла в объект DataFrame. Можно указать, что в качестве индекса мы
хотим видеть объекты Date и чтобы выполнялся автоматический синтаксический
разбор этих дат:
In[35]:

\1ntBridge.csv',
\1nt Bridge West Sidewalk  \\

============================================================

\10304:00:00 | 6.0
                              Fremont Bridge East Sidewalk
         Date
2012-10-0300:00:00 | 9.0
2012-10-0301:00:00 | 6.0
2012-10-0302:00:00 | 1.0
2012-10-0303:00:00 | 3.0
2012-10-0304:00:00 | 1.0

\1
In[36]: data.
\1\1
In[37]: data.dropna().describe()
Out[37]: | West | East | Total
         count  33544.000000  33544.000000  33544.000000
mean | 61.726568 | 53.541706 | 115.268275
std | 83.210813 | 76.380678 | 144.773983
min | 0.000000 | 0.000000 | 0.000000
25% | 8.000000 | 7.000000 | 16.000000
50% | 33.000000 | 28.000000 | 64.000000
75% | 80.000000 | 66.000000 | 151.000000
max | 825.000000 | 717.000000 | 1186.000000

\1зировав его. Начнем с построения графика исходных данных (рис. 3.11).
In[38]: %matplotlib inline
        import seaborn; seaborn.set()
In[39]: data.plot()
        plt.ylabel('Hourly Bicycle Count'); # Количество велосипедов по часам

\1

============================================================
СТРАНИЦА 248
============================================================

\1ndas
In[40]:
\1nt'); # Количество велосипедов еженедельно

\1вычисление скользящего
среднего с помощью функции pd.rolling_mean(). Здесь мы вычисляем для наших
данных скользящее среднее за 30 дней, центрируя при этом окно (рис. 3.13):
In[41]:
\1ng(30,
\1n hourly count'); # Среднее количество по часам

\1
In[42]:
daily.rolling(50,
\1
\1n').sum(
\1\1ndas

\1
In[43]:
\1ndex.time).mean()

\1np.arange(6)
        by_time.plot(
\1\1n[44]:
\1ndex.dayofweek).mean()
        by_weekday.
\1n', 'Tues', 'Wed', 'Thurs',
                            'Fri', 'Sat', 'Sun']
        by_weekday.plot(
\1\1ndas

\1
In[45]:
\1np.where(data.index.
\1nd')

\1nd, data.index.time]).mean()

\1
In[46]: import matplotlib.pyplot as plt
        fig,
\1nd'].plot(
\1nds',

\1ng-an-uptick-in-cycling/) из моего блога, в котором используется под -
множество этих данных. Мы также вернемся к этому набору данных в контексте
моделирования в разделе «Заглянем глубже: линейная регрессия» главы 5.
Увеличение производительности библиотеки
Pandas: eval() и query()
Основные возможности стека PyData основываются на умении библиотек NumPy
и Pandas передавать простые операции на выполнение программам на языке C по -
средством интуитивно понятного синтаксиса: примерами могут послужить век -
торизованные/транслируемые операции в библиотеке NumPy, а также операции
группировки в библиотеке Pandas. Хотя эти абстракции весьма производительны
и эффективно работают для многих распространенных сценариев использова -
ния, они зачастую требуют создания вр еменных вспомогательных объектов, что
приводит к чрезмерным накладным расходам как процессорного времени, так
и оперативной памяти.

============================================================
СТРАНИЦА 253
============================================================
Увеличение производительности библиотеки Pandas: eval() и query()  253
Рис. 3.17. Среднее количество велосипедов по часам, в рабочие и выходные дни

============================================================
СТРАНИЦА 254
============================================================

\1ndas
По состоянию на версию 0.13 (выпущенную в январе 2014 года) библиотека Pandas
включает некоторые экспериментальные инструменты, позволяющие обращаться
к работающим со скоростью написанных на языке C операциям без выделения  су-
щественных объемов памяти на промежуточные массивы. Эти утилиты — функции
eval() и query(), основанные на пакете Numexpr ( https://github.com/pydata/numexpr).
Мы рассмотрим их использование и приведем некоторые эмпирические правила,
позволяющие решить, имеет ли смысл их применять.
Основания для использования функций query() и eval():
составные выражения
Библиотеки NumPy и Pandas поддерживают выполнение быстрых векторизован-
ных операций; например, при сложении элементов двух массивов:
In[1]: import numpy as np

\1np.random.RandomState(42)

\1ng.rand(1E6)

\1ng.rand(1E6)
       %timeit x + y 100 loops, best of 3: 3.39 ms per loop

\1отеки NumPy: универсальные функции» главы 2, такая операция выполняется
гораздо быстрее, чем сложение с помощью цикла или спискового включения
языка Python:
In[2]:
%timeit np.fromiter((xi + yi for xi, yi in zip(x, y)),

\1
\1n(x))
1 loop, best of 3: 266 ms per loop

\1
In[3]:
\1\1
In[4]:
\1ndas: eval() и query()  255

\1элементно, не требуя выделения памяти под промежуточные массивы целиком.
В документации библиотеки Numexpr ( https://github.com/pydata/numexpr) приведено
больше подробностей, но пока достаточно будет сказать, что функции этой библио-
теки принимают на входе строку , содержащую выражение в стиле библиотеки
NumPy, которое требуется вычислить:
In[5]: import numexpr

\1numexpr.evaluate('(
\1np.allclose(mask, mask_numexpr)
Out[5]: True

\1ние, не используя полноразмерных вр еменных массивов, а потому оказывается
намного более эффективной, чем NumPy, особенно в случае больших массивов.
Инструменты query() и eval(), которые мы будем обсуждать, идеологически схожи
и используют пакет Numexpr.
Использование функции pandas.eval() для эффективных
операций
Функция eval() библиотеки Pandas применяет строковые выражения для эффек -
тивных вычислительных операций с объектами DataFrame. Например, рассмотрим
следующие объекты DataFrame:
In[6]: import pandas as pd
       nrows,
\1
\1np.random.RandomState(42)
       df1, df2, df3,
\1ng.rand(nrows, ncols))
                             for i in range(4))
Для вычисления суммы всех четырех объектов DataFrame при стандартном подходе
библиотеки Pandas можно написать сумму:
In[7]: %timeit df1 + df2 + df3 + df4
10 loops, best of 3: 87.1 ms per loop

\1
In[8]: %timeit pd.eval('df1 + df2 + df3 + df4')
10 loops, best of 3: 42.2 ms per loop

\1

============================================================
СТРАНИЦА 256
============================================================

\1ndas
In[9]: np.allclose(df1 + df2 + df3 + df4,
                   pd.eval('df1 + df2 + df3 + df4'))
Out[9]: True
Поддерживаемые функцией pd.eval() операции.  На момент выпуска версии 0.16
библиотеки Pandas функция pd.eval() поддерживает широкий спектр операций.

\1
In[10]: df1, df2, df3, df4,
\1ng.randint(0, 1000, (100,
3)))
                                   for i in range(5))

\1
In[11]:
\1np.allclose(result1, result2)
Out[11]: True

\1
In[12]:
\1np.allclose(result1, result2)
Out[12]: True

\1
In[13]:
\1np.allclose(result1, result2)
Out[13]: True
Кроме того, она допускает использование литералов and и or в булевых выраже-
ниях:
In[14]:
\1nd (
\1np.allclose(result1, result3)
Out[14]: True

\1там объектов с помощью синтаксиса obj.attr и к индексам посредством синтаксиса
obj[index]:

============================================================
СТРАНИЦА 257
============================================================
Увеличение производительности библиотеки Pandas: eval() и query()  257
In[15]:
\1np.allclose(result1, result2)
Out[15]: True

\1сокоуровневой функцией pd.eval() из библиотеки Pandas. Преимущество метода
eval() заключается в возможности ссылаться на столбцы по имени. Возьмем для
примера следующий маркированный массив:
In[16]:
\1ng.rand(1000, 3),
\1\1
In[17]:
\1np.allclose(result1, result2)
Out[17]: True

\1
In[18]:
\1np.allclose(result1, result3)
Out[18]: True
Обратите внимание, что мы обращаемся с названиями столбцов  в вычисляемом
выражении как с переменными и получаем желаемый результат.

============================================================
СТРАНИЦА 258
============================================================

\1ndas

\1
In[19]: df.head()
Out[19]: | A | B | C
         0  0.375506  0.406939  0.069938
         1  0.069087  0.235615  0.154374
         2  0.677945  0.433839  0.652324
         3  0.264038  0.808055  0.347197
         4  0.589161  0.252418  0.557789

\1
In[20]: df.eval('
\1
\1\1
In[21]: df.eval('
\1
\1n. Взгляните на следующий фрагмент кода:
In[22]:
\1n(1)

\1n_mean

\1n_mean')
        np.allclose(result1, result2)
Out[22]:
\1ndas: eval() и query()  259

\1тивно вычислять значение выражений с использованием двух пространств имен:
пространства имен столбцов и пространства имен объектов Python. Обратите вни-
мание, что этот символ @ поддерживается лишь методом DataFrame.eval(), но не
функцией pandas.eval(), поскольку у функции pandas.eval() есть доступ только
к одному пространству имен (языка Python).

\1
In[23]:
\1np.allclose(result1, result2)
Out[23]: True

\1
In[24]:
\1nd
\1np.allclose(result1, result2)
Out[24]: True

\1
In[25]:
\1n()

\1n) & (df.
\1n)]

\1n and
\1n')
        np.allclose(result1, result2)
Out[25]: True

\1ем используемой памяти намного проще. Как уже упоминалось, все составные

============================================================
СТРАНИЦА 260
============================================================

\1ndas
выражения с применением массивов NumPy или объектов DataFrame библиотеки
Pandas приводят к неявному созданию вр еменных массивов. Например, вот это:
In[26]:
\1n[27]:
\1\1
In[28]: df.values.nbytes
Out[28]: 32000
eval() будет работать быстрее, если вы не используете всю доступную в системе
оперативную память. Основную роль играет отношение размера вр еменных объ-
ектов DataFrame по сравнению с размером L1 или L2 кэша процессора в системе
(в 2016 году он составляет несколько мегабайтов). eval() позволяет избежать по-
тенциально медленного перемещения значений между различными кэшами памяти
в том случае, когда это отношение намного больше 1. Я обнаружил, что на практике
различие в скорости вычислений между традиционными методами и методом eval/
query обычно довольно незначительно. Напротив, традиционный метод работает
быстрее для маленьких массивов! Преимущество метода eval/query заключается
в экономии оперативной памяти и иногда — в более понятном синтаксисе.

\1полнительную информацию можно найти в документации по библиотеке Pandas.

\1в разделе Enhancing Performance («Повы -
шение производительности», http://pandas.pydata.org/pandas-docs/dev/enhancingperf.html).

\1блиотеки Pandas для анализа данных. Чтобы изучить библиотеку Pandas глубже,
я бы рекомендовал обратиться к следующим источникам информации.
 Онлайн-документация библиотеки Pandas ( http://pandas.pydata.org/ ). Это все -
сторонний источник документации по данному пакету. Хотя примеры в до -
кументации обычно представляют собой небольшие сгенерированные наборы

============================================================
СТРАНИЦА 261
============================================================
Дополнительные источники информации  261
данных, параметры описываются во всей полноте, что обычно очень удобно для
понимания того, как использовать различные функции.
 Python for Data Analysis 1  ( http://bit.ly/python-for-data-analysis). В книге, написанной

\1менными рядами, обеспечившие его как финансового аналитика средствами
к существованию. В книге также приведено множество интересных примеров
применения Python для получения полезной информации из реальных наборов
данных. Не забывайте, что этой книге уже несколько лет и с того времени в паке-
те Pandas появилось немало новых возможностей, не охваченных ею (впрочем,
в 2017 году 2  ожидается новое издание).
 Обсуждение библиотеки Pandas на форуме Stack Overflow. У библиотеки Pandas
столько пользователей, что любой вопрос, который только мож ет у вас возник-
нуть, вероятно, уже был задан на форуме Stack Overflow и на него получен от-
вет. При использовании библиотеки Pandas вашими лучшими друзьями станут
поисковые системы. Просто введите свой вопрос, сформулируйте проблему
или ошибку, с которой вы столкнулись, — более чем вероятно, что вы найдете
решение на одной из страниц сайта Stack Overflow.
 Библиотека Pandas на сайте PyVideo ( http://pyvideo.org/tag/pandas/). Многие фору -
мы, начиная от PyCon, SciPy и до PyData, выпускали руководства от разработ -
чиков и опытных пользователей библиотеки Pandas. Презентаторы, создающие
руководства PyCon, наиболее опытные и профессиональные.

\1блиотеки Pandas с любой задачей по анализу данных, какая вам только встретится!

\1n и анализ данных. — М.: ДМК-Пресс, 2015. — 482 с.

\1n, предназначенный
для реализации возможности интерактивного построения с помощью утилиты
gnuplot графиков в стиле MATLAB из командной строки IPython. Создатель обо-
лочки IPython Фернандо Перес в этот момент был занят завершением написания
диссертации, он сообщил Джону, что в ближайшие несколько месяцев у него не
будет времени на анализ патча. Хантер принял это как благословение на самосто-
ятельную разработку — так родился пакет Matplotlib, версия 0.1 которого была
выпущена в 2003 году. Институт исследований космоса с помощью космического
телескопа (Space Telescope Science Institute, занимающийся управлением теле -
скопом «Хаббл») финансово поддержал разработку пакета Matplotlib и обеспечил
расширение его возможностей, избрав в качестве пакета для формирования гра -
фических изображений.

\1кросс-платформенный
подход типа «все для всех», который привел к росту по льзователей, что, в свою
очередь, стало причиной появления большого числа активных разработчиков, уве-
личения возможностей инструментов пакета Matplotlib и его распространенности
в мире научных вычислений на языке Python.

\1надежного кросс-платформенного графического ме -
ханизма. Свежие версии Matplotlib упрощают настройку новых глобальных стилей
вывода графики (см. раздел «Пользовательские настройки Matplotlib: конфигурации
и таблицы стилей» данной главы). Разрабатываются новые пакеты, предназначенные
для работы с ней через более современные и «чистые» API, например Seaborn (см. раз -
дел «Визуализация с помощью библиотеки Seaborn» этой главы), ggplot ( http://yhat.
github.io/ggplot), HoloViews ( http://holoviews.org/), Altair ( http://altair-viz.github.io/) и даже
саму библиотеку Pandas можно использовать в качеств е адаптеров для API Matplotlib.

\1лей постепенно отходит от непосредственного использования API Matplotlib.
Общие советы по библиотеке Matplotlib
Прежде чем погрузиться в подробности создания визуализаций с помощью
Matplotlib, расскажу несколько полезных вещей про этот пакет.
Импорт matplotlib
Аналогично тому, как мы использовали сокращение np для библиотеки NumPy
и сокращение pd для библиотеки Pandas, мы будем применять стандартные сокра -
щения для импортов библиотеки Matplotlib:
In[1]: import matplotlib as mpl
       import matplotlib.pyplot as plt

\1
In[2]: plt.style.use('classic')

\1ки Matplotlib. В более ранних версиях доступен только стиль по умолчанию.
Дальнейшую  информацию о таблицах стилей см. в разделе «Пользовательские
настройки Matplotlib: конфигурации и таблицы стилей» этой главы.

============================================================
СТРАНИЦА 264
============================================================

\1n;
 в блокноте IPython.

\1
# ------- файл: myplot.py ------
import matplotlib.pyplot as plt
import numpy as np

\1np.linspace(0, 10, 100)
plt.plot(x, np.sin(x))
plt.plot(x, np.cos(x))
plt.show()

\1
$ python myplot.py

\1ной системы и конкретной версии, но библиотека Matplotlib делает все возможное,
чтобы скрыть от вас эти детали.
Одно важное замечание: команду plt.show() следует использовать только один раз
за сеанс работы с Python, и чаще всего ее можно увидеть в самом конце сценария.

\1ведению в зависимости от прикладной части, так что лучше избегать этого.

============================================================
СТРАНИЦА 265
============================================================
Общие советы по библиотеке Matplotlib  265
Построение графиков из командной оболочки IPython
Очень удобно использовать Matplotlib интерактивно из командной оболочки
IPython (см. главу 1). Оболочка IPython будет отлично работать с библиотекой
Matplotlib, если перевести ее в режим Matplotlib. Для активизации этого режима
после запуска IPython можно воспользоваться «магической» командой %matplotlib:
In [1]: %matplotlib
Using matplotlib backend: TkAgg
In [2]: import matplotlib.pyplot as plt

\1полнять команду plt.show() в режиме Matplotlib не обязательно.
Построение графиков из блокнота IPython
Блокнот IPython — браузерный интерактивный инструмент для анализа данных,
допускающий совмещение комментариев, кода, графики, элементов HTML и мно -
гого другого в единый исполняемый документ (см. главу 1).
Интерактивное построение графиков в блокноте IPython возможно с помощью
команды %matplotlib, работает аналогично командной оболочке IPython. В блок-
ноте IPython у вас появляется возможность включения графики непосредственно
в блокнот с двумя возможными альтернативами:
 использование команды %matplotlib notebook приведет к включению в блокнот
интерактивных графиков;
 выполнение команды %matplotlib  inline приведет к включению в блокнот
статических изображений графиков.
В книге мы будем использовать команду %matplotlib inline:
In[3]: %matplotlib inline
После выполнения этой команды (которое нужно произвести только один раз за
сеанс/для одного ядра Python) все создающие графики блоки в блокноте будут
включать PNG-изображения итогового графика (рис. 4.1):
In[4]: import numpy as np

\1np.linspace(0, 10, 100)

\1np.sin(x), '-')
       plt.plot(x, np.cos(x), '--');

============================================================
СТРАНИЦА 266
============================================================

\1n[5]: fig.savefig('my_figure.png')
В текущем рабочем каталоге появился файл с названием my_figure.png:
In[6]: !ls -lh my_figure.png
-rw-r--r--  1 jakevdp  staff | 16K Aug 1110:59 my_figure.png
Чтобы убедиться, что содержимое этого файла соответствует нашим ожиданиям,
воспользуемся объектом Image оболочки IPython для отображения его содержи -
мого (рис. 4.2):
In[7]: from IPython.display import Image
       Image('my_figure.png')
Рис. 4.2. Простой график в виде
\1nvas рисунка:
In[8]: fig.canvas.get_supported_filetypes()
Out[8]: {'eps': 'Encapsulated Postscript',
         'jpeg': 'Joint Photographic Experts Group',
         'jpg': 'Joint Photographic Experts Group',
         'pdf': 'Portable Document Format',
         'pgf': 'PGF code for LaTeX',
         'png': 'Portable Network Graphics',
         'ps': 'Postscript',
         'raw': 'Raw RGBA bitmap',
         'rgba': 'Raw RGBA bitmap',
         'svg': 'Scalable Vector Graphics',
         'svgz': 'Scalable Vector Graphics',
         'tif': 'Tagged Image File Format',
         'tiff': 'Tagged Image File Format'}

\1свойство, которое потенциально может
привести к путанице. Рассмотрим вкратце различия между ними.
Интерфейс в стиле MATLAB
Библиотека Matplotlib изначально была написана как альтернативный вариант
(на языке Python) для пользователей пакета MATLAB, и значительная часть ее
синтаксиса отражает этот факт. MATLAB-подобные инструменты содержатся
в интерфейсе pyplot ( plt). Например, следующий код, вероятно, выглядит до -
вольно знакомо пользователям MATLAB (рис. 4.3):
In[9]: plt.figure()  # Создаем рисунок для графика
       # Создаем первую из двух областей графика и задаем текущую ось
       plt.subplot(2, 1, 1) # (rows, columns, panel number)
       plt.plot(x, np.sin(x))

============================================================
СТРАНИЦА 268
============================================================

\1np.cos(x));
Важно отметить, что этот интерфейс сохраняет состояние: он отслеживает текущий
рисунок и его оси координат и для него выполняет все команды plt. Получить
на них ссылки можно с помощью команд plt.gcf() (от англ. get current figure —
«получить текущий рисунок») и plt.gca() (от англ. get current axes — «получить
текущие оси координат»).

\1
In[10]: # Сначала создаем сетку графиков
        # ax будет массивом из двух объектов Axes
        fig,
\1np.sin(x))
        ax[1].plot(x, np.cos(x));

============================================================

\1
In[1]: %matplotlib inline
       import matplotlib.pyplot as plt
       plt.style.use('seaborn-whitegrid')
       import numpy as np

\1
In[2]:
\1\1n[3]:
\1np.linspace(0, 10, 1000)
       ax.plot(x, np.sin(x));

\1
In[4]: plt.plot(x, np.sin(x));

============================================================

\1
In[5]: plt.plot(x, np.sin(x))
       plt.plot(x, np.cos(x));
Рис. 4.8. Рисуем несколько линий
Вот и все, что касается построения графиков простых  функций в библиотеке

\1\1n[6]:
plt.plot(x, np.sin(x - 0),
\1np.sin(x - 1),
\1np.sin(x - 2),
\1np.sin(x - 3),
\1np.sin(x - 4),
\1np.sin(x - 5),
\1nestyle
(рис. 4.10):
Рис. 4.10. Примеры различных стилей линий

============================================================
СТРАНИЦА 273
============================================================
Простые линейные графики  273
In[7]: plt.plot(x, x + 0,
\1
\1
\1
\1
\1
\1
\1
\1nestyle и color в одном неключевом аргументе функции plt.plot (рис. 4.11):
In[8]: plt.plot(x, x + 0, '-g')  # сплошная линия зеленого цвета
       plt.plot(x, x + 1, '--c') # штриховая линия голубого цвета
       plt.plot(x, x + 2, '-.k') # штрихпунктирная линия черного цвета
       plt.plot(x, x + 3, ':r'); # пунктирная линия красного цвета
Рис. 4.11. Сокращенный синтаксис для управления цветами и стилями
Эти односимвольные коды цветов отражают стандартные сокращения, принятые
в широко используемых для цифровой цветной графики цветовых моделях RGB
(Red/Green/Blue — «красный/зеленый/синий») и CMYK (Cyan/Magenta/Yellow/
blacK — «голубой/пурпурный/желтый/черный»).

\1треть docstring функции plt.plot() с помощью справочных инструментов оболочки
IPython (см. раздел «Справка и документация в оболочке Python» главы 1).

\1нат по умолчанию, но иногда требуется более точная настройка. Простейший

============================================================
СТРАНИЦА 274
============================================================

\1n[9]: plt.plot(x, np.sin(x))
       plt.xlim(-1, 11)
       plt.ylim(-1.5, 1.5);

\1
In[10]: plt.plot(x, np.sin(x))
        plt.xlim(10, 0)
        plt.ylim(1.2, -1.2);

\1plt.axis() (не перепутайте метод plt.axis()
с методом plt.axes()!). Метод plt.axis() предоставляет возможность задавать

============================================================
СТРАНИЦА 275
============================================================
Простые линейные графики  275
пределы осей X  и Y с помощью одного вызова путем передачи списка, в котором
указываются [xmin, xmax, ymin, ymax] (рис. 4.14):
In[11]: plt.plot(x, np.sin(x))
        plt.axis([-1, 11, -1.5, 1.5]);

\1
In[12]: plt.plot(x, np.sin(x))
        plt.axis('tight');

\1
In[13]: plt.plot(x, np.sin(x))
        plt.axis('equal');

============================================================
СТРАНИЦА 276
============================================================

\1n[14]: plt.plot(x, np.sin(x))
        plt.title("A Sine Curve") # Синусоидальная кривая
        plt.xlabel("x")
        plt.ylabel("sin(x)");

\1тации библиотеки Matplotlib и в разделе docstring для каждой из функций.

\1блиотеке Matplotlib для быстрого создания такой легенды имеется встроенный
метод plt.legend(). Хотя существует несколько возможных способов, проще всего,
как мне кажется, задать метку каждой линии с помощью ключевого слова label
функции plot (рис. 4.18):
In[15]: plt.plot(x, np.sin(x), '-g',
\1n(x)')
        plt.plot(x, np.cos(x), ':b',
\1nd();
Рис. 4.18. Пример легенды графика
Функция plt.legend() отслеживает стиль и цвет линии и устанавливает их соот-
ветствие с нужной меткой. Больше информации по заданию и форматированию
легенд графиков можно найти в docstring метода plt.legend(). Кроме того, мы
рассмотрим некоторые продвинутые параметры задания легенд в разделе «Поль-
зовательские настройки легенд на графиках» этой главы.

\1фейса ax носят такое же название (например, plt.plot() → ax.plot() , plt.
legend() → ax.legend() и т. д.), это касается не всех команд. В частности, функции
для задания пределов, меток и названий графиков называются несколько иначе. Вот
список соответствий между MATLAB-подобным функциями и объектно-ориенти-
рованным методами:

============================================================
СТРАНИЦА 278
============================================================

\1n[16]:
\1np.sin(x))
        ax.set(
\1n(x)',

\1\1
In[1]: %matplotlib inline
       import matplotlib.pyplot as plt
       plt.style.use('seaborn-whitegrid')
       import numpy as
\1\1
In[2]:
\1np.linspace(0, 10, 30)

\1np.sin(x)
       plt.plot(x, y, 'o',
\1\1
In[3]:
\1np.random.RandomState(0)
       for marker in ['o', '.', ',', 'x', '+', 'v', '^', '<', '>', 's', 'd']:
           plt.plot(rng.rand(5), rng.rand(5), marker,

\1nd(
\1\1
In[4]: plt.plot(x, y, '-ok'); | # линия (-), маркер круга (o), черный цвет (k)

\1
In[5]: plt.plot(x, y, '-p',
\1
\1\1n[6]: plt.scatter(x, y,
\1\1
In[7]:
\1np.random.RandomState(0)

\1ng.randn(100)

\1ng.randn(100)

\1ng.rand(100)

\1ng.rand(100)
       plt.scatter(x, y,
\1\1n, каждая вы -
борка представляет собой один из трех типов цветов с тщательно измеренными
лепестками и чашелистиками (рис. 4.26):
In[8]: from sklearn.datasets import load_iris

\1names[0])
       plt.ylabel(iris.feature_names[1]);

\1
In[1]: %matplotlib inline
       import matplotlib.pyplot as plt
       plt.style.use('seaborn-whitegrid')

============================================================
СТРАНИЦА 284
============================================================

\1numpy as np
In[2]:
\1np.linspace(0, 10, 50)

\1np.sin(x) + dy * np.random.randn(50)
       plt.errorbar(x, y,
\1\1
In[3]: plt.errorbar(x, y,
\1
\1ng функции plt.errorbar.

\1лита для решения данной задачи, не составит особого труда скомбинировать такие
примитивы, как plt.plot и plt.fill_between, для получения искомого результата.

\1Learn (см. подробности в разделе «Зна-
комство с библиотекой Scikit-Learn» главы 5) простую регрессию на основе Гауссова
процесса (Gaussian process regression, GPR). Она представляет собой метод подбора
по имеющимся данным очень гибкой непараметрической функции с непрерывной
мерой неопределенности измерения. Мы не будем углубляться в детали регрессии
на основе Гауссова процесса, а сконцентрируемся на визуализации подобной не-
прерывной погрешности измерения:
In[4]: from sklearn.gaussian_process import GaussianProcess
       # Описываем модель и отрисовываем некоторые данные

\1np.sin(x)

\1np.array([1, 3, 5, 6, 8])

\1nProcess1(
\1
\1np.newaxis], ydata)

\1np.linspace(0, 10, 1000)
       yfit,
\1np.newaxis],
\1np.sqrt(MSE)  # 2*сигма ~ область с уровнем доверия 95%

\1ции plt.errorbar, но рисовать 1000 точек с помощью 1000 планок погрешностей не
хотелось бы. Вместо этого можно воспользоваться функцией plt.fill_between и ви-
зуализировать эту непрерывную погрешность с помощью светлого цвета (рис. 4.29):
In[5]: # Визуализируем результат
       plt.plot(xdata, ydata, 'or')
       plt.plot(xfit, yfit, '-',
\1\1n. Вместо него рекомендуется использовать класс Gaussian Process Regressor.

============================================================
СТРАНИЦА 286
============================================================

\1n(xfit, yfit - dyfit, yfit + dyfit,

\1n параметры: мы
передали в нее значение x, затем нижнюю границу по y, затем верхнюю границу
по y, в результате получили заполненную область между ними.

\1ничена, что и отражается в малых ошибках модели. В удаленных же от измеренной
точки данных областях модель жестко не ограничивается и ошибки модели растут.
Чтобы узнать больше о возможностях функции plt.fill_between() (и родствен -
ной ей функции plt.fill() ), см. ее docstring или документацию библиотеки
Matplotlib.

\1мощью библиотеки Seaborn» данной главы, в котором мы обсудим пакет Seaborn
с продвинутым API для визуализации подобных непрерывных погрешностей.

\1
 plt.contour — для контурных графиков;
 plt.contourf — для контурных графиков с заполнением;
 plt.imshow — для отображения изображений. В этом разделе мы рассмотрим
несколько примеров использования данных функций. Начнем с настройки
блокнота для построения графиков и нужных нам импортов:

============================================================
СТРАНИЦА 287
============================================================
Графики плотности и контурные графики  287
In[1]: %matplotlib inline
       import matplotlib.pyplot as plt
       plt.style.use('seaborn-white')
       import numpy as np

\1
In[2]: def f(x, y):
           return np.sin(x) ** 10 + np.cos(10 +  * x) * np.cos(x)
Создать контурный график можно с помощью функции plt.contour. У нее имеется
три аргумента: координатная сетка значений x , координатная сетка значений y и ко-
ординатная сетка значений z . Значения x  и y представлены точками на графике,
а значения z  будут представлены контурами уровней. Вероятно, наиболее простой
способ подготовить такие данные — воспользоваться функцией np.meshgrid, фор -
мирующей двумерные координатные сетки из одномерных массивов:
In[3]:
\1np.linspace(0, 5, 50)

\1np.linspace(0, 5, 40)
       X,
\1np.meshgrid(x, y)

\1\1
In[4]: plt.contour(X, Y, Z,
\1\1

============================================================
СТРАНИЦА 288
============================================================

\1n[5]: plt.contour(X, Y, Z, 20,
\1n путем TAB-автодополнения названия модуля plt.cm:
plt.cm.<
\1ntourf()
(обратите внимание на букву f в конце ее названия), синтаксис которой не отли-
чается от синтаксиса функции plt.contour().

\1
In[6]: plt.contourf(X, Y, Z, 20,
\1\1
In[7]: plt.imshow(Z,
\1
\1nt [xmin, xmax, ymin, ymax].
 По умолчанию функция plt.imshow()  следует стандартному определению
массива для изображения, в котором начало координат находится в верхнем
левом, а не в нижнем левом углу, как на большинстве контурных графиков. Это
поведение можно изменить в случае отображения данных с привязкой к сетке.
 Функция plt.imshow() автоматически настраивает соотношение сторон гра -
фика в соответствии с входными данными. Это поведение можно из менить,
задав, например, plt.axis(
\1\1n[8]:
\1ntour(X, Y, Z, 3,
\1ntours,
\1
\1
\1
\1ntour, plt.contourf и plt.imshow — предо-
ставляет практически неограниченные возможности по отображению подобных
трехмерных данных на двумерных графиках. Дальнейшую информацию относи-
тельно имеющихся у этих функций параметров вы можете найти в их docstring.

\1
In[1]: %matplotlib inline
       import numpy as np
       import matplotlib.pyplot as plt
       plt.style.use('seaborn-white')

============================================================
СТРАНИЦА 291
============================================================
Гистограммы, разбиения по интервалам и плотность  291

\1np.random.randn(1000)
In[2]: plt.hist(data);

\1
In[3]: plt.hist(data,
\1
\1none');
Рис. 4.36. Гистограмма с пользовательскими настройками
Docstring функции plt.hist  содержит более подробную информацию о дру -
гих доступных возможностях пользовательской настройки. Сочетание опции

\1n[4]:
\1np.random.normal(0, 0.8, 1000)

\1np.random.normal(-2, 1, 1000)

\1np.random.normal(3, 2, 1000)

============================================================
СТРАНИЦА 292
============================================================

\1
\1
\1np.histogram():
In[5]: counts,
\1np.histogram(data,
\1nt(counts)
[ 12 190 468 301  29]

\1
In[6]:
\1np.random.multivariate_normal(mean, cov, 10000).T

\1
In[12]: plt.hist2d(x, y,
\1nts in bin') # Количествво в интервале

============================================================
СТРАНИЦА 293
============================================================
Гистограммы, разбиения по интервалам и плотность  293
Рис. 4.38. Двумерная гистограмма, построенная с помощью функции plt.hist2d
У функции plt.hist2d, как и у функции plt.hist, имеется немало дополнительных
параметров для тонкой настройки графика и разбиения по интервалам, подробно
описанных в ее docstring. Аналогично тому, как у функции plt.hist есть эквива-
лент np.histogram, так и у функции plt.hist2d имеется эквивалент np.histogram2d,
который используется следующим образом:
In[8]: counts, xedges,
\1np.histogram2d(x, y,
\1np.histogramdd.
Функция plt.hexbin: гексагональное разбиение по интервалам

\1правильный шестиугольник. Для этих целей  библиотека Matplotlib
предоставляет функцию plt.hexbin  — двумерный набор данных, разбитых по
интервалам на сетке из шестиугольников (рис. 4.39):
In[9]: plt.hexbin(x, y,
\1nt in bin') # Количество в интервале
Рис. 4.39. Создание двумерной гистограммы с помощью функции plt.
\1\1n имеется множество интересных параметров, включая воз-
можность задавать вес для каждой точки и менять выводимое значение для каждого
интервала на любой сводный показатель библиотеки NumPy (среднее значение
весов, стандартное отклонение весов и т. д.).

\1ядерная оценка плотности распределения (kernel density estimation, KDE).

\1
In[10]: from scipy.stats import gaussian_kde
        # Выполняем подбор на массиве размера [Ndim, Nsamples]

\1np.vstack([x, y])

\1n_kde(data)
        # Вычисляем на регулярной координатной сетке

\1np.linspace(-3.5, 3.5, 40)

\1np.linspace(-6, 6, 40)
        Xgrid,
\1np.meshgrid(xgrid, ygrid)

\1np.vstack([Xgrid.ravel(), Ygrid.ravel()]))
        # Выводим график результата в виде изображения
        plt.imshow(Z.reshape(Xgrid.shape),

\1
\1nsity") # Плотность
Рис. 4.40. Ядерная оценка плотности распределения

============================================================
СТРАНИЦА 295
============================================================
Пользовательские настройки легенд на графиках  295
Длина сглаживания метода KDE позволяет эффективно в ыбирать компромисс между
гладкостью и детализацией (один из примеров вездесущих компромиссов между
смещением и дисперсией). Существует обширная литература, посвященная выбору
подходящей длины сглаживания: в функции gaussian_kde используется эмпириче-
ское правило для поиска квазиоптимальной длины сглаживания для входных данных.
В экосистеме SciPy имеются и другие реализации метода KDE, каждая со своими
сильными и слабыми сторонами, например методы sklearn.neighbors.KernelDensity
и statsmodels.nonparametric.kernel_density.KDEMultivariate . Использование
библиотеки Matplotlib для основанных на методе KDE визуализаций требует на-
писания излишнего кода. Библиотека Seaborn, которую мы будем обсуждать в раз -
деле «Визуализация с помощью библиотеки Seaborn» данной главы, предлагает для
создания таких визуализаций API с намного более сжатым синтаксисом.

\1ментов графика. Мы ранее уже рассматривали создание простой легенды, здесь
продемонстрируем возможности пользовательской настройки расположения
и внешнего вида легенд в Matplotlib.
С помощью команды plt.legend()  можно автоматически создать простейшую
легенду для любых маркированных элементов графика (рис. 4.41):
In[1]: import matplotlib.pyplot as plt
       plt.style.use('classic')
In[2]: %matplotlib inline
       import numpy as np
In[3]:
\1np.linspace(0, 10, 1000)
       fig,
\1np.sin(x), '-b',
\1ne') | # Синус
       ax.plot(x, np.cos(x), '--r',
\1ne') # Косинус
       ax.axis('equal')

\1nd();
Рис. 4.41. Легенда графика по умолчанию

============================================================
СТРАНИЦА 296
============================================================

\1n[4]: ax.legend(
\1
\1ncol, чтобы задать количество столбцов
в легенде (рис. 4.43):
In[5]: ax.legend(
\1nter',
\1ncybox)
или добавить тень, поменять прозрачность (альфа-фактор) рамки или поля около
текста (рис. 4.44):
In[6]: ax.legend(
\1ng функции plt.legend.

\1либо из них функции plt.legend() вместе с задаваемы-
ми метками (рис. 4.45):
In[7]:
\1np.sin(x[:, np.newaxis] + np.pi * np.arange(0, 2, 0.5))

\1nes представляет собой список экземпляров класса plt.Line2D
       plt.legend(lines[:2], ['first', 'second']); # Первый, второй
Рис. 4.45. Пользовательские настройки элементов легенды

============================================================
СТРАНИЦА 298
============================================================

\1n[8]: plt.plot(x, y[:, 0],
\1nd')
       plt.plot(x, y[:, 2:])
       plt.legend(
\1
\1\1
In[9]: import pandas as pd

\1nia_cities.csv')
       # Извлекаем интересующие нас данные
       lat,
\1ngd']
       population,
\1n_total'], cities['area_total_km2']
       # Распределяем точки по нужным местам,
       # с использованием размера и цвета, но без меток
       plt.scatter(lon, lat,
\1ne,
                   c=np.log10(population),
\1
\1ngitude')
       plt.ylabel('latitude')
       plt.colorbar(
\1n)')
       plt.clim(3, 7)
       # Создаем легенду:
       # выводим на график пустые списки с нужным размером и меткой
       for area in [100, 300, 500]:
           plt.scatter([], [],
\1nd(
\1
\1
\1nia Cities: Area and Population');
       # Города Калифорнии: местоположение и население

\1дополнительный набор утилит
Basemap для библиотеки Matplotlib, который мы рассмотрим в разделе «Отобра-
жение географических данных с помощью Basemap» данной главы.

============================================================
СТРАНИЦА 300
============================================================

\1nd, можно
создавать только одну легенду для всего графика. Если попытаться создать вторую
легенду с помощью функций plt.legend() и ax.legend(), она просто перекроет
первую. Решить эту проблему можно, создав изначально для легенды новый ри -
сователь (artist), после чего добавить вручную второй рисователь на график с по-
мощью низкоуровневого метода ax.add_artist() (рис. 4.48):
In[10]: fig,
\1
\1np.linspace(0, 10, 1000)
       for i in range(4):
           lines += ax.plot(x, np.sin(x - i * np.pi / 2),
                            styles[i],
\1nd(lines[:2], ['line A', 'line B'], | # Линия А, линия B

\1
\1nd import Legend

\1nd(ax, lines[2:], ['line C', 'line D'], # Линия С, линия D

\1
\1nd() (напомню, что сделать это можно в блокноте оболочки IPython с по-
мощью команды legend??), то увидите, что эта функция состоит просто из логики
создания подходящего рисователя Legend, сохраняемого затем в атрибуте legend_
и добавляемого к рисунку при отрисовке графика.

\1отдельная система координат, предоставляющая ключ к значению
цветов на графике. Поскольку эта книга напечатана в черно-белом исполнении,
для данного раздела имеется дополнительное онлайн-приложение, в котором вы
можете посмотреть на оригинальные графики в цвете ( https://github.com/jakevdp/
PythonDataScienceHandbook). Начнем с настройки блокнота для построения графиков
и импорта нужных функций:
In[1]: import matplotlib.pyplot as plt
       plt.style.use('classic')
In[2]: %matplotlib inline
       import numpy as np

\1
In[3]:
\1np.linspace(0, 10, 1000)

\1np.sin(x) * np.cos(x[:, np.newaxis])
       plt.imshow(I)
       plt.colorbar();

\1тов и эффективному их использованию в разных ситуациях.

============================================================
СТРАНИЦА 302
============================================================

\1n[4]: plt.imshow(I,
\1n:
plt.cm.<
\1n Simple
Rules for Better Figures («Десять простых правил для улучшения рисунков», http://
bit.ly/2fDJn9J). Онлайн-документация библиотеки Matplotlib также содержит ин -
тересную информацию по вопросу выбора карты цветов ( http://matplotlib.org/1.4.1/
users/colormaps.html).

\1
 последовательные карты цветов. Состоят из одной непрерывной последователь-
ности цветов (например, binary или viridis);
 дивергентные карты цветов. Обычно содержат два хорошо различимых цвета,
отражающих положительные и отрицательные отклонения от среднего значе -
ния (например, RdBu или PuOr);
 качественные карты цветов. В них цвета смешиваются без какого-либо четкого
порядка (например, rainbow или jet).

============================================================

\1
In[5]:
from matplotlib.colors import LinearSegmentedColormap
def grayscale_cmap(cmap):
    """Возвращает версию в оттенках серого заданной карты цветов"""

\1np.arange(cmap.N))
    # Преобразуем RGBA в воспринимаемую глазом светимость серого цвета
    # ср. http://alienryderflex.com/hsp.html

\1
\1np.sqrt(np.dot(colors[:, :3] ** 2, RGB_weight))
    colors[:, :3] = luminance[:, np.newaxis]
    return LinearSegmentedColormap.from_list(cmap.name + "_gray",
    colors, cmap.N)
def view_colormap(cmap):
    """Рисует карту цветов в эквивалентных оттенках серого"""

\1np.arange(cmap.N))

\1np.arange(cmap.N))
    fig,
\1
\1
\1n[6]: view_colormap('jet')
Рис. 4.51. Карта цветов jet и ее неравномерная шкала светимости
Отметим яркие полосы в ахроматическом изображении. Даже в полном цвете
эта неравномерная яркость означает, что определенные части диапазона цветов
будут притягивать внимание, что потенциально привед ет к акцентированию

============================================================
СТРАНИЦА 304
============================================================

\1n[7]: view_colormap('viridis')

\1
In[8]: view_colormap('cubehelix')

\1синий»). Однако,
как вы можете видеть на рис. 4.54, такая информация будет потеряна при переходе
к оттенкам серого!
In[9]: view_colormap('RdBu')

\1синяя) карта цветов и ее светимость
Далее мы увидим примеры использования некоторых из этих карт цветов.
В библиотеке Matplotlib существует множество карт цветов, для просмотра их
списка вы можете воспользоваться оболочкой IPython для просмотра содержимого

============================================================

\1ке Python можно найти в инструментах и документации по библиотеке Seaborn
(см. раздел «Визуализация с помощью библиотеки Seaborn» этой главы).

\1просто экземпляры клас-
са plt.Axes, поэтому для них можно использовать все уже изученные нами трюки,
связанные с форматированием осей координат и делений на них. Шкалы цветов об-
ладают достаточной гибкостью: например, можно сузить границы диапазона цветов,
обозначив выходящие за пределы этого диапазона значения с помощью тре угольных
стрелок вверху и внизу путем задания значения свойства extend. Это может оказать-
ся удобно, например, при выводе зашумленного изображения (рис. 4.55):
In[10]: # создаем шум размером 1% от пикселов изображения

\1np.random.random(I.shape) < 0.01)
        I[speckles] = np.random.normal(0, 3, np.count_nonzero(speckles))
        plt.figure(
\1
\1\1n[11]: plt.imshow(I,
\1n и состоят почти из 2000 миниа-
тюр размером 8 × 8 с рукописными цифрами.

\1
In[12]: # Загружаем изображения цифр от 0 до 5 и визуализируем некоторые из них
        from sklearn.datasets import load_digits

\1
\1n enumerate(ax.flat):
            axi.imshow(digits.images[i],
\1nary')
            axi.set(
\1nsionality reduction), например обучением на базе многообразий  (manifold
learning) с целью снижения размерности данных с сохранением интересующих нас
зависимостей. Понижение размерности — пример машинного обучения без учителя
(unsupervised machine learning). Мы обсудим его подробнее в разделе «Что такое
машинное обучение» главы 5.

\1
In[13]: # Отображаем цифры на двумерное пространство с помощью функции IsoMap
        from sklearn.manifold import Isomap

\1
\1
\1nsform(digits.data)

\1

============================================================
СТРАНИЦА 308
============================================================

\1n[14]: # Выводим результаты на график
        plt.scatter(projection[:, 0], projection[:, 1],
\1nge(6),
\1\1
In[1]: %matplotlib inline
       import matplotlib.pyplot as plt
       plt.style.use('seaborn-white')
       import numpy as
\1\1
In[2]:
\1\1
In[3]:
\1np.linspace(0, 10)
       ax1.plot(np.sin(x))
       ax2.plot(np.cos(x));

============================================================
СТРАНИЦА 310
============================================================

\1n[4]: for i in range(1, 7):
           plt.subplot(2, 3, i)
           plt.text(0.5, 0.5, str((2, 3, i)),

\1nter')

\1
In[5]:
\1n range(1, 7):

\1
\1nter')

\1

============================================================
СТРАНИЦА 312
============================================================

\1n[6]: fig,
\1\1
In[7]: # Системы координат располагаются в двумерном массиве,
       # индексируемом по [строка, столбец]
       for i in range(2):
           for j in range(3):
               ax[i, j].text(0.5, 0.5, str((i, j)),

\1nter')
       fig
Рис. 4.64. Нумерация графиков в сетке субграфиков
По сравнению с plt.subplot() функция plt.subplots() намного лучше согласуется
с принятой в языке Python индексацией, начинающейся с 0.

============================================================

\1
In[8]:
\1n (рис. 4.65):
In[9]: plt.subplot(grid[0, 0])
       plt.subplot(grid[0, 1:])
       plt.subplot(grid[1, :2])
       plt.subplot(grid[1, 2]);

\1
In[10]: # Создаем нормально распределенные данные

\1np.random.multivariate_normal(mean, cov, 3000).T
        # Задаем системы координат с помощью функции GridSpec

\1
\1n_ax)

\1n_ax)
        # Распределяем точки по основной системе координат

============================================================
СТРАНИЦА 314
============================================================

\1n_ax.plot(x, y, 'ok',
\1
\1nvert_yaxis()
        y_hist.hist(y, 40,
\1
\1ntal',
\1nvert_xaxis()

\1пространено, что в пакете Seaborn для построения ег о графиков предусмотрено
отдельное API. Подробную информацию см. в разделе «Визуализация с помощью
библиотеки Seaborn» данной главы.

\1
In[1]: %matplotlib inline
       import matplotlib.pyplot as
\1n-whitegrid')
       import numpy as np
       import pandas as pd

\1дений детей в зависимости от дня календарного года. Эти данные можно скачать
по адресу https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/births.csv.

\1
In[2]:

\1np.percentile(births['births'], [25, 50, 75])
mu,
\1nt)
births.
\1nth +
                              births.day,
\1ndex.month, births.index.day])
births_by_date.
\1nth, day)
                        for (month, day) in births_by_date.index]
In[3]: fig,
\1\1n[4]: fig,
\1ndependence Day",
\1nter', **style)
       ax.text('2012-9-4', 4850, "Labor Day",
\1nter', **style)
       ax.text('2012-10-31', 4600, "Halloween",
\1nksgiving",
\1nter', **style)
       ax.text('2012-12-25', 3850, "Christmas ",
\1nthLocator())
       ax.xaxis.set_minor_locator(mpl.dates.MonthLocator(
\1nor_formatter(mpl.dates.DateFormatter('%h'));
Рис. 4.68. Ежедневное количество рождаемых детей в зависимости от даты, с комментариями
Метод ax.text принимает на входе координату x , координату y, строковое значение
и необязательные ключевые слова, задающие цвет, размер, стиль, выравнивание
и другие свойства текста. В данном случае мы использовали значения
\1nter', где ha — сокращение от horizontal alignment («выравнивание по го-
ризонтали»). См. дальнейшую информацию об имеющихся настройках в docstring
функций plt.text() и mpl.text.Text().

============================================================
СТРАНИЦА 317
============================================================
Текст и поясняющие надписи  317
Преобразования и координаты текста
В предыдущем примере мы привязали наши текстовые пояснения к конкретным
значениям данных. Иногда бывает удобнее привязать текст к координатам на осях
рисунка, независимо от данных. В библиотеке Matplot lib это осуществляется путем
модификации преобразования (transform).

\1lib.transforms ).

\1
 ax.transData — преобразование из системы координат данных;
 ax.transAxes — преобразование из системы координат объекта Axes (в единицах
размеров рисунка);
 fig.transFigure — преобразование из системы координат объекта Figure (в еди -
ницах размеров рисунка)

\1
In[5]: fig,
\1
\1nsData – значение по умолчанию,
       # но мы все равно указываем его
       ax.text(1, 5, ". Data: (1, 5)",
\1nsData)
       ax.text(0.5, 0.1, ". Axes: (0.5, 0.1)",
\1nsAxes)
       ax.text(0.2, 0.2, ". Figure: (0.2, 0.2)",
\1nsFigure);

\1близительно отмечает здесь заданные координаты.
Координаты transData задают обычные координаты данных, соответствующие
меткам на осях X  и Y . Координаты transAxes задают местоположение, считая от
нижнего левого угла системы координат (здесь — белый прямоугольник), в виде

============================================================
СТРАНИЦА 318
============================================================

\1nsFigure схожи с transAxes,
но задают местоположение, считая от нижнего левого угла рисунка (здесь — серый
прямоугольник) в виде доли от размера рисунка.

\1динаты transData, а другие останутся неизменными (рис. 4.70):
In[6]: ax.set_xlim(0, 2)
       ax.set_ylim(-6, 6)
       fig
Рис. 4.70. Сравнение различных систем координат библиотеки Matplotlib
Наблюдать это поведение более наглядно можно путем интерактивного изменения
пределов осей координат. При выполнении кода в блокноте этого можно добиться,
заменив %matplotlib inline на %matplotlib notebook и воспользовавшись меню
каждого из графиков для работы с ним.

============================================================

\1объекты, подверженные изменениям в зависимости от соотношения сторон
графиков, поэтому результат редко оказывается соответствующим ожиданиям.
Вместо этого я предложил бы воспользоваться функцией plt.annotate() . Она
создает текст и стрелку, причем позволяет очень гибко задавать настройки для
стрелки.
В следующем фрагменте кода мы используем функцию annotate с несколькими
параметрами (рис. 4.71):
In[7]: %matplotlib inline
       fig,
\1np.linspace(0, 20, 1000)
       ax.plot(x, np.cos(x))
       ax.axis('equal')
       ax.annotate('local maximum',
\1
\1nnotate('local minimum',
\1np.pi, -1),
\1
\1ngle3,
\1
\1\1n[8]:
fig,
\1nnotate("New Year's Day",
\1nts',

\1
\1nnotate("Independence Day",
\1nd",
\1none",
\1nts',
\1nter',

\1nnotate('Labor Day',
\1nter',

\1nts')
ax.annotate('',
\1nnotate('Halloween',
\1nts',

\1ncy",

\1none",

\1ngle3,
\1
\1nnotate('Thanksgiving',
\1nts',

\1nd4,
\1
\1ngle,
\1
\1nnotate('Christmas',
\1nts',

\1nter",

\1nd",
\1nthLocator())
ax.xaxis.set_minor_locator(mpl.dates.MonthLocator(
\1nor_formatter(mpl.dates.DateFormatter('%h'));
ax.set_ylim(3600, 5400);

\1процесс, занимающий немало времени, если речь идет о создании графики
типографского уровня качества! Наконец, отмечу, что использовать для представ -
ления данных продемонстрированную выше смесь стилей я отнюдь не рекомендую,
она дана в качестве примера возможностей.
Рис. 4.72. Средняя рождаемость по дням с пояснениями
Дальнейшее обсуждение и примеры стилей стрелок и поясняющих надписей
можно найти в галерее библиотеки Matplotlib по адресу http://matplotlib.org/examples/
pylab_examples/annotation_demo2.html.

\1фиков библиотеки Matplotlib. Matplotlib старается делать объектами языка Python
все элементы на графике, например объект figure — ограничивающий снаружи
все элементы графика прямоугольник. Каждый объект библ иотеки Matplotlib
также служит контейнером подобъектов. Например, любой объект figure может

============================================================
СТРАНИЦА 322
============================================================

\1n[1]: %matplotlib inline
       import matplotlib.pyplot as plt
       plt.style.use('seaborn-whitegrid')
       import numpy as np
In[2]:
\1\1
In[3]: print(ax.xaxis.get_major_locator())
       print(ax.xaxis.get_minor_locator())
<matplotlib.ticker.LogLocator object at 0
\1n[4]: print(ax.xaxis.get_major_formatter())
       print(ax.xaxis.get_minor_formatter())
<matplotlib.ticker.LogFormatterMathtext object at 0
\1\1
In[5]:
\1np.random.rand(50))
       ax.yaxis.set_major_locator(plt.NullLocator())
       ax.xaxis.set_major_formatter(plt.NullFormatter())

\1сутствие делений может быть полезно во многих случаях, например, если нужно
отобразить сетку изображений. Например, рассмотрим рис. 4.75, содержащий

============================================================
СТРАНИЦА 324
============================================================

\1n[6]: fig,
\1n
       from sklearn.datasets import fetch_olivetti_faces

\1n range(5):
           for j in range(5):
               ax[i, j].xaxis.set_major_locator(plt.NullLocator())
               ax[i, j].yaxis.set_major_locator(plt.NullLocator())
               ax[i, j].imshow(faces[10 * i + j],
\1ne")

\1
In[7]: fig,
\1\1
In[8]: # Задаем, для всех систем координат, локаторы основных делений осей X и Y
           for axi in ax.flat:
           axi.xaxis.set_major_locator(plt.MaxNLocator(3))
           axi.yaxis.set_major_locator(plt.MaxNLocator(3))
       fig

\1ваться классом plt.MultipleLocator, который мы обсудим в следующем разделе.
Более экзотические форматы делений
Форматирование делений, используемое по умолчанию в библиотеке Matplotlib,
оставляет желать лучшего. В качестве варианта по умолчанию, который бы

============================================================
СТРАНИЦА 326
============================================================

\1n[9]: # Строим графики синуса и косинуса
       fig,
\1np.linspace(0, 3 * np.pi, 1000)
       ax.plot(x, np.sin(x),
\1ne')
       ax.plot(x, np.cos(x),
\1ne')
       # Настраиваем сетку, легенду и задаем пределы осей координат
       ax.grid(True)
       ax.legend(
\1np.pi);

\1
In[10]: ax.xaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))
        ax.xaxis.set_minor_locator(plt.MultipleLocator(np.pi / 4))
        fig

\1чи нет, поэтому мы воспользуемся форматером plt.FuncFormatter, принимающим
на входе пользовательскую функцию, обеспечивающую более точный контроль за
форматом вывода делений (рис. 4.80):
In[11]: def format_func(value, tick_number):
            # Определяем количество кратных пи/2 значений
            # [в требуемом диапазоне]

\1nt(np.round(2 * value / np.pi))
            if
\1n "0"
            elif
\1n r"$\pi/2$"
            elif
\1n r"$\pi$"
            elif N % 2 > 0:
                return r"${0}\pi/2$".format(N)
            else:
                return r"${0}\pi$".format(N // 2)
        ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))
        fig

\1ходимо заключить нужную строку в знаки доллара с двух сторон. Это облегчает
отображение математических символов и формул. В нашем случае "$\pi$" визуа-
лизируется в виде греческой буквы π.
Форматер plt.FuncFormatter  обеспечивает возможность чрезвычайно тонкого
контроля внешнего вида делений графика и очень удобен при подготовке графиков
для презентаций или публикации.

============================================================
СТРАНИЦА 328
============================================================

\1ng или в онлайн-документации
библиотеки Matplotlib. Все форматеры и локаторы доступны в пространстве имен
plt (табл. 4.1, 4.2).
Таблица 4.1. Локаторы
Класс локатора Описание
NullLocator Без делений
FixedLocator Положения делений фиксированы
IndexLocator Локатор для графика индексированной переменной (например,
для
\1nge(len(y)))
LinearLocator Равномерно распределенные деления от min до max
LogLocator Логарифмически распределенные деления от min до max
MultipleLocator Деления и диапазон значений кратны заданному основанию
MaxNLocator Находит удачные местоположения для делений в количестве, не превы-
шающем заданного максимального числа
AutoLocator (По умолчанию.) MaxNLocator с простейшими значениями по умолчанию
AutoMinorLocator Локатор для промежуточных делений
Таблица 4.2. Форматеры
Класс форматера Описание
NullFormatter Деления без меток
IndexFormatter Задает строковые значения для меток на основе списка меток
FixedFormatter Позволяет задавать строковые значения для меток вручную
FuncFormatter Значения меток задаются с помощью пользовательской функции
FormatStrFormatter Для всех значений используется строка формата
ScalarFormatter (По умолчанию.) Форматер для скалярных значений
LogFormatter Форматер по умолчанию для логарифмических систем координат

\1
In[1]: import matplotlib.pyplot as plt
       plt.style.use('classic')
       import numpy as np
       %matplotlib inline
In[2]:
\1np.random.randn(1000)
       plt.hist(x);

\1
In[3]: # Используем серый фон

\1
\1ne in ax.spines.values():
           spine.set_visible(False)
       # Скрываем деления сверху и справа

============================================================
СТРАНИЦА 330
============================================================

\1
\1n ax.get_xticklabels():
           tick.set_color('gray')
       for tick in ax.get_yticklabels():
           tick.set_color('gray')
       # Задаем цвет заливки и границ гистограммы
       ax.hist(x,
\1\1
In[4]:
\1\1

============================================================
СТРАНИЦА 331
============================================================
Пользовательские настройки Matplotlib: конфигурации и таблицы стилей  331
In[5]: from matplotlib import cycler

\1none',

\1
\1
\1
\1nes',
\1\1
In[6]: plt.hist(x);

\1
In[7]: for i in range(4):
           plt.plot(np.random.rand(10))
Рис. 4.84. Линейный график с пользовательскими стилями

============================================================
СТРАНИЦА 332
============================================================

\1n[8]: plt.style.available[:5]
Out[8]: ['fivethirtyeight',
         'seaborn-pastel',
         'seaborn-whitegrid',
         'ggplot',
         'grayscale']

\1
plt.style.use('stylename')

\1
with plt.style.context('stylename'):
    make_a_plot()

\1
In[9]: def hist_and_lines():
           np.random.seed(0)
           fig,
\1np.random.randn(1000))
           for i in range(3):
               ax[1].plot(np.random.rand(10))
           ax[1].legend(['a', 'b', 'c'],
\1\1
In[10]: # Восстанавливаем rcParams
        plt.rcParams.update(IPython_default);

\1
In[11]: hist_and_lines()
Рис. 4.85. Стиль библиотеки Matplotlib по умолчанию
Стиль FiveThirtyEight
Стиль FiveThirtyEight подражает оформлению популярного сайта FiveThirtyEight
( http://fivethirtyeight.com/). Как вы можете видеть на рис. 4.86, он включает жирные
шрифты, толстые линии и прозрачные оси координат.
In[12]: with plt.style.context('fivethirtyeight'):
            hist_and_lines()
Рис. 4.86. Стиль
\1\1n[13]: with plt.style.context('ggplot'):
            hist_and_lines()

\1совские методы для хакеров» (Probabilistic Programming and Bayesian Methods for
Hackers , http://bit.ly/2fDJsKC). Она содержит рисунки, созданные с помощью библио-
теки Matplotlib, и использует в книге для создания единообразного и приятного
внешне стиля набор параметров rc. Этот стиль воспроизведен в таблице стилей
bmh (рис. 4.88):
In[14]: with plt.style.context('bmh'):
            hist_and_lines()
Рис. 4.88. Стиль
\1nd (рис. 4.89):
In[15]: with plt.style.context('dark_background'):
            hist_and_lines()
Рис. 4.89. Стиль dark_background

\1
In[16]: with plt.style.context('grayscale'):
            hist_and_lines()
Рис. 4.90. Стиль grayscale
Стиль Seaborn
В библиотеке Matplotlib есть также стили, источником вдохновения для которых
послужила библиотека Seaborn (обсуждаемая подробнее в разделе «Визуализация

============================================================
СТРАНИЦА 336
============================================================

\1n» данной главы). Как мы увидим, эти стили загру-
жаются автоматически при импорте пакета Seaborn в блокнот. Мне эти настройки
очень нравятся, я склонен использовать их как настр ойки по умолчанию в моих
собственных исследованиях данных (рис. 4.91):
In[17]: import seaborn
        hist_and_lines()
Рис. 4.91. Стили построения графиков
библиотеки Seaborn

\1
In[1]: from mpl_toolkits import mplot3d
После импорта этого подмодуля появляется возможность создавать трехмерные
системы координат путем передачи ключевого слова
\1n[2]: %matplotlib inline
       import numpy as np
       import matplotlib.pyplot as plt
In[3]:
\1
\1nline использовать команду
%matplotlib notebook.

\1
In[4]:
\1
\1
\1np.linspace(0, 15, 1000)

\1np.sin(zline)

============================================================
СТРАНИЦА 338
============================================================

\1
\1np.cos(zline)
       ax.plot3D(xline, yline, zline, 'gray')
       # Данные для трехмерных точек

\1np.random.random(100)

\1np.sin(zdata) + 0.1 * np.random.randn(100)

\1np.cos(zdata) + 0.1 * np.random.randn(100)
       ax.scatter3D(xdata, ydata, zdata,
\1ns');

\1добно двумерным графикам, создаваемым с помощью функции ax.contour, для
функции ax.contour3D необходимо, чтобы все входные данные находились в фор-
ме двумерных регулярных сеток, с вычислением данных по оси  Z  в каждой точке.

\1
In[5]: def f(x, y):
           return np.sin(np.sqrt(x ** 2 + y ** 2))

\1np.linspace(-6, 6, 30)

\1np.linspace(-6, 6, 30)
       X,
\1np.meshgrid(x, y)

============================================================
СТРАНИЦА 339
============================================================
Построение трехмерных графиков в библиотеке Matplotlib  339

\1n[6]:
\1
\1ntour3D(X, Y, Z, 50,
\1nary')
       ax.set_xlabel('x')
       ax.set_ylabel('y')
       ax.set_zlabel('z');
Рис. 4.94. Трехмерный контурный график
Иногда используемый по умолчанию угол зрения неидеален. В этом случае можно
воспользоваться методом view_init для задания угла возвышения и азимутального
угла. В нашем примере (результат которого показан на рис. 4.95) используется угол
возвышения 60 градусов (то есть 60 градусов над плоскостью X-Y ) и азимут 35 гра -
дусов (то есть график повернут на 35 градусов против часовой стрелки вокруг оси Z ):
In[7]: ax.view_init(60, 35)
       fig

\1ретаскиванием, при использовании одной из интерактивных прикладных частей
библиотеки Matplotlib.

============================================================
СТРАНИЦА 340
============================================================

\1n[8]:
\1
\1\1
In[9]:
\1
\1none')
       ax.set_title('surface'); # Поверхность

\1
In[10]:
\1np.linspace(0, 6, 20)

\1np.linspace(-0.9 * np.pi, 0.8 * np.pi, 40)
        r,
\1np.meshgrid(r, theta)

\1np.sin(theta)

\1np.cos(theta)

\1
\1none');

\1
In[11]:
\1np.pi * np.random.random(1000)

\1np.random.random(1000)

\1np.ravel(r * np.sin(theta))

\1np.ravel(r * np.cos(theta))

\1\1n[12]:
\1
\1
\1\1
In[13]:
\1
\1none');

\1
In[14]:
\1np.linspace(0, 2 * np.pi, 30)

\1np.linspace(-0.25, 0.25, 8)
        w,
\1np.meshgrid(w, theta)

\1скручивание полоски относительно
ее оси координат (назовем эту координату ϕ). Чтобы получилась лента Мебиуса,
полоска должна выполнить половину скручивания за время полного сворачивания
в кольцо, то есть Δϕ = Δθ / 2 .
In[15]:
\1\1
In[16]: # радиус в плоскости X-Y

\1np.cos(phi)

\1np.ravel(r * np.cos(theta))

\1np.ravel(r * np.sin(theta))

\1np.ravel(w * np.sin(phi))

\1
In[17]: # Выполняем триангуляцию в координатах базовой параметризации
        from matplotlib.tri import Triangulation

\1ngulation(np.ravel(w), np.ravel(theta))

\1
\1
\1ngles,

\1
\1\1n. В этом разделе
мы продемонстрируем несколько примеров, возможных благодаря этому набору
инструментов визуализаций карт.

\1пользуете conda, для скачивания и установки пакета вам достаточно набрать
команду:
$ conda install basemap1

\1

\1n  2 или
же Python 3 с младшей версией не выше 3.3. При необходимости вы можете установить на
свою машину неофициальный выпуск пакета Basemap с помощью следующей команды:
 conda install -c conda-forge
\1n[1]: %matplotlib inline
       import numpy as np
       import matplotlib.pyplot as plt
       from mpl_toolkits.basemap import Basemap

\1тов Basemap от построения географических графиков (построение графика
на рис. 4.102 требует также наличия пакета PIL в Python 2 или пакета pillow
в Python 3):
In[2]: plt.figure(
\1
\1
\1ne,
\1
\1\1

============================================================
СТРАНИЦА 346
============================================================

\1n[3]:
\1
\1
\1ne,

\1
\1
\1n.

\1
In[4]: from itertools import chain
       def draw_map(m,
\1np.linspace(-90, 90, 13))

\1ns(np.linspace(-180, 180, 13))
           # Ключи, содержащие экземпляры класса plt.Line2D

\1n(*(tup[1][0] for tup in lats.items()))

\1n(*(tup[1][0] for tup in lons.items()))

\1n(lat_lines, lon_lines)
           # Выполняем цикл по этим линиям, устанавливая нужный стиль
           for line in all_lines:
               line.set(
\1nt cylindrical projection), характеризующейся выбором такого масштабиро -
вания широты, при котором расстояния вдоль меридианов остаются неизменными.

\1проекция Меркатора (
\1ndrical projection).
In[5]:
\1
\1
\1ne,

\1
\1
\1
\1\1n) нижнего левого ( llcrnr) и верхнего правого
( urcrnr) углов карты в градусах.
Псевдоцилиндрические проекции
В псевдоцилиндрических проекциях отсутствует требов ание вертикальности
меридианов (линии одинаковой долготы), это позволяет улучшить показатели
возле полюсов проекции. Проекция Мольвейде (
\1
\1nu')
и проекция Робинсона (
\1n').
In[6]:
\1
\1
\1ne,

\1
\1n_0) центра карты.

\1ортографическая проекция (
\1
\1nom') и стереографическая (
\1\1
In[7]:
\1
\1
\1ne,

\1
\1\1
\1
\1
\1n[8]:
\1
\1
\1ne,

\1ns()для рисования линий с постоянной широтой
или долготой. Пакет Basemap содержит множество удобн ых функций для рисо -
вания границ физических объектов, например континентов, океанов, озер и рек,
а также политических границ — границ стран или штатов/округов США. Далее
приведены некоторые из имеющихся функций рисования, возможно, вы захотите
изучить их подробнее с помощью справочных средств оболочки IPython.
 Физические границы и водоемы:
 y drawcoastlines() — рисует континентальные береговые линии;
 y drawlsmask() — рисует маску «земля/море» с целью проекции изображений
на то или другое;
 y drawmapboundary()  — рисует границы на карте, включая заливку цветом
океанов;
 y drawrivers() — рисует реки на карте;
 y fillcontinents() — заливает пространство континентов заданным цветом;
в качестве дополнительной настройки может залить озера другим цветом.
 Политические границы:
 y drawcountries() — рисует границы стран;
 y drawstates() — рисует границы штатов США;
 y drawcounties() — рисует границы округов США.
 Свойства карты:
 y drawgreatcircle() — рисует большой круг между двумя точками;
 y drawparallels() — рисует линии с постоянной широтой (меридианы);
 y drawmeridians() — рисует линии с постоянной долготой (параллели);
 y drawmapscale() — рисует на карте линейную шкалу масштаба.
 Изображения всего земного шара:
 y bluemarble() — проецирует сделанную NASA фотографию «голубого ша -
рика» на карту;
 y shadedrelief() — проецирует на карту изображение с оттененным рельефом;
 y etopo() — рисует на карте изображение рельефа на основе набора данных etopo;
 y warpimage() — проецирует на карту пользовательское изображение.

============================================================
СТРАНИЦА 352
============================================================

\1n класса Basemap задает уровень
детализации границ 1  ( 'c' (от англ. crude) — грубая детализация, 'l' (от англ.
low) — низкая детализация, 'i' (от англ. intermediate) — средняя детализация, 'h'
(от англ. high) — высокая детализация, 'f' (от англ. full) — полная детализация,
или None — если границы не используются). Выбор значения этого параметра очень
важен. Например, отрисовка границ с высоким разрешением на карте земного шара
может происходить очень медленно.

\1
Рис. 4.108. Границы на карте при низком и высоком разрешении
In[9]: fig,
\1n enumerate(['l', 'h']):

\1
\1nom',
\1
\1
\1ntinents(
\1ndary(
\1\1nda install -c conda-forge basemap-data-
\1nes()
           ax[i].set_title("
\1n, позволяющий (при равном True значении) передавать им
исходные значения широты и долготы, а не их проекции на координаты ( X , Y ) .

\1
 contour()/contourf() — рисует контурные линии или заполненные контуры;
 imshow() — отображает изображение;
 pcolor() / pcolormesh()  — рисует псевдоцветной график для нерегулярных
и регулярных сеток;
 plot() — рисует линии и/или маркеры;
 scatter() — рисует точки с маркерами;
 quiver() — рисует вектора;
 barbs() — рисует стрелки ветра;
 drawgreatcircle() — рисует большой круг1 .

\1документации
Basemap.

\1n[10]: import pandas as pd

\1nia_cities.csv')
        # Извлекаем интересующие нас данные

\1
\1ngd'].values

\1n_total'].values

\1\1
In[11]: # 1. Рисуем фон карты

\1
\1
\1
\1nes(
\1ntries(
\1n, lat,
\1np.log10(population),
\1\log_{10}({\rm population})$')
        plt.clim(3, 7)
        # Делаем легенду с фиктивными точками
        for a in [100, 300, 500]:
            plt.scatter([], [],
\1nd(
\1
\1
\1na sa.gov/).
В этом случае мы воспользуемся температурными данными GIS 250, которые
можно скачать с помощью командной оболочки (возможно, на работающих под
управлением операционной системы Windows эти команды придется несколько
изменить). Используемые здесь данные скачивались 12 июня 2016 года, и их раз-
мер тогда составлял примерно 9 Мбайт:
In[12]: # !curl -O https://data.giss.nasa.gov/pub/gistemp/gistemp250.nc.gz
        # !gunzip gistemp250.nc.
\1\1n можно прочитать
с помощью библиотеки netCDF4. Установить эту библиотеку можно следующим
образом:
$ conda install netcdf4

\1
In[13]: from netCDF4 import Dataset

\1nc')

\1
In[14]: from netCDF4 import date2index
        from datetime import datetime

\1ndex(datetime(2014, 1, 15),
                               data.variables['time'])

\1
In[15]:
\1
\1n'][:]
        lon,
\1np.meshgrid(lon, lat)

\1nomaly'][timeindex]

\1вые линии:
In[16]:
\1
\1
\1
\1n, lat, temp_anomaly,

\1nes(
\1nuary 2014 Temperature Anomaly')
        plt.colorbar(
\1nomaly (°C)');
        # Температурные аномалии

============================================================
СТРАНИЦА 357
============================================================
Визуализация с помощью библиотеки Seaborn  357

\1ра была гораздо ниже обычного, в то время как на западе и Аляске было намного
теплее. В местах, где температура не регистрировалась, мы видим фон карты.
Визуализация с помощью библиотеки Seaborn

\1относительно низкоуровневый. С его помощью
можно создавать сложные статистические визуализации, но это требует немало
шаблонного кода.
 Matplotlib была выпущена на десятилетие раньше, чем библиотека Pandas, и по -
тому не ориентирована на работу с объектами DataFrame библиотеки Pandas.

============================================================
СТРАНИЦА 358
============================================================

\1ndas приходится
извлекать все объекты Series и зачастую конкатенировать их в нужный формат.

\1ствовали бы возможности по интеллектуальному использованию меток DataFrame
на графиках.
Библиотека Seaborn — решение этих проблем. Seaborn предоставляет API поверх
библиотеки Matplotlib, обеспечивающий разумные варианты стилей графиков
и цветов по умолчанию, определяющий простые высокоу ровневые функции для
часто встречающихся типов графиков и хорошо интегрирующийся с функциональ-
ностью, предоставляемой объектами DataFrame библиотеки Pandas.
Справедливости ради упомяну, что команда разработчиков библиотеки Matplotlib
тоже пытается решить эти проблемы: они добавили утилиты plt.style (которые
мы обсуждали в разделе «Пользовательские настройки Matplotlib: конфигурации
и таблицы стилей» этой главы) и принимают меры к более органичной обработке
данных Pandas. Версия 2.0 библиотеки Matplotlib включает новую таблицу стилей
по умолчанию, исправляющую ситуацию. Но по вышеизложенным причинам би-
блиотека Seaborn остается исключительно удобным дополнением.
Seaborn по сравнению с Matplotlib

\1
In[1]: import matplotlib.pyplot as plt
       plt.style.use('classic')
       %matplotlib inline
       import numpy as np
       import pandas as pd

\1
In[2]: # Создаем данные

\1np.random.RandomState(0)

\1np.linspace(0, 10, 500)

\1np.cumsum(rng.randn(500, 6), 0)

\1
In[3]: # Рисуем график, используя параметры Matplotlib по умолчанию
       plt.plot(x, y)
       plt.legend('ABCDEF',
\1n  359
Рис. 4.111. Данные в стиле библиотеки Matplotlib по умолчанию
Теперь посмотрим, как можно сделать это с помощью Seaborn. Помимо множества
собственных высокоуровневых функций построения графиков библиотеки Seaborn,
она может также перекрывать параметры по умолчанию библиотеки Matplotlib,
благодаря чему применение даже более простых сценариев Matplotlib приводит к на -
много лучшему результату. Задать стиль можно с помощью метода set() библиотеки
Seaborn. По принятым соглашениям Seaborn импортируется под именем sns:
In[4]: import seaborn as sns
       sns.set()

\1
In[5]: # Тот же самый код для построения графика, что и выше!
       plt.plot(x, y)
       plt.legend('ABCDEF',
\1n
О, намного лучше!

============================================================
СТРАНИЦА 360
============================================================

\1n
Основная идея библиотеки Seaborn — предоставление высокоуровневых команд
для создания множества различных типов графиков, удобных для исследования
статистических данных и даже подгонки статистических моделей.
Рассмотрим некоторые из имеющихся в Seaяborn наборов данных и типов гра -
фиков. Обратите внимание, что все изложенное далее можно  выполнить и с по -
мощью обычных команд библиотеки Matplotlib, но API Seaborn намного более
удобен.

\1
In[6]:
\1np.random.multivariate_normal([0, 0], [[5, 2], [2, 2]],

\1
\1n 'xy':
           plt.hist(data[col],
\1n выполняет с помощью
функции sns.kdeplot (рис. 4.114):

============================================================
СТРАНИЦА 361
============================================================
Визуализация с помощью библиотеки Seaborn  361
In[7]: for col in 'xy':
           sns.kdeplot(data[col],
\1\1
In[8]: sns.distplot(data['x'])
       sns.distplot(data['y']);

\1
In[9]: sns.kdeplot(data);

============================================================
СТРАНИЦА 362
============================================================

\1ns.jointplot . Для этого графика мы зададим стиль
с белым фоном (рис. 4.117):
In[10]: with sns.axes_style('white'):
            sns.jointplot("x", "y", data,
\1n  363
Функции jointplot можно передавать и другие параметры, например можно вос-
пользоваться гистограммой на базе шестиугольников (рис. 4.118):
In[11]: with sns.axes_style('white'):
            sns.jointplot("x", "y", data,
\1\1
In[12]:
\1ns.load_dataset("iris")
        iris.head()
Out[12]: | sepal_length  sepal_width  petal_length  petal_width species
0 | 5.1 | 3.5 | 1.4 | 0.2  setosa
1 | 4.9 | 3.0 | 1.4 | 0.2  setosa

\1ndows и ваша версия Python — 3.6, при
выполнении этих команд вы можете столкнуться с известной ошибкой, связанной с из -
менением кодировки имен файлов по умолчанию в Python 3.6. Простейшим решением
проблемы будет изменение кодировки на использовавшуюся в предыдущих версиях:
 import sys
 sys._enablelegacywindowsfsencoding().

============================================================
СТРАНИЦА 364
============================================================

\1ns.pairplot (рис. 4.119):
In[13]: sns.pairplot(iris,
\1n делает эту задачу элементарной. Рас -
смотрим данные, отображающие суммы, которые персона л ресторана получает
в качестве чаевых, в зависимости от данных различных индикаторов (рис. 4.120):
In[14]:
\1ns.load_dataset('tips')
        tips.head()
Out[14]: | total_bill | tip | sex smoker  day | time  size
0 | 16.99  1.01  Female | No  Sun  Dinner | 2

============================================================
СТРАНИЦА 365
============================================================
Визуализация с помощью библиотеки Seaborn  365
1 | 10.34  1.66 | Male | No  Sun  Dinner | 3
2 | 21.01  3.50 | Male | No  Sun  Dinner | 3
3 | 23.68  3.31 | Male | No  Sun  Dinner | 2
4 | 24.59  3.61  Female | No  Sun  Dinner | 4
In[15]: tips['tip_pct'] = 100 * tips['tip'] / tips['total_bill']

\1ns.FacetGrid(tips,
\1
\1ns=np.linspace(0, 40, 15));

\1
In[16]: with sns.axes_style(
\1ns.factorplot("day", "total_bill", "sex",
\1
\1ns.jointplot для отображения совместного распределения
между различными наборами данных, а также соответствующих частных распре-
делений (рис. 4.122):

============================================================
СТРАНИЦА 366
============================================================

\1n[17]: with sns.axes_style('white'):
            sns.jointplot("total_bill", "tip",
\1
\1\1
In[18]: sns.jointplot("total_bill", "tip",
\1
\1n  367
Рис. 4.123. График совместного распределения
с подбором регрессии
Столбчатые диаграммы
Графики временных рядов можно строить с помощью функции sns.factorplot.
В следующем примере, показанном на рис. 4.124, мы воспользуемся данными из
набора Planets («Планеты»), которые мы уже видели в разделе «Агрегирование
и группировка» главы 2:
In[19]:
\1ns.load_dataset('planets')
        planets.head()
Out[19]: | method  number  orbital_period | mass  distance  year
0  Radial Velocity | 1 | 269.300 | 7.10 | 77.40  2006
1  Radial Velocity | 1 | 874.774 | 2.21 | 56.95  2008
2  Radial Velocity | 1 | 763.000 | 2.60 | 19.84  2011
3  Radial Velocity | 1 | 326.030  19.40 | 110.62  2007
4  Radial Velocity | 1 | 516.220  10.50 | 119.47  2009
In[20]: with sns.axes_style('white'):

\1ns.factorplot("year",
\1nets,
\1
\1nt",
\1\1n[21]: with sns.axes_style('white'):

\1ns.factorplot("year",
\1nets,
\1
\1nt',

\1nge(2001, 2015))
            g.set_ylabels('Number of Planets Discovered')
            # Количество обнаруженных планет
Дополнительную информацию о построении графиков с помощью библиотеки
Seaborn можно найти в документации, справочном руководстве и галерее Seaborn.
Пример: время прохождения марафона
В этом разделе мы рассмотрим использование библиотеки Seaborn для визуали-
зации и анализа данных по времени прохождения марафонской дистанции. Эти
данные я собрал из различных интернет-источников, агрегировал, убрал все иден -
тифицирующие данные и поместил на GitHub, откуда их можно скачать (если вас
интересует использование языка Python для веб-скрапинга, рекомендую книгу
Web Scraping with Python 1  ( http://shop.oreilly.com/product/063 69200 34391.do ) Райана
Митчелла. Начнем со скачивания данных из Интернета и загрузки их в Pandas:
In[22]: # !curl -O https://raw.githubusercontent.com/jakevdp/marathon-data/
# master/marathon-data.csv
In[23]:
\1n-data.csv')
        data.head()
Out[23]: | age gender | split | final
0 | 33 | M  01:05:38  02:08:51
1 | 32 | M  01:06:26  02:09:28
2 | 31 | M  01:06:49  02:10:42
3 | 38 | M  01:06:16  02:13:45
4 | 31 | M  01:06:32  02:13:59

\1n. — М.: ДМК-Пресс, 2016.

============================================================
СТРАНИЦА 369
============================================================
Визуализация с помощью библиотеки Seaborn  369

\1режиме (https://github.com/jakevdp/PythonDataScienceHandbook))

============================================================
СТРАНИЦА 370
============================================================

\1ndas загружает столбцы с временем как строки
Python (тип object), убедиться в этом можно, посмотрев значение атрибута dtypes
объекта DataFrame:
In[24]: data.dtypes
Out[24]: age | int64
gender | object
split | object
final | object
         dtype: object

\1
In[25]: def convert_time(s):
            h, m,
\1nt, s.split(':'))
            return pd.datetools.timedelta(
\1
\1
\1n-data.csv',

\1nvert_time,
                           'final':convert_time})
        data.head()
Out[25]: | age gender | split | final
0 | 33 | M 01:05:3802:08:51
1 | 32 | M 01:06:2602:09:28
2 | 31 | M 01:06:4902:10:42
3 | 38 | M 01:06:1602:13:45
4 | 31 | M 01:06:3202:13:59
In[26]: data.dtypes
Out[26]: age | int64
gender | object
split | timedelta64[ns]
final | timedelta64[ns]
         dtype: object

\1
In[27]: data['split_sec'] = data['split'].astype(int) / 1E9
        data['final_sec'] = data['final'].astype(int) / 1E9
        data.head()
Out[27]: | age gender | split | final  split_sec  final_sec
0 | 33 | M 01:05:3802:08:51 | 3938.0 | 7731.0
1 | 32 | M 01:06:2602:09:28 | 3986.0 | 7768.0
2 | 31 | M 01:06:4902:10:42 | 4009.0 | 7842.0
3 | 38 | M 01:06:1602:13:45 | 3976.0 | 8025.0
4 | 31 | M 01:06:3202:13:59 | 3992.0 | 8039.0

\1n в 64-битной операционной системе Windows
может возникнуть ошибка преобразования типа. Один из путей решения этой проблемы —
использовать в следующем коде тип np.int64 вместо int.

============================================================
СТРАНИЦА 371
============================================================
Визуализация с помощью библиотеки Seaborn  371
Чтобы понять, что представляют собой данные, можно нарисовать для них график
jointplot (рис. 4.126):
In[28]: with sns.axes_style('white'):

\1ns.jointplot("split_sec", "final_sec", data,
\1nt.plot(np.linspace(4000, 16000),
                            np.linspace(8000, 32000), ':k')

\1
In[29]: data['split_frac'] = 1 - 2 * data['split_sec'] / data['final_sec']
        data.head()
Out[29]: | age gender | split | final  split_sec  final_sec  split_frac
0 | 33 | M 01:05:3802:08:51 | 3938.0 | 7731.0 | -0.018756
1 | 32 | M 01:06:2602:09:28 | 3986.0 | 7768.0 | -0.026262
2 | 31 | M 01:06:4902:10:42 | 4009.0 | 7842.0 | -0.022443
3 | 38 | M 01:06:1602:13:45 | 3976.0 | 8025.0 | 0.009097
4 | 31 | M 01:06:3202:13:59 | 3992.0 | 8039.0 | 0.006842

============================================================
СТРАНИЦА 372
============================================================

\1n[30]: sns.distplot(data['split_frac'],
\1ne(0,
\1
\1n[31]: sum(data.
\1\1
In[32]:

\1ns.PairGrid(data,
\1nal_sec', 'split_frac'],

\1nder',
\1nd();

\1лять свои силы поровну. Как мы видим на этом графике, библиотека Seaborn — не
панацея от «недугов» библиотеки Matplotlib, если речь идет о стилях графиков:

============================================================
СТРАНИЦА 373
============================================================
Визуализация с помощью библиотеки Seaborn  373
в частности, метки на оси X  перекрываются. Однако, поскольку результат — про-
стой график Matplotlib, можно воспользоваться методами из раздела «Пользо -
вательские настройки делений на осях координат» данной главы для настройки
подобных вещей.

\1
In[33]: sns.kdeplot(data.split_frac[data.
\1n',
\1ns.kdeplot(data.split_frac[data.
\1n',
\1\1n[34]:
sns.violinplot("gender", "split_frac",
\1nk"]);

\1
In[35]: data['age_dec'] = data.age.map(lambda age: 10 * (age // 10))
data.head()
Out[35]:
age gender | split | final  split_sec  final_sec  split_frac  age_dec
0 | 33 | M 01:05:3802:08:51 | 3938.0 | 7731.0 | -0.018756 | 30
1 | 32 | M 01:06:2602:09:28 | 3986.0 | 7768.0 | -0.026262 | 30
2 | 31 | M 01:06:4902:10:42 | 4009.0 | 7842.0 | -0.022443 | 30
3 | 38 | M 01:06:1602:13:45 | 3976.0 | 8025.0 | 0.009097 | 30
4 | 31 | M 01:06:3202:13:59 | 3992.0 | 8039.0 | 0.006842 | 30
In[36]:

\1
\1n  375

\1
\1ns.axes_style(
\1ne):
    sns.violinplot("age_dec", "split_frac",
\1nder",
\1
\1nk"]);
Рис. 4.130. «Скрипичная» диаграмма, показывающая зависимость
коэффициента распределения от пола
Рис. 4.131. «Скрипичная» диаграмма, отображающая зависимость
коэффициента распределения от пола и возраста

============================================================
СТРАНИЦА 376
============================================================

\1n[38]: (data.
\1\1
In[37]:
\1ns.lmplot('final_sec', 'split_frac',
\1nder',
\1ne,
\1n (см. раздел «Справка и документация в оболочке Python» главы 1) может
принести немалую пользу при изучении API библиотеки Matplotlib. Кроме того,
полезным источником информации может стать онлайн-документация Matplotlib
( http://matplotlib.org/), в частности галерея Matplotlib ( http://matplotlib.org/gallery.html).
В ней представлены миниатюры сотен различных типов гра фиков, каждая из
которых представляет собой ссылку на страницу с фраг ментом кода на языке
Python, используемым для его генерации. Таким образом, вы можете визуально
изучить широкий диапазон различных стилей построения графиков и методик
визуализации.

\1ратить внимание на книгу Interactive Applications Using Matplotlib  ( http://bit.ly/2fSqswQ),
написанную разработчиком ядра Matplotlib Беном Рутом.
Другие графические библиотеки языка Python

\1наиболее значительная из предназначенных для визуализации
библиотек языка Python, существуют и другие, более современные инструменты,
заслуживающие пристального внимания. Я перечислю некоторые из них.
 Bokeh ( http://bokeh.pydata.org/ ) — JavaScript-библиотека визуализации с кли -
ентской частью для языка Python, предназначенная дл я создания высокоин -
терактивных визуализаций с возможностью обработки очень больших и/или
потоковых наборов данных. Клиентская часть Python возвращает структуры
данных в формате JSON, интерпретируемые затем JavaScript-движком библио-
теки Bokeh.
 Plotly ( http://plot.ly/) — продукт с открытым исходным кодом одноименной ком -
пании, аналогичный по духу библиотеке Bokeh. Поскольку Plotly — основной
продукт этого стартапа, разработчики прилагают максимум усилий к его раз-
работке. Использовать эту библиотеку можно совершенно бесплатно.
 Vispy ( http://vispy.org/) — активно разрабатываемый программный продукт, ори -
ентированный на динамические визуализации очень больших наборов данных.
В силу его ориентации на OpenGL и эффективное использование графических
процессоров он способен формировать очень большие и впечатляющие визуа-
лизации.

============================================================
СТРАНИЦА 378
============================================================

\1n находится в процессе разработки, в пакете Al tair ( http://altair-viz.github.io/).
Хотя он еще не вполне готов, меня радует сама возможность, что этот проект
послужит общей точкой отсчета для визуализаций на языке Python и других
языках программирования.
Сфера визуализации в Python-сообществе меняется очень динамично, и я уверен,
что этот список устареет сразу после публикации. Внимательно следите за ново-
стями в данной области!

============================================================
СТРАНИЦА 379
============================================================

\1nce), пере-
обучение (overfitting) и недообучение (underfitting) и т. д.

\1Learn ( http://scikit-learn.org/) языка Python. Здесь не планируется
всестороннее введение в сферу машинного обучения, поскольку это обширная
тема, требующая более формализованного подхода. Не планируется и всестороннее
руководство по пакету Scikit-Learn (такие руководства вы можете найти в разделе
«Дополнительные источники информации по машинному обучению» этой главы).

\1
 с базовой терминологией и понятиями машинного обучения;
 с API библиотеки Scikit-Learn и некоторыми примерами его использования;
 с подробностями нескольких наиболее важных методов машинного обучения,
помочь разобраться в том, как они работают, а также где и когда применимы.

\1Learn, а также семи-
наров, проводившихся мной на PyCon, SciPy, PyData и других конференциях.
Многолетние отзывы участников и других докладчиков семинаров позволили
сделать изложение материала более доходчивым!

============================================================
СТРАНИЦА 380
============================================================

\1ning) — включает моделирование
признаков данных и соответствующих данным меток. После выбора модели ее
можно использовать для присвоения меток новым, неизвестным ранее данным.

\1фикации метки представляют собой дискретные категории, а при регрессии они
являются непрерывными величинами. Мы рассмотрим примеры обоих типов
машинного обучения с учителем в следующем разделе.
 Машинное обучение без учителя  (unsupervised learning) — включает модели -
рование признаков набора данных без каких-либо меток и описывается фра -
зой «Пусть набор данных говорит сам за себя». Эти модели включают такие
 з адачи, как кластеризация (clustering) и понижение размерности (
\1n). Алгоритмы кластеризации служат для выделения отдельных групп
данных, в то время как алгоритмы понижения размерности предназначены для
поиска более сжатых представлений данных. Мы рассмотрим примеры обоих
типов машинного обучения без учителя в следующем разделе.

\1supervised learning), располагающиеся примерно посередине между машинным
обучением с учителем и машинным обучением без учителя. Методы частичного
обучения бывают полезны в случае наличия лишь неполных меток.

\1дать интуи -
тивно понятный обзор тех разновидностей машинного обучения, с которыми мы
столкнемся в этой главе. В следующих разделах мы рассмотрим подробнее соот -
ветствующие модели и их использование. Чтобы получить представление о более
технических аспектах, вы можете заглянуть в онлайн-приложение ( https://github.
com/jakevdp/PythonDataScienceHandbook ), а также в генерирующие соответствующие
рисунки исходные коды на языке Python.

\1ляют собой конкретные числа, описывающие местоположение и направленность

============================================================
СТРАНИЦА 382
============================================================

\1ning
the model). Рисунок 5.2 демонстрирует вид обученной модели для наших данных.
Рис. 5.1. Простой набор данных для классификации
Рис. 5.2. Простая модель классификации

============================================================
СТРАНИЦА 383
============================================================
Что такое машинное обучение  383
После обучения модели ее можно обобщить на новые, немаркированные данные.
Другими словами, можно взять другой набор данных, провести прямую модели
через них, после чего на основе этой прямой присвоить метки новым точкам. Этот
этап обычно называют предсказанием  (prediction) (рис. 5.3).

\1
 признак 1 , признак 2  и т. д. → нормированные количества ключевых слов или
фраз («Виагра», «Нигерийский принц» и т. д.);
 метка → «спам» или «не спам».

\1фикации в разделе «Заглянем глубже: наивная байесовская классификация»
данной главы.

============================================================
СТРАНИЦА 384
============================================================

\1n, http://scikit-learn.org/stable/modules/clustering.html).

\1еще один пример алгоритма обучения без учителя,
в котором метки или другая информация определяются исходя из структуры
самого набора данных. Алгоритм понижения размерности несколько труднее для
понимания, чем рассмотренные нами ранее примеры, но он заключается в попытке
получения представления низкой размерности, которое бы в какой-то мере со -
храняло существенные качества полного набора данных. Различные алгоритмы
понижения размерности оценивают существенность этих качеств по-разному, как
мы увидим далее в разделе «Заглянем глубже: обучение на базе многообразий»
этой главы.

\1дель понижения размерности в таком случае должна учитывать эту нелинейную

============================================================
СТРАНИЦА 389
============================================================
Что такое машинное обучение  389
вложенную структуру, чтобы суметь получить из нее представление более низкой
размерности.
Рис. 5.10. Пример данных для понижения размерности
На рис. 5.11 представлена визуализация результатов работы алгоритма обучения
на базе многообразий Isomap, который именно это и делает.
Рис. 5.11. Данные с метками, полученными с помощью
алгоритма понижения размерности

============================================================
СТРАНИЦА 390
============================================================

\1nDataScienceHandbook).

============================================================

\1Learn  391

\1Learn
Существует несколько библиотек языка Python с надежными реализациями ши-
рокого диапазона алгоритмов машинного обучения. Одна из самых известных —
Scikit-Learn, пакет, предоставляющий эффективные версии множества распро -
страненных алгоритмов. Пакет Scikit-Learn отличает аккуратный, единообразный
и продвинутый API, а также удобная и всеохватывающая онлайн-документация.

\1Learn для одного типа моделей, вы сможете легко перейти
к другой модели или алгоритму.

\1Learn. Ясное понимание
элементов API — основа для более углубленного практического обсуждения алго -
ритмов и методов машинного обучения в следующих разделах.
Начнем с представления данных  (data representation) в библиотеке Scikit-Learn,
затем рассмотрим API Estimator (API статистического оценивания) и, наконец,
взглянем на интересный пример использования этих инструментов для исследо-
вания набора изображений рукописных цифр.

\1Learn

\1Learn данные в виде таблиц.

\1атрибуты, связанные с каждым
из этих элементов. Например, рассмотрим набор данных Iris ( https://en.wikipedia.
org/wiki/Iris_flower_data_set), проанализированный Рональдом Фишером в 1936 году.
Скачаем его в виде объекта DataFrame библиотеки Pandas с помощью библиотеки
Seaborn:
In[1]: import seaborn as sns

\1ns.load_dataset('iris')
       iris.head()
Out[1]: | sepal_length  sepal_width  petal_length  petal_width species
0 | 5.1 | 3.5 | 1.4 | 0.2  setosa
1 | 4.9 | 3.0 | 1.4 | 0.2  setosa
2 | 4.7 | 3.2 | 1.3 | 0.2  setosa
3 | 4.6 | 3.1 | 1.5 | 0.2  setosa
4 | 5.0 | 3.6 | 1.4 | 0.2
\1\1n_samples.

\1ками (features), а количество столбцов полагать равным n_features.

\1двумерная , с формой
[n_samples, n_features] , и хранят ее чаще всего в массиве NumPy или объекте
DataFrame библиотеки Pandas, хотя некоторые модели библиотеки Scikit-Learn
допускают использование также разреженных матриц из библиотеки SciPy.

\1сивом меток), который принято обозначать y. Целевой массив обычно одномерен,
длиной n_samples. Его хранят в массиве NumPy или объекте Series библиотеки
Pandas. Значения целевого массива могут быть непрерывными числовыми или
дискретными классами/метками. Хотя некоторые оцениватели библиотеки Scikit-
Learn умеют работать с несколькими целевыми величинами в виде двумерного
целевого массива [n_samples, n_targets], мы в основном будем работать с более
простым случаем одномерного целевого массива.
Отличительная черта целевого массива от остальных столбцов признаков в том,
что он представляет собой величину, значения которой мы хотим предсказать на

\1n  393
основе имеющихся данных. Говоря статистическим языком, это зависимая перемен-
ная (dependent variable). Например, для предыдущих данных это могла оказаться
модель для предсказания вида цветка на основе остальных измерений. В таком
случае столбец species рассматривался бы как целевой массив.
С учетом вышесказанного можно воспользоваться библиотекой Seaborn (которую
мы рассматривали в разделе «Визуализация с помощью библиотеки Seaborn» гла -
вы 4), чтобы без труда визуализировать данные (рис. 5.12):
In[2]: %matplotlib inline
       import seaborn as sns; sns.set()
       sns.pairplot(iris,
\1n мы извлечем матрицу при-
знаков и целевой массив из объекта DataFrame. Сделать это можно с помощью
обсуждавшихся в главе 3 операций объекта DataFrame библиотеки Pandas:

============================================================
СТРАНИЦА 394
============================================================

\1n[3]:
\1n[4]:
\1\1

\1Learn

\1Learn.
API статистического оценивания библиотеки Scikit-Learn

\1Learn говорится, что он основывается на следующих
принципах:
 единообразие — интерфейс всех объектов идентичен и основан на ограниченном
наборе методов, причем документация тоже единообразна;
 контроль  — видимость всех задаваемых значений параметров как открытых
атрибутов;
 ограниченная иерархия объектов — классы языка Python используются только
для алгоритмов; наборы данных представлены в стандартных форматах (массивы
NumPy, объекты DataFrame библиотеки Pandas, разреженные матрицы библиотеки
SciPy), а для имен параметров используются стандартные строки языка Python;
 объединение — многие из задач машинного обучения можно выразить в виде по-
следовательностей алгоритмов более низкого уровня, и библиотека Scikit-Learn
пользуется этим фактом при любой возможности;

============================================================

\1Learn  395
 разумные значения по умолчанию  — библиотека задает для необходимых
моделей пользовательских параметров соответствующие значения по умол -
чанию.

\1Learn.

\1Learn реализуются через
API статистического оценивания, предоставляющий единообразный интерфейс
для широкого диапазона прикладных задач машинного обучения.

\1Learn включает следующие шаги (далее мы рассмотрим несколько подробных
примеров).

\1n.

\1nsform()
или predict().

\1
In[5]: import matplotlib.pyplot as plt
       import numpy as np

\1np.random.RandomState(42)

\1ng.rand(50)

\1ng.randn(50)
       plt.scatter(x, y);

============================================================
СТРАНИЦА 396
============================================================

\1n представлен соответствующим
классом языка Python. Так, например, для расчета модели простой линейной
регрессии можно импортировать класс линейной регрессии:
In[6]: from sklearn.linear_model import LinearRegression

\1arn.li near_model ( http://scikit-learn.org/stable/modules/linear_model.html).

\1n  397

\1Learn осуществляется путем передачи
значений при создании экземпляра модели. Мы рассмотрим количественные
обоснования выбора гиперпараметров в разделе «Гиперпараметры и проверка
модели» данной главы.
Создадим экземпляр класса LinearRegression и укажем с помощью гиперпара-
метра fit_intercept, что нам бы хотелось выполнить подбор точки пересечения
с осью координат:
In[7]:
\1nearRegression(
\1nearRegression(
\1
\1
\1
\1n очень четко разделяет выбор
модели и применение модели к данным.

\1n, для которого необходимы двумерная матрица признаков и одномерный
целевой вектор. Наша целевая переменная y уже имеет нужный вид (массив
длиной n_samples), но нам придется проделать небольшие манипуляции с дан -
ными x, чтобы сделать из них матрицу размера [n_samples, n_features]. В дан-
ном случае манипуляции сводятся просто к изменению фор мы одномерного
массива:
In[8]:
\1np.newaxis]
       X.shape
Out[8]: (50, 1)

\1n[9]: model.fit(X, y)
Out[9]: LinearRegression(
\1
\1
\1
\1n по

============================================================
СТРАНИЦА 398
============================================================

\1n[10]: model.coef_
Out[10]: array([ 1.97 76566])
In[11]: model.intercept_
Out[11]: -0.90 33107 25531 11635

\1Learn не предоставляет
инструментов, позволяющих делать выводы непосредственно из внутренних
параметров модели: интерпретация параметров скорее вопрос статистического
моделирования, а не машинного обучения. Машинное обучение концентрируется
в основном на том, что предсказывает модель. Для тех, кто хочет узнать больше
о смысле подбираемых параметров модели, существуют другие инструменты,
включая пакет StatsModels языка Python ( http://statsmodels.sourceforge.net/).

\1n
можно посредством метода predict(). В этом примере наши новые данные будут
сеткой x -значений и нас будет интересовать, какие y-значения предсказывает
модель:
In[12]:
\1np.linspace(-1, 11)

\1значения требуется преобразовать в матрицу признаков
[n_samples, n_features], после чего можно подать их на вход модели:
In[13]:
\1np.newaxis]

\1\1
In[14]: plt.scatter(x, y)
        plt.plot(xfit, yfit);

\1Learn  399

\1

\1хороший кандидат на роль эталонной
классификации. Имеет смысл поэкспериментировать с ним, прежде чем выяснять,
можно ли получить лучшие результаты с помощью более сложных моделей.
Мы собираемся проверить работу модели на неизвестных ей данных, так что
необходимо разделить данные на обучающую последовательность  (training set)
и контрольную последовательность (testing set). Это можно сделать вручную, но
удобнее воспользоваться вспомогательной функцией train_test_split:
In[15]: from sklearn.cross_validation import train_test_split
        Xtrain, Xtest, ytrain,
\1n_test_split(X_iris, y_iris,

\1\1
In[16]: from sklearn.naive_bayes import GaussianNB # 1. Выбираем класс модели

\1nNB() | # 2. Создаем экземпляр модели

============================================================
СТРАНИЦА 400
============================================================

\1n, ytrain) | # 3. Обучаем модель на данных

\1\1
In[17]: from sklearn.metrics import accuracy_score
        accuracy_score(ytest, y_model)
Out[17]: 0.97368421052631582

\1
In[18]:
from sklearn.decomposition import PCA  # 1. Выбираем класс модели

\1
\1nsform(X_iris) | # 4. Преобразуем данные в двумерные

\1
In[19]: iris['PCA1'] = X_2D[:, 0]
        iris['PCA2'] = X_2D[:, 1]
        sns.lmplot("PCA1", "PCA2",
\1n  401

\1либо меткам. Здесь
мы собираемся использовать мощный алгоритм кластеризации под названием
смесь Гауссовых распределений (Gaussian mixture model, GMM), которую из -
учим подробнее в разделе «Заглянем глубже: смеси Гауссовых распределений»
этой главы. Метод GMM состоит в попытке моделирования данных в виде набора

\1
In[20]:
from sklearn.mixture import GMM | # 1. Выбираем класс модели

\1
\1
\1n для построения графика результатов (рис. 5.17):

============================================================
СТРАНИЦА 402
============================================================

\1n  403
In[21]:
iris['cluster'] = y_gmm
sns.lmplot("PCA1", "PCA2",
\1nica
смешались между собой. Следовательно, даже если у нас нет эксперта, который мог
бы сообщить нам, к каким видам относятся отдельные цветки, одних измерений
вполне достаточно для автоматического распознания этих различных разновид-
ностей цветков с помощью простого алгоритма кластеризации! Подобный алгоритм
может в дальнейшем помочь специалистам по предметной области выяснить связи
между исследуемыми образцами.

\1распознавание рукописных
цифр. Традиционно эта задача включает как определение местоположения на рисун-
ке, так и распознание символов. Мы пойдем самым коротким путем и воспользуемся
встроенным в библиотеку Scikit-Learn набором преформатированных цифр.

\1Learn и посмо-
трим на эти данные:
In[22]: from sklearn.datasets import load_digits

\1\1
In[23]: import matplotlib.pyplot as plt
        fig,
\1n enumerate(axes.flat):
            ax.imshow(digits.images[i],
\1nary',

\1nearest')

============================================================
СТРАНИЦА 404
============================================================

\1
\1nsAxes,
\1n')

\1Learn нам нужно получить их
двумерное [n_samples, n_features] представление. Для этого мы будем тракто -
вать каждый пиксел в изображении как признак, то есть «расплющим» массивы
пикселов  так, чтобы каждую цифру представлял массив пикселов длиной 64 эле-
мента. Кроме этого, нам понадобится целевой массив, задающий для каждой ци-
фры предопределенную метку. Эти два параметра встроены в набор данных цифр
в виде атрибутов data и target, соответственно:
In[24]:
\1n[25]:
\1n  405

\1
In[26]: from sklearn.manifold import Isomap

\1
\1nsform(digits.data)
        data_projected.shape
Out[26]: (1797, 2)

\1
In[27]: plt.scatter(data_projected[:, 0], data_projected[:, 1],

\1none',
\1nge(10))
        plt.clim(-0.5, 9.5);

\1ном пространстве. Например, нули (отображаемые черным цветом) и единицы

============================================================
СТРАНИЦА 406
============================================================

\1n[28]: Xtrain, Xtest, ytrain,
\1n_test_split(X, y,
\1n[29]: from sklearn.naive_bayes import GaussianNB

\1nNB()
        model.fit(Xtrain, ytrain)

\1\1
In[30]: from sklearn.metrics import accuracy_score
        accuracy_score(ytest, y_model)
Out[30]: 0.83333333333333337

\1ную
точность классификации цифр! Однако из одного числа сложно понять, где наша
модель ошиблась. Для этой цели удобна так называемая матрица различий (confusion
matrix), вычислить которую можно с помощью библиотеки Scikit-Learn, а нарисовать
посредством Seaborn (рис. 5.20):
In[31]: from sklearn.metrics import confusion_matrix

\1nfusion_matrix(ytest, y_model)
        sns.heatmap(mat,
\1
\1n  407

\1
In[32]: fig,
\1n enumerate(axes.flat):
            ax.imshow(digits.images[i],
\1nary',

\1nearest')
            ax.text(0.05, 0.05, str(y_model[i]),

\1nsAxes,

\1n' if (ytest[i] == y_model[i]) else 'red')

\1сификации.

============================================================
СТРАНИЦА 408
============================================================

\1nDataScienceHandbook)

\1Learn, а также API статистического оценивания. Независимо от
типа оценивателя применяется одна и та же схема: импорт/создание экземпляра/
обучение/предсказание. Вооружившись этой информацией по API статистического
оценивания, вы можете, изучив документацию библиотеки Scikit-Learn, начать
экспериментировать, используя различные модели для своих данных.
В следующем разделе мы рассмотрим, вероятно, самый важный вопрос машинного
обучения: выбор и проверку модели.
Гиперпараметры и проверка модели
В предыдущем разделе описана основная схема использования моделей машинного
обучения с учителем.

\1n[1]: from sklearn.datasets import load_iris

\1
\1n[2]: from sklearn.neighbors import KNeighborsClassifier

\1
\1\1
In[3]: model.fit(X, y)

\1\1

============================================================
СТРАНИЦА 410
============================================================

\1n[4]: from sklearn.metrics import accuracy_score
       accuracy_score(y, y_model)
Out[4]: 1.0

\1

\1оцениватель, рабо -
тающий путем обучения на примерах  (instance-based estimator), попросту сохра -
няющий обучающие данные и предсказывающий метки путем сравнения новых
данных с этими сохраненными точками. За исключением некоторых специально
сконструированных случаев его точность будет всегда равна 100 %!

\1Learn можно произвести с помощью утилиты train_test_split:
In[5]: from sklearn.cross_validation import train_test_split
       # Разделяем данные: по 50% в каждом из наборов
       X1, X2, y1,
\1n_test_split(X, y,
\1
\1n), то есть выполнение последовательности аппроксимаций, в которых
каждое подмножество данных используется как в качестве обучающей последо -
вательности, так и проверочного набора. Наглядно его можно представить в виде
рис. 5.22.

\1
In[6]:
\1n).

\1крестная проверка.

============================================================
СТРАНИЦА 412
============================================================

\1n для большей краткости синтаксиса:
In[7]: from sklearn.cross_validation import cross_val_score
       cross_val_score(model, X, y,
\1n реализует множество схем перекрестной проверки, удоб -
ных в определенных конкретных ситуациях. Они реализованы с помощью итера-
торов в модуле cross_validation. Например, мы взяли предельный случай, при
котором количество блоков равно количеству точек данных, и обучаем модель
в каждой попытке на всех точках, кроме одной. Такой тип перекрестной проверки
известен под названием перекрестной проверки по отдельным объектам  (leave-
one-out cross-validation — дословно «перекрестная проверка по всем без одного»),
ее можно использовать следующим образом:

============================================================
СТРАНИЦА 413
============================================================
Гиперпараметры и проверка модели  413
In[8]: from sklearn.cross_validation import LeaveOneOut

\1neOut(len(X)))
       scores
Out[8]: array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
                1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
                1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
                1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
                1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
                1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,
                1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
                1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
                1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
                1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
                1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
                1.,  1.,  1.,  1.,  1.,  1.,  1.])

\1
In[9]: scores.mean()
Out[9]: 0.95999999999999996

\1Learn, воспользуйтесь
оболочкой IPython для просмотра подмодуля sklearn.cross_validation или обра-
титесь к онлайн-документации по перекрестной проверке библиотеки Scikit-Learn.

\1
 использовать более сложную/гибкую модель;
 применять менее сложную/гибкую модель;

============================================================
СТРАНИЦА 414
============================================================

\1nce). Рассмотрим
рис. 5.24, на котором представлены два случая регрессионной аппроксимации
одного набора данных.

\1щью прямолинейной модели невозможно описать этот набор данных достаточно
хорошо. О подобной модели говорят, что она недообучена (underfit), то есть гибкость
модели недостаточна для удовлетворительного учета всех признаков в данных.

\1коэффициент де -
терминации  или коэффициент смешанной корреляции  (подробнее о нем можно
прочитать по адресу https://ru.wikipedia.org/wiki/Коэффициент_детерминации). Он пред -
ставляет собой меру того, насколько хорошо модель работает по сравнению с про-
стым средним значением целевых величин. R 2 = 1 означает идеальное совпадение
предсказаний, а R 2  = 0 показывает, что модель оказалась ничем не лучше простого
среднего значения данных, а отрицательные значения указывают на модели, ко -
торые работают еще хуже.
На основе оценок эффективности вышеприведенных двух моделей мы можем
сделать следующее обобщенное наблюдение.

============================================================
СТРАНИЦА 416
============================================================

\1n curve).

\1Learn
Рассмотрим пример перекрестной проверки для расчета кривой проверки для класса
моделей. Мы будем использовать модель полиномиальной регрессии  (polynomial
regression model): это обобщенная линейная модель с параметризованной степе -
нью многочлена. Например, многочлен 1-й степени аппроксимирует наши данные
прямой линией; при параметрах модели a  и b :

\1\1

\1n реализовать это можно с помощью простой линейной регрессии
в сочетании с полиномиальным препроцессором. Мы воспользуемся конвейером
(pipeline) для соединения этих операций в единую цепочку (мы обсудим поли -
номиальные признаки и конвейеры подробнее в разделе  «Проектирование при -
знаков» данной главы):
In[10]: from sklearn.preprocessing import PolynomialFeatures
        from sklearn.linear_model import LinearRegression
        from sklearn.pipeline import make_pipeline
        def PolynomialRegression(
\1n make_pipeline(PolynomialFeatures(degree),
                                 LinearRegression(**kwargs))

\1
In[11]: import numpy as np
        def make_data(N,
\1
\1np.random.RandomState(rseed)

\1ng.rand(N, 1) ** 2

\1\1ng.randn(N)
            return X, y
        X,
\1\1
In[12]: %matplotlib inline
        import matplotlib.pyplot as plt
        import seaborn; seaborn.set()  # plot formatting

\1np.linspace(-0.1, 1.1, 500)[:, None]
        plt.scatter(X.ravel(), y,
\1n [1, 3, 5]:

\1nomialRegression(degree).fit(X, y).predict(X_test)
            plt.plot(X_test.ravel(), y_test,
\1nd(
\1\1

============================================================

\1Learn удобной утилиты validation_curve. Эта функция, получив на
входе модель, данные, название параметра и диапазон для анализа, автоматически
вычисляет в этом диапазоне значение как оценки эффективности для обучения,
так и оценки эффективности для проверки (рис. 5.28):
In[13]:
from sklearn.learning_curve import validation_curve

\1np.arange(0, 21)
train_score,
\1n_curve(PolynomialRegression(), X, y,
                                          'polynomialfeatures__degree',
                                          degree,
\1np.median(train_score, 1),
\1ning score') | # Оценка обучения
plt.plot(degree, np.median(val_score, 1),
\1n score') # Оценка проверки
plt.legend(
\1\1n[14]: plt.scatter(X.ravel(), y)

\1nomialRegression(3).fit(X, y).predict(X_test)
        plt.plot(X_test.ravel(), y_test);
        plt.axis(lim);

\1
In[15]: X2,
\1\1
In[16]:

\1np.arange(21)
train_score2,
\1n_curve(PolynomialRegression(), X2, y2,
                                            'polynomialfeatures__degree',
                                            degree,
\1np.median(train_score2, 1),
\1ning score')
plt.plot(degree, np.median(val_score2, 1),
\1n score')
plt.plot(degree, np.median(train_score, 1),
\1
\1np.median(val_score, 1),
\1
\1nd(
\1nter')
plt.ylim(0, 1)
plt.xlabel('degree')
plt.ylabel('score');

\1оценки эффективности для проверки и обу-
чения остаются очень близки друг к другу.

============================================================
СТРАНИЦА 422
============================================================

\1ning curve).

\1сходимость к конкретном у значению
оценки при росте числа обучающих выборок. В частности, если количество точек
достигло значения, при котором данная конкретная модель сошлась, то добавление
новых обучающих данных не поможет ! Единственным способом улучшить каче -
ство модели в этом случае будет использование другой (зачастую более сложной)
модели.
 Рис. 5.32. Схематическое изображение типичной кривой обучения

\1Learn.  Библиотека Scikit-Learn предо -
ставляет удобные утилиты для вычисления кривых обучения для моделей. В этом
разделе мы вычислим кривую обучения для нашего исходного набора данных с по-
линомиальными моделями второй и девятой степени (рис. 5.33):
In[17]:
from sklearn.learning_curve import learning_curve
fig,
\1n enumerate([2, 9]):
    N, train_lc,
\1ning_curve(PolynomialRegression(degree),
                                   X, y,
\1n_sizes=np.linspace(0.3, 1, 25))
    ax[i].plot(N, np.mean(train_lc, 1),
\1ning score')
    ax[i].plot(N, np.mean(val_lc, 1),
\1n score')
    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],

============================================================
СТРАНИЦА 424
============================================================

\1
\1ning size') # Размерность обучения
    ax[i].set_ylabel('score')
    ax[i].set_title('
\1nd(
\1n предоставляет для этой цели специальные автоматиче -
ские инструменты, содержащиеся в модуле grid_search. Рассмотрим трехмерную
сетку признаков модели — степени многочлена, флага, указывающего, нужно ли
подбирать точку пересечения с осью координат, и флага, указывающего, следует
ли выполнять нормализацию. Выполнить эти настройки можно с помощью мета-
оценивателя GridSearchCV библиотеки Scikit-Learn:
In[18]: from sklearn.grid_search import GridSearchCV

\1nomialfeatures__degree': np.arange(21),
                      'linearregression__fit_intercept': [True, False],
                      'linearregression__normalize': [True, False]}

\1nomialRegression(), param_grid,
\1\1
In[19]: grid.fit(X, y);

\1
In[20]: grid.best_params_
Out[20]: {'linearregression__fit_intercept': False,
          'linearregression__normalize': True,
          'polynomialfeatures__degree': 4}

\1
In[21]:
\1\1n, посвященной поиску по сетке
( http://scikit-learn.org/stable/modules/grid_search.html).

\1дела при дальнейшем чтении книги и изучении упомянутых методов машинного
обучения!

============================================================
СТРАНИЦА 427
============================================================
Проектирование признаков  427
Проектирование признаков
В предыдущих разделах мы обрисовали базовые понятия машинного обучения,
но во всех предполагалось, что наши данные находятся в аккуратном формате
[n_samples, n_features] . На практике же данные редко поступают к нам в по -
добном виде. Поэтому одним из важнейших этапов использования машинного
обучения на практике становится проектирование признаков (feature engineering),
то есть преобразование всей касающейся задачи информации в числа, пригодные
для построения матрицы признаков.

\1один из распространенных типов нечисловых данных.

\1то данные по ценам на жилье
и, помимо числовых признаков, таких как «цена» и «количество комнат», в них
имеется информация о микрорайоне (neighborhood). Например, пусть наши дан -
ные выглядят следующим образом:
In[1]:
\1neighborhood': 'Queen Anne'},
           {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},
           {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},
           {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}
       ]

\1
In[2]: {'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3};

\1Learn такой подход не очень удобен: мо-
дели данного пакета исходят из базового допущения о том, что числовые признаки
отражают алгебраические величины. Следовательно, подобное отображение будет
подразумевать, например, что Queen
\1
\1ngford  или даже что
Wallingford – Queen
\1nt , что, не считая сомнительных демографических
шуток, не имеет никакого смысла.

============================================================
СТРАНИЦА 428
============================================================

\1ne-hot
encoding), означающее создание дополнительных столбцов-индикаторов наличия/
отсутствия категории с помощью значений 1 или 0 соответственно. При наличии
данных в виде списка словарей для этой цели можно воспользоваться утилитой
DictVectorizer библиотеки Scikit-Learn:
In[3]: from sklearn.feature_extraction import DictVectorizer

\1nt)
       vec.fit_transform(data)
Out[3]: array([[ | 0, | 1, | 0, 850000, | 4],
[ | 1, | 0, | 0, 700000, | 3],
[ | 0, | 0, | 1, 650000, | 3],
[ | 1, | 0, | 0, 600000, | 2]],
\1nt64)
Обратите внимание, что столбец neighborhood  превратился в три отдельных
столбца, отражающих три метки микрорайонов, и что в каждой строке стоит 1
в соответствующем ее микрорайону столбце. После под обного кодирования
категориальных признаков можно продолжить обучение модели Scikit-Learn
обычным образом.

\1
In[4]: vec.get_feature_names()
Out[4]: ['
\1nt',
         '
\1n Anne',
         '
\1ngford',
         'price',
         'rooms']

\1
In[5]:
\1nt)
       vec.fit_transform(data)
Out[5]: <4x5 sparse matrix of type '<class 'numpy.int64'>'
            with 12 stored elements in Compressed Sparse Row
\1n допускают пере-
дачу им подобных разреженных входных данных при обучении и оценке моделей.

\1Learn включает две
дополнительные утилиты: sklearn.preprocessing.OneHotEncoder и skle arn.fea-
ture_extraction.FeatureHasher.

============================================================

\1
In[6]:
\1n',
                 'horizon problem']

\1ветствующие словам problem, evil, horizon и т. д. Хотя это можно сделать вручную,
мы избежим нудной работы, воспользовавшись утилитой CountVectorizer библио-
теки Scikit-Learn:
In[7]: from sklearn.feature_extraction.text import CountVectorizer

\1ntVectorizer()

\1nsform(sample)
       X
Out[7]: <3x5 sparse matrix of type '<class 'numpy.int64'>'
            with 7 stored elements in Compressed Sparse Row
\1\1
In[8]: import pandas as pd
       pd.DataFrame(X.toarray(),
\1names())
Out[8]: | evil  horizon  of  problem  queen
0 | 1 | 0 | 1 | 1 | 0
1 | 1 | 0 | 0 | 0 | 1
2 | 0 | 1 | 0 | 1 | 0

\1обратная частотность документа»  (term frequency-
inverse document frequency) или TF-IDF. При нем слова получают вес с учетом

============================================================
СТРАНИЦА 430
============================================================

\1n[9]: from sklearn.feature_extraction.text import TfidfVectorizer

\1nsform(sample)
       pd.DataFrame(X.toarray(),
\1names())
Out[9]: | evil | horizon | of | problem | queen
        0  0.517856  0.000000  0.680919  0.517856  0.000000
        1  0.605349  0.000000  0.000000  0.000000  0.795961
        2  0.000000  0.795961  0.000000  0.605349  0.000000

\1тот, который мы использовали для
набора данных рукописных цифр в разделе «Знакомство с библиотекой Scikit-Learn»
этой главы, — использовать значения интенсивности самих пикселов. Но подобные
подходы могут, в зависимости от прикладной задачи, оказаться неоптимальными.

\1Learn и пакета Scikit-Image
вы можете найти в разделе «Прикладная задача: конвейер распознавания лиц»
данной главы.

\1выведенные математически из каких-либо
входных признаков. Мы уже встречались с ними в разделе «Гиперпараметры и про-
верка модели» этой главы, когда создавали полиномиальные признаки из входных
данных. Мы видели, что можно преобразовать линейную регрессию в полиноми-
альную регрессию не путем изменения модели, а преобразования входных данных!
Этот метод, известный под названием регрессии по комбинации базисных функций
(basis function regression), рассматривается подробнее в разделе «Заглянем глубже:
линейная регрессия» текущей главы.

\1

============================================================
СТРАНИЦА 431
============================================================
Проектирование признаков  431
In[10]: %matplotlib inline
        import numpy as np
        import matplotlib.pyplot as plt

\1np.array([1, 2, 3, 4, 5])

\1np.array([4, 2, 1, 3, 7])
        plt.scatter(x, y);

\1щью функции LinearRegression и получить оптимальный результат (рис. 5.36):
In[11]: from sklearn.linear_model import LinearRegression

\1np.newaxis]

\1nearRegression().fit(X, y)

\1\1n[12]: from sklearn.preprocessing import PolynomialFeatures

\1nomialFeatures(
\1
\1nsform(X)
        print(X2)
[[ | 1. | 1. | 1.]
[ | 2. | 4. | 8.]
[ | 3. | 9. | 27.]
[ | 4. | 16. | 64.]
[ | 5. | 25.  125.]]

\1
In[13]:
\1nearRegression().fit(X2, y)

\1
\1\1
In[14]: from numpy import nan

\1np.array([[ nan, 0, | 3  ],
[ 3, | 7, | 9  ],
[ 3, | 5, | 2  ],
[ 4, | nan, 6  ],
[ 8, | 8, | 1  ]])

\1np.array([14, 16, -1,  8, -5])

\1либо подходящим
значением. Это действие называется заполнением (imputation) пропущенных зна-
чений, и методики его выполнения варьируются от простых (например, замены
пропущенных значений средним значением по столбцу) до сложных (например,
с использованием восстановления матриц (matrix completion) или ошибкоустой-
чивого алгоритма для обработки подобных данных).

\1Learn предоставляет
класс Imputer:
In[15]: from sklearn.preprocessing import Imputer

\1n')

\1nsform(X)
        X2
Out[15]: array([[ 4.5,  0. ,  3. ],
                [ 3. ,  7. ,  9. ],
                [ 3. ,  5. ,  2. ],
                [ 4. ,  5. ,  6. ],
                [ 8. ,  8. ,  1. ]])
В результате мы получили данные, в которых два пропущенные значения заменены
на среднее значение остальных элементов соответствующего столбца. Эти данные
можно передать, например, непосредственно оценивателю LinearRegression:

============================================================
СТРАНИЦА 434
============================================================

\1n[16]:
\1nearRegression().fit(X2, y)
        model.predict(X2)
Out[16]:
array([ 13.14869292,  14.3784627 ,  -1.15539732,  10.96606197,  -5.33782027])
Конвейеры признаков
Во всех предыдущих примерах может быстро надоесть выполнять преобразования
вручную, особенно если нужно связать цепочкой несколько шагов. Например, нам
может понадобиться следующий конвейер обработки.

\1n предоставляет объект конвейера, который можно использовать следующим
образом:
In[17]: from sklearn.pipeline import make_pipeline

\1ne(Imputer(
\1n'),
                              PolynomialFeatures(
\1nearRegression())

\1Learn, и выполняет все заданные шаги для любых входных
данных.
In[18]: model.fit(X, y)  # Вышеприведенный массив X с пропущенными значениями
        print(y)
        print(model.predict(X))
[1416 -1  8 -5]
[ 14.  16.  -1. | 8.  -5.]

\1Learn вы увидите
в следующем разделе, посвященном наивной байесовской классификации, а также
в разделах «Заглянем глубже: линейная регрессия» и «Заглянем глубже: метод
опорных векторов» этой главы.

============================================================

\1

\1модель, с помощью которой можно было бы вы -
числить P(признаков | L i ) для каждой из меток. Подобная модель называется
порождающей моделью (generative model), поскольку определяет гипотетический
случайный процесс генерации данных. Задание порождающей модели для каждой
из меток/категорий — основа обучения подобного байесовского классиф икатора.

\1непростая задача, но мы упро -
стим ее, приняв некоторые упрощающие допущения о виде модели.

============================================================
СТРАНИЦА 436
============================================================

\1n[1]: %matplotlib inline
       import numpy as np
       import matplotlib.pyplot as plt
       import seaborn as sns; sns.set()

\1
In[2]: from sklearn.datasets import make_blobs
       X,
\1
\1
\1n.naive_bayes.GaussianNB:
In[3]: from sklearn.naive_bayes import GaussianNB

\1nNB()
       model.fit(X, y);

\1
In[4]:
\1np.random.RandomState(0)

\1ng.rand(2000, 2)

\1new)
Теперь у нас есть возможность построить график этих новых данных и понять, где
пролегает граница принятия решений (decision boundary) (рис. 5.40):
In[5]: plt.scatter(X[:, 0], X[:, 1],
\1new[:, 0], Xnew[:, 1],
\1new,
\1\1n[6]:
\1new)
       yprob[-8:].round(2)
Out[6]: array([[ 0.89,  0.11],
               [ 1.  ,  0.  ],
               [ 1.  ,  0.  ],
               [ 1.  ,  0.  ],
               [ 1.  ,  0.  ],
               [ 1.  ,  0.  ],
               [ 0.  ,  1.  ],
               [ 0.15,  0.85]])

\1
In[7]: from sklearn.datasets import fetch_20newsgroups

\1newsgroups()
       data.target_names
Out[7]: ['alt.atheism',
         'comp.graphics',
         'comp.os.ms-windows.misc',
         'comp.sys.ibm.pc.hardware',
         'comp.sys.mac.hardware',
         'comp.windows.x',
         'misc.forsale',
         'rec.autos',
         'rec.motorcycles',
         'rec.sport.baseball',
         'rec.sport.hockey',
         'sci.crypt',
         'sci.electronics',
         'sci.med',
         'sci.space',
         'soc.religion.christian',
         'talk.politics.guns',
         'talk.politics.mideast',
         'talk.politics.misc',
         'talk.religion.misc']

============================================================
СТРАНИЦА 440
============================================================

\1n[8]:

\1n.misc', 'soc.religion.christian', 'sci.space',
              'comp.graphics']

\1newsgroups(
\1n',
\1newsgroups(
\1\1
In[9]: print(train.data[5])
From: dmcgee@uluhe.soest.hawaii.edu (Don McGee)
Subject: Federal Hearing
Originator: dmcgee@uluhe
Organization: School of Ocean and Earth Science and Technology
Distribution: usa
Lines: 10
Fact or rumor....?  Madalyn Murray O'Hare an atheist who eliminated the
use of the bible reading and prayer in public schools 15 years ago is now
going to appear before the FCC with a petition to stop the reading of the
Gospel on the airways of America.  And she is also campaigning to remove
Christmas programs, songs, etc from the public schools.  If it is true
then mail to Federal Communications Commission 1919 H Street Washington DC
20054 expressing your opposition to her request.  Reference Petition number

\1n[10]: from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.naive_bayes import MultinomialNB
        from sklearn.pipeline import make_pipeline

\1ne(TfidfVectorizer(), MultinomialNB())

\1
In[11]: model.fit(train.data, train.target)

\1\1

============================================================
СТРАНИЦА 441
============================================================
Заглянем глубже: наивная байесовская классификация  441
In[12]:
from sklearn.metrics import confusion_matrix

\1nfusion_matrix(test.target, labels)
sns.heatmap(mat.T,
\1
\1n.target_names,
\1n.target_names)
plt.xlabel('true label')
plt.ylabel('predicted label');

\1
In[13]: def predict_category(s,
\1n,
\1n train.target_names[pred[0]]

============================================================
СТРАНИЦА 442
============================================================

\1n[14]: predict_category('sending a payload to the ISS')
Out[14]: 'sci.space'
In[15]: predict_category('discussing islam vs atheism')
Out[15]: 'soc.religion.christian'
In[16]: predict_category('determining the screen resolution')
Out[16]: 'comp.graphics'

\1
In[1]: %matplotlib inline
       import matplotlib.pyplot as plt
       import seaborn as sns; sns.set()
       import numpy as np

\1

============================================================
СТРАНИЦА 444
============================================================

\1n[2]:
\1np.random.RandomState(1)

\1ng.rand(50)

\1ng.randn(50)
       plt.scatter(x, y);
Воспользуемся оценивателем LinearRegression из библиотеки Scikit-Learn для
обучения на этих данных и поиска оптимальной прямой (рис. 5.43):
In[3]: from sklearn.linear_model import LinearRegression

\1nearRegression(
\1np.newaxis], y)

\1np.linspace(0, 10, 1000)

\1np.newaxis])
       plt.scatter(x, y)
       plt.plot(xfit, yfit);

\1Learn всегда содержат
в конце знак подчеркивания) включают угловой коэффициент и точку пересе -
чения с осью координат. В данном случае соответствующие параметры — coef_
и intercept_:
In[4]: print("Model slope: | ", model.coef_[0])
       print("Model intercept:", model.intercept_)
Model slope: | 2.02720881036
Model intercept: -4.99857708555

============================================================
СТРАНИЦА 445
============================================================
Заглянем глубже: линейная регрессия  445
Рис. 5.43. Линейная регрессионная модель
Видим, что результаты очень близки к входным данным, как мы и надеялись.
Однако возможности оценивателя LinearRegression намного шире этого: помимо
аппроксимации прямыми линиями, он может также работать с многомерными
линейными моделями следующего вида:

\1\1
In[5]:
\1np.random.RandomState(1)

\1ng.rand(100, 3)

\1np.dot(X, [1.5, -2., 1.])
       model.fit(X, y)
       print(model.intercept_)
       print(model.coef_)
0.5
[ 1.5 -2. | 1. ]
Здесь данные величины y сформированы из трех случайных значений величи ны x ,
а линейная регрессия восстанавливает использовавшие ся для их формирования
коэффициенты.

============================================================
СТРАНИЦА 446
============================================================

\1nearRegression  для
аппроксимации наших данных прямыми, плоскостями и гиперплоскостями. По-
прежнему складывается впечатление, что этот подход ограничивается лишь строго
линейными отношениями между переменными, но оказывается, что ослабление
этого требования также возможно.

\1преобразование данных в соответствии с но-
выми базисными функциями. Один из вариантов этого трюка мы уже встречали
в конвейере PolynomialRegression, который использовался в разделах «Гиперпа-
раметры и проверка модели» и «Проектирование признаков» данной главы. Идея
состоит в том, чтобы взять многомерную линейную модель:

\1
\1n( x ), где f n( x ) — некая функция, выполняющая преобразование
данных.
Например, если f n( x ) = x n, наша модель превращается в полиномиальную регрес -
сию:

\1n никогда не умножаются и не делятся друг
на друга. Фактически мы взяли наши одномерные значения x  и выполнили про -
екцию их на более многомерное пространство, так что с помощью линейной ап -
проксимации мы можем теперь отражать более сложные зависимости между x  и y.

\1Learn в виде преобразователя PolynomialFeatures:
In[6]: from sklearn.preprocessing import PolynomialFeatures

\1np.array([2, 3, 4])

\1nomialFeatures(3,
\1nsform(x[:, None])
Out[6]: array([[  2., | 4., | 8.],
[  3., | 9.,  27.],
               [  4.,  16.,  64.]])

============================================================

\1
In[7]: from sklearn.pipeline import make_pipeline

\1ne(PolynomialFeatures(7), LinearRegression())

\1
In[8]:
\1np.random.RandomState(1)

\1ng.rand(50)

\1np.sin(x) + 0.1 * rng.randn(50)
       poly_model.fit(x[:, np.newaxis], y)

\1np.newaxis])
       plt.scatter(x, y)
       plt.plot(xfit, yfit);

\1ных данных!

============================================================
СТРАНИЦА 448
============================================================

\1n, но мы можем написать для их
создания пользовательский преобразователь, как показано ниже и проиллюстри -
ровано на рис. 5.46 (преобразователи библиотеки Scikit-Learn реализованы как
классы языка Python; чтение исходного кода библиотеки Scikit-Learn — отличный
способ разобраться с их созданием):
In[9]:
from sklearn.base import BaseEstimator, TransformerMixin
class GaussianFeatures(BaseEstimator, TransformerMixin):
    """Равномерно распределенные Гауссовы признаки
       для одномерных входных данных"""
    def __init__(self, N,
\1ne):

============================================================
СТРАНИЦА 449
============================================================
Заглянем глубже: линейная регрессия  449

\1n np.exp(-0.5 * np.sum(arg ** 2, axis))
    def fit(self, X,
\1ne):
        # Создаем N центров, распределенных по всему диапазону данных
        self.
\1np.linspace(X.min(), X.max(), self.N)
        self.
\1nters_[1] - self.centers_[0])
        return self
    def transform(self, X):
        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,
                                 self.width_,
\1ne(GaussianFeatures(20),
                            LinearRegression())
gauss_model.fit(x[:, np.newaxis], y)

\1np.newaxis])
plt.scatter(x, y)
plt.plot(xfit, yfit)
plt.xlim(0, 10);

\1тоже можете его использовать.
Рис. 5.46. Аппроксимация Гауссовыми базисными функциями, вычисленными с помощью
пользовательского преобразователя

============================================================
СТРАНИЦА 450
============================================================

\1n[10]:
\1ne(GaussianFeatures(30),
                              LinearRegression())
        model.fit(x[:, np.newaxis], y)
        plt.scatter(x, y)
        plt.plot(xfit, model.predict(xfit[:, np.newaxis]))
        plt.xlim(0, 10)
        plt.ylim(-1.5, 1.5);

\1
In[11]: def basis_plot(model,
\1ne):
            fig,
\1np.newaxis], y)
            ax[0].scatter(x, y)
            ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))
            ax[0].set(
\1nters_,
                       model.steps[1][1].coef_)
            ax[1].set(
\1n', # Базовое местоположение

\1nt', | # Коэффициент

\1ne(GaussianFeatures(30), LinearRegression())
        basis_plot(model)

\1гребневая регрессия
(ridge regression), или L 2 -регуляризация (L 2 -regularization), также иногда называ -
емая регуляризацией Тихонова (Tikhonov regularization). Она заключается в нало-
жении штрафа на сумму квадратов (евклидовой нормы) коэффициентов модели.

\1

============================================================
СТРАНИЦА 452
============================================================

\1n в виде оценивателя Ridge
(рис. 5.49):
In[12]: from sklearn.linear_model import Ridge

\1ne(GaussianFeatures(30), Ridge(
\1n') # Гребневая регрессия

\1
In[13]: from sklearn.linear_model import Lasso

\1ne(GaussianFeatures(30), Lasso(
\1n') # Лассо-регуляризация

\1ющих Фримонтский мост в Сиэтле велосипедов, основываясь на данных о погоде,
времени года и других факторах. Мы уже работали с этими данными в разделе
«Работа с временными рядами» главы 3.

============================================================
СТРАНИЦА 454
============================================================

\1ndas дает
нам возможность с легкостью соединить эти два источника данных. Мы установим
отношение между погодой и другой информацией с количество м велосипедов ,
выполнив простую линейную регрессию, чтобы оценить, как изменения этих па -
раметров повлияют на число велосипедистов в заданный день.

\1Learn инстру -
ментов в фреймворке статистического моделирования, где предполагается осмыс-
ленность параметров модели. Это отнюдь не стандартный подход для машинного
обучения, но для некоторых моделей подобная трактовка возможна.

\1
In[14]:
import pandas as pd

\1nt_hourly.csv',
\1
\1\1
In[15]:
\1nts.resample('d',
\1\1
In[16]:
\1n', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
        for i in range(7):
            daily[days[i]] = (daily.index.
\1\1
In[17]: from pandas.tseries.holiday import USFederalHolidayCalendar

\1ndar()

\1n(pd.Series(1,
\1
\1na(0,
\1\1

============================================================
СТРАНИЦА 455
============================================================
Заглянем глубже: линейная регрессия  455
In[18]:
        def hours_of_daylight(date,
\1np.tan(np.radians(latitude))
                 * np.tan(np.radians(axis) *
                 np.cos(days * 2 * np.pi / 365.25)))
            return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.
        daily['daylight_hrs'] = list(map(hours_of_daylight, daily.index))
        daily[['daylight_hrs']].plot();

\1
In[19]: # Температуры указаны в десятых долях градуса Цельсия;
        # преобразуем в градусы
        weather['TMIN'] /= 10
        weather['TMAX'] /= 10
        weather['Temp (C)'] = 0.5 * (weather['TMIN'] + weather['TMAX'])
        # Осадки указаны в десятых долях миллиметра; преобразуем в дюймы
        weather['PRCP'] /= 254
        weather['dry day'] = (weather['PRCP'] == 0).astype(int)

\1n(weather[['PRCP', 'Temp (C)', 'dry day']])

============================================================
СТРАНИЦА 456
============================================================

\1n[20]: daily['annual'] = (daily.index - daily.index[0]).days / 365.

\1
In[21]: daily.head()
Out[21]:
           Total  Mon  Tue  Wed  Thu  Fri  Sat  Sun  holiday  daylight_hrs  \\
Date
2012-10-03  3521 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 11.277359
2012-10-04  3475 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 11.219142
2012-10-05  3148 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 11.161038
2012-10-06  2006 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 11.103056
2012-10-07  2142 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 11.045208
PRCP  Temp (C)  dry day | annual
Date
2012-10-03 | 0 | 13.35 | 1  0.000000
2012-10-04 | 0 | 13.60 | 1  0.002740
2012-10-05 | 0 | 15.30 | 1  0.005479
2012-10-06 | 0 | 15.85 | 1  0.008219
2012-10-07 | 0 | 15.85 | 1  0.010959
После этого можно выбрать нужные столбцы и обучить линейную регрессионную
модель на наших данных. Зададим параметр
\1n[22]:

\1n', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'holiday',
                'daylight_hrs', 'PRCP', 'dry day', 'Temp (C)', 'annual']

\1n_names]

\1nearRegression(
\1\1
In[23]: daily[['Total', 'predicted']].plot(
\1\1
In[24]:
\1
\1ns)
        params
Out[24]: Mon | 503.797330
Tue | 612.088879
Wed | 591.611292
Thu | 481.250377
Fri | 176.838999
Sat | -1104.321406
Sun | -1134.610322
holiday | -1187.212688
daylight_hrs | 128.873251
PRCP | -665.185105
dry day | 546.185613
Temp (C) | 65.194390
annual | 27.865349
         dtype:
\1\1n[25]: from sklearn.utils import resample
        np.random.seed(1)

\1np.std([model.fit(*resample(X, y)).coef_
                      for i in range(1000)], 0)

\1
In[26]: print(pd.DataFrame({'effect': params.round(0),
                            'error': err.round(0)}))
              effect  error
Mon | 504 | 85
Tue | 612 | 82
Wed | 592 | 82
Thu | 481 | 85
Fri | 177 | 81
Sat | -1104 | 79
Sun | -1135 | 82
holiday | -1187 | 164
daylight_hrs | 129 | 9
PRCP | -665 | 62
dry day | 546 | 33
Temp (C) | 65 | 4
annual | 28 | 18

\1ратуры на 1 градус Цельсия стимулирует 65 ± 4 челов ек взяться за велосипед; сухой
день означает в среднем на 546 ± 33 больше велосипедистов; каждый дюйм осадков
означает, что на 665 ± 62 больше людей оставляют велосипед дома. После учета всех
влияний мы получаем умеренный рост ежедневного количества велосипедистов на
28 ± 18 человек в год.
Нашей модели почти наверняка недостает определенной относящейся к делу
информации. Например, нелинейные влияния (такие как совместное влияние
осадков и низкой температуры) и нелинейные тренды в пределах каждой из
переменных (такие как нежелание ездить на велосипедах в очень холодную
и очень жаркую погоду) не могут быть учтены в этой модели. Кроме того, мы
отбросили некоторые нюансы (такие как различие между дождливым утром

\1nes, SVMs) — очень мощный
и гибкий класс алгоритмов обучения с учителем как д ля классификации, так
и регрессии. В этом разделе мы научимся интуитивно понимать, как использо -
вать метод опорных векторов в задачах классификации.

\1
In[1]: %matplotlib inline
       import numpy as np
       import matplotlib.pyplot as plt
       from scipy import stats
       # Воспользуемся настройками по умолчанию библиотеки Seaborn
       import seaborn as sns; sns.set()

\1деления меток для новых точек. Это был пример порождающей классификации
(generative classification), здесь же мы рассмотрим разделяющую классификацию
(discriminative classification).

\1
In[2]: from sklearn.datasets.samples_generator import make_blobs
       X,
\1
\1
\1
\1n');

============================================================
СТРАНИЦА 460
============================================================

\1n[3]:
\1np.linspace(-1, 3.5)
       plt.scatter(X[:, 0], X[:, 1],
\1n')
       plt.plot([0.6], [2.1], 'x',
\1n [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:
           plt.plot(xfit, m * xfit + b, '-k')
       plt.xlim(-1, 3.5);

\1деляющих наши выборки. В зависимости от того, какой из них вы выберете, новой
точке данных (например, отмеченной знаком «X» на рис. 5.54) будут присвоены
различные метки! Очевидно, наш интуитивный подход с «проведением прямой
между классами» работает недостаточно хорошо и нужно подойти к вопросу с более
глубоких позиций.
Метод опорных векторов: максимизируем отступ
Метод опорных векторов предоставляет способ решения этой проблемы. Идея
заключается в следующем: вместо того чтобы рисовать между классами прямую
нулевой ширины, можно нарисовать около каждой из прямых отступ (margin)
некоторой ширины, простирающийся до ближайшей точки. Вот пример того, как
подобный подход мог бы выглядеть (рис. 5.55).
Рис. 5.55. Визуализация «отступов» в разделяющих классификаторах
In[4]:

\1np.linspace(-1, 3.5)
plt.scatter(X[:, 0], X[:, 1],
\1n')

============================================================
СТРАНИЦА 462
============================================================

\1n [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:

\1n(xfit, yfit - d, yfit + d,
\1none',

\1n estimator).

\1
In[5]: from sklearn.svm import SVC # "Классификатор на основе метода опорных
                                   # векторов"

\1
\1near',
\1ne,
\1
\1ne,
\1
\1near',

\1
\1ne,
\1\1
In[6]:
     def plot_svc_decision_function(model,
\1ne,
\1ne:

\1np.linspace(xlim[0], xlim[1], 30)

\1np.linspace(ylim[0], ylim[1], 30)
           Y,
\1np.meshgrid(y, x)

\1np.vstack([X.ravel(), Y.ravel()]).T

\1n_function(xy).reshape(X.shape)
           # Рисуем границы принятия решений и отступы
           ax.contour(X, Y, P,
\1
\1
\1none');
           ax.set_xlim(xlim)
           ax.set_ylim(ylim)
In[7]: plt.scatter(X[:, 0], X[:, 1],
\1n')
       plot_svc_decision_function(model);

\1ключевые элементы
аппроксимации, они известны под названием опорных векторов (support vectors),
в их честь алгоритм и получил свое название. В библиотеке Scikit-Learn данные об
этих точках хранятся в атрибуте support_vectors_ классификатора:
In[8]: model.support_vectors_
Out[8]: array([[ 0.44359863,  3.11530945],
               [ 2.33812285,  3.43116792],
               [ 2.06156753,  1.96918596]])

\1ние опорных векторов. Все, находящиеся на правильной стороне, но дальше от

============================================================
СТРАНИЦА 464
============================================================

\1n[9]: def plot_svm(
\1ne):
           X,
\1
\1
\1
\1
\1near',
\1n')
           ax.set_xlim(-1, 4)
           ax.set_ylim(-1, 6)
           plot_svc_decision_function(model, ax)
       fig,
\1n zip(ax, [60, 120]):
           plot_svm(N, axi)
           axi.set_title('
\1n для интерактивного про-
смотра этой возможности модели SVM (рис. 5.58):
In[10]: from ipywidgets import interact, fixed
        interact(plot_svm,
\1ne));

\1приложении (https://github.com/jakevdp/PythonDataScienceHandbook))

\1ядро
Возможности метода SVM особенно расширяются при его комбинации с ядрами
(kernels). Мы уже сталкивались с ними ранее, в регрессии по комбинации базисных
функций из раздела «Заглянем глубже: линейная регрессия» данной главы. Там
мы занимались проекцией данных в пространство с большей размерностью, опре-
деляемое полиномиальными и Гауссовыми базисными фун кциями, и благодаря
этому имели возможность аппроксимировать нелинейные зависимости с помощью
линейного классификатора.

\1
In[11]: from sklearn.datasets.samples_generator import make_circles
        X,
\1
\1
\1near').fit(X, y)
        plt.scatter(X[:, 0], X[:, 1],
\1n')
        plot_svc_decision_function(clf,
\1\1n[12]:
\1np.exp(-(X ** 2).sum(1))

\1
In[13]: from mpl_toolkits import mplot3d
        def plot_3D(
\1
\1n')
            ax.view_init(
\1nteract(plot_3D,
\1nel transformation), основана на отношении подобия (или ядре) между каждой
парой точек.

\1состоит в том, что при росте N она может потребовать колоссальных объ -
емов вычислений. Однако благодаря изящной процедуре, известной под названием
kernel trick  ( https://en.wikipedia.org/wiki/Kernel_method), обучение на преобразованных
с помощью ядра данных можно произвести неявно, то есть даже без построения
полного N-мерного представления ядерной проекции! Этот kernel trick является
частью SVM и одной из причин мощи этого метода.

\1Learn, чтобы применить алгоритм SVM с использованием
ядерного преобразования, достаточно просто заменить линейное ядро на ядро RBF
(radial basis function — «радиальная базисная функция») с помощью гиперпараме -
тра модели kernel (рис. 5.61):
In[14]:
\1
\1\1ne,
\1
\1ne,
\1
\1
\1ne,
\1n[15]: plt.scatter(X[:, 0], X[:, 1],
\1n')
        plot_svc_decision_function(clf)
        plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],

\1none');

\1зоваться kernel trick.

\1
In[16]: X,
\1
\1
\1
\1n');

============================================================

\1тра C оказывает на итоговую аппроксимацию посредством размытия отступа.
Рис. 5.63. Влияние параметра C на аппроксимацию методом опорных векторов
In[17]: X,
\1
\1
\1
\1\1n zip(ax, [10.0, 0.1]):

\1
\1near',
\1n')
            plot_svc_decision_function(model, axi)
            axi.scatter(model.support_vectors_[:, 0],
                        model.support_vectors_[:, 1],

\1none');
            axi.set_title('
\1n the Wild 1  (LFW), состоя -
щим из нескольких тысяч упорядоченных фотографий различных общественных дея-
телей. В библиотеку Scikit-Learn встроена утилита для загрузки этого набора данных:
In[18]: from sklearn.datasets import fetch_lfw_people

\1
\1nt(faces.target_names)
        print(faces.images.shape)
['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'
 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']
(1348, 62, 47)

\1
In[19]: fig,
\1n enumerate(ax.flat):
            axi.imshow(faces.images[i],
\1ne')
            axi.set(
\1names[faces.target[i]])

\1чае мы воспользуемся методом главных компонент (см. раздел «Заглянем глубже:
метод главных компонент» данной главы) для извлечения 150 базовых компонент,

\1n[20]: from sklearn.svm import SVC
        from sklearn.decomposition import RandomizedPCA
        from sklearn.pipeline import make_pipeline

\1ndomizedPCA(
\1
\1
\1
\1nced')

\1ne(pca, svc)
Рис. 5.64. Примеры из набора данных Labeled Faces in the Wild

\1
In[21]: from sklearn.cross_validation import train_test_split
        Xtrain, Xtest, ytrain,
\1n_test_split(faces.data,
                                                        faces.target,

\1\1

============================================================
СТРАНИЦА 472
============================================================

\1n[22]: from sklearn.grid_search import GridSearchCV

\1n, ytrain)
        print(grid.best_params_)
CPU times: user 47.8 s, sys: 4.08 s, total: 51.8 s
Wall time: 26 s
{'svc__gamma': 0.001, 'svc__C': 10}

\1
In[23]:
\1\1
Рис. 5.65. Прогнозируемые имена. Неверные метки выделены полужирным

============================================================
СТРАНИЦА 473
============================================================
Заглянем глубже: метод опорных векторов  473
In[24]: fig,
\1n enumerate(ax.flat):
            axi.imshow(Xtest[i].reshape(62, 47),
\1ne')
            axi.set(
\1names[yfit[i]].split()[-1],

\1ncorrect Labels in Red',
\1\1
In[25]: from sklearn.metrics import classification_report
        print(classification_report(ytest, yfit,

\1names))
precision | recall  f1-score | support
Ariel Sharon | 0.65 | 0.73 | 0.69 | 15
Colin Powell | 0.81 | 0.87 | 0.84 | 68
Donald Rumsfeld | 0.75 | 0.87 | 0.81 | 31
George W Bush | 0.93 | 0.83 | 0.88 | 126
Gerhard Schroeder | 0.86 | 0.78 | 0.82 | 23
Hugo Chavez | 0.93 | 0.70 | 0.80 | 20
Junichiro Koizumi | 0.80 | 1.00 | 0.89 | 12
Tony Blair | 0.83 | 0.93 | 0.88 | 42
avg / total | 0.85 | 0.85 | 0.85 | 337

\1
In[26]: from sklearn.metrics import confusion_matrix

\1nfusion_matrix(ytest, yfit)
        sns.heatmap(mat.T,
\1
\1names,

\1names)
        plt.xlabel('true label')
        plt.ylabel('predicted label');

\1варительно в аккуратные сетки, единственное отличие в схеме классификации лиц
будет состоять в выборе признаков. Необходимо будет использовать более сложный
алгоритм для поиска лиц и извлекать не зависящие от пикселизации признаки. Для
подобных приложений удобно применять библиотеку OpenCV ( http://opencv.org/),
которая, помимо прочего, включает заранее обученные реализации современных
инструментов выделения признаков для изображений вообще и лиц в частности.

============================================================
СТРАНИЦА 474
============================================================

\1ndom forests).

\1пример одного из методов ансамблей (ensemble), основанных
на агрегировании результатов ансамбля более простых оценивателей. Несколько
неожиданный результат использования подобных методов ансамблей — то, что
целое в данном случае оказывается больше суммы составных частей. Результат
«голосования» среди достаточного количества оценивателей может оказаться
лучше результата любого из отдельных участников «голосования»! Мы увидим
примеры этого в следующих разделах. Начнем с обычных импортов:
In[1]: %matplotlib inline
       import numpy as np
       import matplotlib.pyplot as plt
       import seaborn as sns; sns.set()

\1пример обучаемого ансамбля на основе деревьев принятия ре-
шений. Поэтому мы начнем с обсуждения самих деревьев решений.

============================================================
СТРАНИЦА 476
============================================================

\1n[2]: from sklearn.datasets import make_blobs
       X,
\1
\1
\1
\1nbow');

============================================================

\1Learn с помощью оценивателя DecisionTreeClassifier:
In[3]: from sklearn.tree import DecisionTreeClassifier

\1nTreeClassifier().fit(X, y)

\1

============================================================
СТРАНИЦА 478
============================================================

\1n[4]:
       def visualize_classifier(model, X, y,
\1ne,
\1nbow'):

\1n(), y.max()),
\1np.meshgrid(np.linspace(*xlim,
\1np.linspace(*ylim,
\1np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
           # Создаем цветной график с результатами

\1n(np.unique(y))

\1ntourf(xx, yy, Z,
\1np.arange(n_classes + 1) - 0.5,

\1n(), y.max()),

\1\1
In[5]: visualize_classifier(DecisionTreeClassifier(), X, y)

\1приложение ( https://github.com/jakevdp/
PythonDataScienceHandbook) для вызова интерактивной визуализации процесса по -
строения дерева принятия решений (рис. 5.71):
In[6]: # Модуль helpers_05_08 можно найти в онлайн-приложении к книге
       # (https://github.com/jakevdp/PythonDataScienceHandbook)
       import helpers_05_08
       helpers_05_08.plot_tree_interactive(X, y);

\1приложении (https://github.com/jakevdp/PythonDataScienceHandbook)

\1обратиться
к моделям, обученным на различных подмножествах набора данных, например,
на рис. 5.72 показано обучение двух различных деревьев, каждое на половине ис-
ходного набора данных.

============================================================
СТРАНИЦА 480
============================================================

\1n[7]: # Модуль helpers_05_08 можно найти в онлайн-приложении к книге
       # (https://github.com/jakevdp/PythonDataScienceHandbook)
       import helpers_05_08
       helpers_05_08.randomized_tree_interactive(X, y)

\1приложении (https://github.com/jakevdp/PythonDataScienceHandbook)

============================================================

\1та этого переобучения лежит в основе метода ансамблей под названием «баггинг»
(bagging). Баггинг использует ансамбль (например, своеобразную «шляпу фокус-
ника») параллельно работающих переобучаемых оценивателей и усредняет резуль-
таты для получения оптимальной классификации. Ансамбль случайных деревьев
принятия решений называется случайным лесом (random forest).

\1таоценивателя BaggingClassifier  из библиотеки Scikit-Learn, как показано на
рис. 5.74:
In[8]: from sklearn.tree import DecisionTreeClassifier
       from sklearn.ensemble import BaggingClassifier

\1nTreeClassifier()

\1ngClassifier(tree,
\1
\1\1n ( http://scikit-learn.org/stable/modules/ensemble.
html#forest) и упомянутых в ней справочных руководствах.

\1Learn подобный оптимизированный ансамбль случайных дере -
вьев принятия решений, автоматически выполняющий всю рандомизацию, реали -
зован в оценивателе RandomForestClassifier. Все, что остается сделать, — выбрать
количество оценивателей и он очень быстро (при необходимости параллельно)
обучит ансамбль деревьев (рис. 5.75):
In[9]: from sklearn.ensemble import RandomForestClassifier

\1ndomForestClassifier(
\1
\1ndomForestRegressor, синтаксис которого напоминает показанный
выше.

============================================================

\1
In[10]:
\1np.random.RandomState(42)

\1ng.rand(200)
        def model(x,
\1
\1np.sin(5 * x)

\1np.sin(0.5 * x)

\1ng.randn(len(x))
            return slow_oscillation + fast_oscillation + noise

\1\1
In[11]: from sklearn.ensemble import RandomForestRegressor

\1ndomForestRegressor(200)
        forest.fit(x[:, None], y)

\1np.linspace(0, 10, 1000)

\1ne])

\1\1n» этой главы). Воспользуемся ими снова, чтобы посмотреть
на применение классификатора на основе случайных лесов в данном контексте.
In[12]: from sklearn.datasets import load_digits

\1names', 'DESCR', 'images'])

\1
In[13]:
# Настройки рисунка

\1n range(64):

\1nary,
\1nearest')

============================================================

\1
In[14]:
from sklearn.cross_validation import train_test_split
Xtrain, Xtest, ytrain,
\1n_test_split(digits.data, digits.target,

\1ndomForestClassifier(
\1n, ytrain)

\1\1
In[15]: from sklearn import metrics
        print(metrics.classification_report(ypred, ytest))
precision | recall  f1-score | support
0 | 1.00 | 0.97 | 0.99 | 38
1 | 1.00 | 0.98 | 0.99 | 44
2 | 0.95 | 1.00 | 0.98 | 42

============================================================
СТРАНИЦА 486
============================================================

\1n[16]: from sklearn.metrics import confusion_matrix

\1nfusion_matrix(ytest, ypred)
        sns.heatmap(mat.T,
\1
\1nsemble
estimators) и, в частности, модели случайного леса — ансамбля случайных деревьев

============================================================

\1мощный метод, обладающий несколькими
достоинствами.
 Как обучение, так и предсказание выполняются очень быстро в силу простоты
лежащих в основе модели деревьев принятия решений. Кроме того, обе задачи
допускают эффективную параллелизацию, так как отдельные деревья пред -
ставляют собой совершенно независимые сущности.
 Вариант с несколькими деревьями дает возможность использования вероят -
ностной классификации: решение путем «голосования» оценивателей дает
оценку вероятности (в библиотеке Scikit-Learn ее можно получить с помощью
метода predict_proba()).
 Непараметрическая модель исключительно гибка и может э ффективно ра -
ботать с задачами, на которых другие оцениватели оказываются недообучен -
ными.

\1метод главных компонент (principal component
analysis, PCA). PCA представляет собой алгоритм понижения размерности, но он
может быть также удобен в качестве инструмента визуализации, фильтрации шума,
выделения и проектирования признаков, а также многого другого. После краткого
концептуального обзора алгоритма PCA мы рассмотрим несколько примеров при -
кладных задач. Начнем с обычных импортов:
In[1]: %matplotlib inline
       import numpy as np
       import matplotlib.pyplot as plt
       import seaborn as sns; sns.set()

\1быстрый и гибкий метод машинного обучения без
учителя, предназначенный для понижения размерности данных. Мы познакоми-
лись с ним в разделе «Знакомство с библиотекой Scikit-Learn» этой главы. Легче

============================================================
СТРАНИЦА 488
============================================================

\1n[2]:
\1np.random.RandomState(1)

\1np.dot(rng.rand(2, 2), rng.randn(2, 200)).T
       plt.scatter(X[:, 0], X[:, 1])
       plt.axis('equal');

\1сти путем нахождения списка главных осей координат (principal axes) данных и их
использования для описания набора данных. Выполнить это с помощью оценива-
теля PCA из библиотеки Scikit-Learn можно следующим образом:
In[3]: from sklearn.decomposition import PCA

\1
\1
\1
\1ned variance):
In[4]: print(pca.components_)

============================================================

\10.94446029]]
In[5]: print(pca.explained_variance_)
[ 0.75871884  0.01838551]

\1
In[6]: def draw_vector(v0, v1,
\1ne):

\1
\1
\1
\1nnotate('', v1, v0,
\1ngth, vector in zip(pca.explained_variance_, pca.components_):

\1np.sqrt(length)
           draw_vector(pca.mean_, pca.mean_ + v)
       plt.axis('equal');

\1екции точек данных на главные оси и есть главные компоненты данных.

============================================================
СТРАНИЦА 490
============================================================

\1ne transformation). По существу, это значит, что оно
состоит из сдвига (translation), вращения (rotation) и пропорционального масшта -
бирования (uniform scaling).

\1
In[7]:
\1
\1nsform(X)
print("original shape: | ", X.shape)
       print("transformed shape:", X_pca.shape)
original shape: | (200, 2)
transformed shape: (200, 1)

============================================================

\1
In[8]:
\1nverse_transform(X_pca)
       plt.scatter(X[:, 0], X[:, 1],
\1new[:, 0], X_new[:, 1],
\1\1n[9]: from sklearn.datasets import load_digits

\1\1
In[10]:
\1nsform(digits.data)
        print(digits.data.shape)
        print(projected.shape)
(1797, 64)
(1797, 2)

\1
In[11]: plt.scatter(projected[:, 0], projected[:, 1],

\1none',
\1nent 1') # Компонента 1
        plt.ylabel('component 2') # Компонента 2
        plt.colorbar();

\1

\1общий
вклад этих пикселов в структуру изображения. С помощью только восьми из
компонент пиксельного базиса можно сконструировать лишь небольшую часть
64-пиксельного изображения. Продолжив эту последовательность действий и ис-
пользовав все 64 пиксела, мы бы получили исходное изображение.

============================================================
СТРАНИЦА 494
============================================================

\1ned variance ratio) в виде функции от количества компонент (рис. 5.87):
In[12]:
\1np.cumsum(pca.explained_variance_ratio_))
        plt.xlabel('number of components')
        plt.ylabel('cumulative explained variance');

\1
In[13]:
       def plot_digits(data):
            fig,
\1n enumerate(axes.flat):

============================================================
СТРАНИЦА 496
============================================================

\1nary',
\1nearest',

\1\1
In[14]: np.random.seed(42)

\1np.random.normal(digits.data, 4)
        plot_digits(noisy)

\1
In[15]:
\1noisy)
        pca.n_components_
Out[15]: 12

============================================================

\1
In[16]:
\1nsform(noisy)

\1nverse_transform(components)
        plot_digits(filtered)

\1размерном представлении, что автоматически приведет к фильтрации случайного
шума во входных данных.
Пример: метод Eigenfaces

\1зуем набор данных Labeled Faces in the Wild (LFW), доступный через библиотеку
Scikit-Learn:
In[17]: from sklearn.datasets import fetch_lfw_people

\1
\1nt(faces.target_names)
        print(faces.images.shape)
['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'
 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']
(1348, 62, 47)
Выясним, какие главные оси координат охватывают этот набор данных. Поскольку
набор данных велик, воспользуемся классом RandomizedPCA — содержащийся в нем

============================================================
СТРАНИЦА 498
============================================================

\1n[18]: from sklearn.decomposition import RandomizedPCA

\1ndomizedPCA(150)
        pca.fit(faces.data)
Out[18]: RandomizedPCA(
\1
\1
\1ne,
\1nvectors), так что подобные изображения часто
называют «собственными лицами» (eigenfaces)). Как вы видите из рис. 5.91, они
такие же жуткие, как и их название:
In[19]: fig,
\1n enumerate(axes.flat):
            ax.imshow(pca.components_[i].reshape(62, 47),
\1ne')

\1
In[20]: plt.plot(np.cumsum(pca.explained_variance_ratio_))
        plt.xlabel('number of components')
        plt.ylabel('cumulative explained variance');

============================================================

\1
In[21]: # Вычисляем компоненты и проекции лиц

\1ndomizedPCA(150).fit(faces.data)

\1nsform(faces.data)

\1nverse_transform(components)
In[22]: # Рисуем результаты
        fig,
\1n range(10):
            ax[0, i].imshow(faces.data[i].reshape(62, 47),
\1nary_r')
            ax[1, i].imshow(projected[i].reshape(62, 47),
\1nary_r')
        ax[0, 0].set_ylabel('full-dim\ninput')
        # Полноразмерные входные данные
        ax[1, 0].set_ylabel('150-dim\nreconstruction');
        # 150-мерная реконструкция

\1мерная реконструкция данных из LFW с помощью метода
\1\1n содержит несколько интересных вариантов метода
PCA, включая классы RandomizedPCA и SparsePCA, находящиеся в модуле sklearn.
decomposition. RandomizedPCA, который мы уже встречали ранее, использует неде -
терминированный метод для быстрой аппроксимации нескольких первых из глав-
ных компонент данных с очень высокой размерностью, а SparsePCA вводит понятие
регуляризации (см. раздел «Заглянем глубже: линейная регрессия» данной главы),
служащее для обеспечения разреженности компонент.

\1снижения количества признаков набора данных
с сохранением существенных зависимостей между точками. Хотя метод PCA гибок,

============================================================

\1ных под названием обучения на базе многообразий  (manifold learning). Это класс
оценивателей без учителя, нацеленных на описание наборов данных как низкораз-
мерных многообразий, вложенных в пространство большей размерности. Чтобы
понять, что такое многообразие, представьте себе лист бумаги: это двумерный объ -
ект в нашем привычном трехмерном мире, который можно изогнуть или свернуть
в трех измерениях. В терминах обучения на базе многообразий можно считать этот
лист двумерным многообразием, вложенным в трехмерное пространство.

\1ние (multidimensional scaling, MDS), локально линейное вложение  (locally linear
embedding, LLE) и изометрическое отображение  (isometric mapping, Isomap).

\1
In[1]: %matplotlib inline
       import matplotlib.pyplot as plt
       import seaborn as sns; sns.set()
       import numpy as np

\1
In[2]:
def make_hello(
\1nter',
\1nter',
\1ng')
    plt.close(fig)

============================================================
СТРАНИЦА 502
============================================================

\1ng')[::-1, :, 0].T

\1np.random.RandomState(rseed)

\1ng.rand(4 * N, 2)
    i,
\1nt).T

\1n X[np.argsort(X[:, 0])]

\1
In[3]:
\1nbow', 5))
       plt.scatter(X[:, 0], X[:, 1], **colorize)
       plt.axis('equal');

\1

============================================================
СТРАНИЦА 503
============================================================
Заглянем глубже: обучение на базе многообразий  503
In[4]:
       def rotate(X, angle):

\1np.deg2rad(angle)

\1np.cos(theta), np.sin(theta)],
                [-np.sin(theta), np.cos(theta)]]
           return np.dot(X, R)

\1nces из библиотеки
Scikit-Learn, чтобы выполнить этот расчет для наших исходных данных:
In[5]: from sklearn.metrics import pairwise_distances

\1nces(X)
       D.shape
Out[5]: (1000, 1000)

\1
In[6]: plt.imshow(D,
\1
\1nearest')
       plt.colorbar();

============================================================
СТРАНИЦА 504
============================================================

\1n[7]:
\1nces(X2)
       np.allclose(D, D2)
Out[7]: True

\1
In[8]: from sklearn.manifold import MDS

\1
\1
\1nsform(D)
       plt.scatter(out[:, 0], out[:, 1], **colorize)
       plt.axis('equal');

\1
In[9]:
       def random_projection(X,
\1
\1
\1np.random.RandomState(rseed)

\1ng.randn(dimension, dimension)
           e,
\1np.linalg.eigh(np.dot(C, C.T))
           return np.dot(X, V[:X.shape[1]])

\1ndom_projection(X, 3)
       X3.shape
Out[9]: (1000, 3)

\1
In[10]: from mpl_toolkits import mplot3d

\1
\1nit(
\1\1n[11]:
\1
\1
\1nsform(X3)
        plt.scatter(out3[:, 0], out3[:, 1], **colorize)
        plt.axis('equal');

\1
In[12]: def make_hello_s_curve(X):

\1np.pi

\1np.sin(t)

\1np.sign(t) * (np.cos(t) - 1)
            return np.vstack((x, y, z)).T

\1\1
In[13]: from mpl_toolkits import mplot3d

\1
\1\1n[14]: from sklearn.manifold import MDS

\1
\1
\1nsform(XS)
        plt.scatter(outS[:, 0], outS[:, 1], **colorize)
        plt.axis('equal');

\1
In[15]:
from sklearn.manifold import LocallyLinearEmbedding

\1nearEmbedding(
\1
\1
\1nse')

\1nsform(XS)
fig,
\1\1n ( https://github.com/mmp2/
megaman) реализованы методы обучения на базе многообразий, масштабиру-
ющиеся гораздо лучше).

\1Learn реализовано несколько рас пространенных вариантов обу-
чения на базе многообразий и локально линейного вложения: в документации Scikit-
Learn имеется их обсуждение и сравнение ( http://scikit-learn.org/stable/modules/manifold.html).

\1стрируют отличные результаты. Они реализованы в классе sklearn.mani fold.Lo-
callyLinearEmbedding.
 В случае многомерных данных, полученных из реальных источников, метод LLE
часто работает плохо, и изометрическое отображение (Isomap), похоже, выдает
более осмысленные вложения. Оно реализовано в классе sklearn.manifold.Isomap.
 Для сильно кластеризованных данных отличные результаты демонстрирует
метод стохастического вложения соседей на основе распределения Стьюдента
(t-distributed stochastic neighbor embedding), хотя и работает иногда очень медленно
по сравнению с другими методами. Он реализован в классе sklearn.manifold.TSNE.

\1яркость каждого пиксела в каждом изображении соответствует
координате в соответствующем измерении.

\1ся набором данных Labeled Faces in the Wild (LFW), с которым уже сталкивались
в разделах «Заглянем глубже: метод опорных векторов» и «Заглянем глубже: метод

============================================================
СТРАНИЦА 512
============================================================

\1n[16]: from sklearn.datasets import fetch_lfw_people

\1
\1\1
In[17]: fig,
\1n enumerate(ax.flat):
            axi.imshow(faces.images[i],
\1\1
In[18]: from sklearn.decomposition import RandomizedPCA

\1ndomizedPCA(100).fit(faces.data)
        plt.plot(np.cumsum(model.explained_variance_ratio_))
plt.xlabel('n components') | # Количество компонент
        plt.ylabel('cumulative variance'); # Интегральная дисперсия

============================================================

\1
In[19]: from sklearn.manifold import Isomap

\1
\1nsform(faces.data)
        proj.shape
Out[19]: (2370, 2)

\1
In[20]: from matplotlib import offsetbox
        def plot_components(data, model,
\1ne,
\1ne,

\1nsform(data)
            ax.plot(proj[:, 0], proj[:, 1], '.k')
            if images is not None:

============================================================
СТРАНИЦА 514
============================================================

\1
\1n(0))) ** 2

\1np.array([2 * proj.max(0)])
                for i in range(data.shape[0]):

\1np.sum((proj[i] - shown_images) ** 2, 1)
                    if np.min(dist) < min_dist_2:
                        # Не отображаем слишком близко расположенные точки
                        Continue

\1np.vstack([shown_images, proj[i]])

\1nnotationBbox(
                        offsetbox.OffsetImage(images[i],
\1\1
In[21]: fig,
\1nents(faces.data,

\1
\1n:
In[22]: from sklearn.datasets import fetch_mldata

\1nal')
        mnist.data.shape
Out[22]: (70000, 784)

\1
In[23]: fig,
\1n enumerate(ax.flat):
            axi.imshow(mnist.data[1250 * i].reshape(28, 28),
\1\1n[24]:
# используем только 1/30 часть данных:
# вычисления для полного набора данных занимают длительное время!

\1nist.data[::30]

\1nist.target[::30]

\1
\1nsform(data)
plt.scatter(proj[:, 0], proj[:, 1],
\1nge(10))
plt.clim(-0.5, 9.5);

\1
In[25]: from sklearn.manifold import Isomap
        # Выбираем для проекции 1/4 цифр "1"

\1nist.data[mnist.
\1
\1
\1
\1nse')
        plot_components(data, model,
\1\1n и других местах имеется множество алгоритмов класте -
ризации, но, вероятно, наиболее простой для понимания — алгоритм кластеризации
методом k-средних  (k-means clustering), реализованный в классе sklearn.cluster.KMe-
ans. Начнем с обычных импортов:
In[1]: %matplotlib inline
       import matplotlib.pyplot as plt
       import seaborn as sns; sns.set()  # для стилизации графиков
       import numpy as np

\1
In[2]: from sklearn.datasets.samples_generator import make_blobs
       X,
\1
\1
\1
\1n API
статистических оценок:
In[3]: from sklearn.cluster import KMeans

\1ns(
\1ns.fit(X)

\1ns.predict(X)

============================================================

\1
In[4]: plt.scatter(X[:, 0], X[:, 1],
\1ns,
\1
\1ns.cluster_centers_
       plt.scatter(centers[:, 0], centers[:, 1],
\1\1n-maximization, EM).

\1особенно простое
и понятное приложение этого алгоритма, и мы рассмотрим его здесь вкратце. Подход
максимизации математического ожидания состоит из следующей процедуры.

\1n), назван так потому, что включает актуа -
лизацию математического ожидания того, к каким кластерам относятся точки.
M-шаг, или шаг максимизации (maximization), назван так потому, что включа-
ет максимизацию некоторой целевой функции, описывающей расположения
центров кластеров. В таком случае максимизация достигается путем простого
усреднения данных в кластере.

\1приложении ( https://github.com/jakevdp/PythonDataScienceHandbook).

\1
In[5]: from sklearn.metrics import pairwise_distances_argmin
       def find_clusters(X, n_clusters,
\1
\1np.random.RandomState(rseed)

\1ng.permutation(X.shape[0])[:n_clusters]

\1nces_argmin(X, centers)
               # 2b. Находим новые центры, исходя из средних значений точек

\1np.array([X[
\1n(0)
                                       for i in range(n_clusters)])
               # 2c. Проверяем сходимость
               if np.all(
\1new_centers):
                   break

\1new_centers
           return centers, labels
       centers,
\1nd_clusters(X, 4)
       plt.scatter(X[:, 0], X[:, 1],
\1\1n[6]: centers,
\1nd_clusters(X, 4,
\1n (это задается с помощью параметра
n_init, по умолчанию имеющего значение 10).
 Количество кластеров следует выбирать заранее. Еще одна часто встреча-
ющаяся проблема с методом k -средних заключается в том, что ему необходимо
сообщить, какое количество кластеров вы ожидаете: он не умеет вычислять
количество кластеров на основе данных. Например, если предложить алгоритму
выделить шесть кластеров, он с радостью это сделает и найдет шесть оптималь-
ных кластеров (рис. 5.115):

============================================================

\1средних  523
In[7]:
\1ns(6,
\1n.org/stable/
auto_examples/cluster/plot_kmeans_silhouette_analysis.html ).

\1пространения аффинности (affinity propagation), находящиеся в подмодуле
sklearn.cluster ).
 Применение метода k-средних ограничивается случаем линейных границ класте-
ров. Базовое допущение модели k -средних (точки должны быть ближе к центру
их собственного кластера, чем других) означает, что этот алгоритм зачастую
будет неэффективен в случае сложной геометрии кластеров.

\1

============================================================
СТРАНИЦА 524
============================================================

\1n[8]: from sklearn.datasets import make_moons
       X,
\1ns(200,
\1
\1n[9]:
\1ns(2,
\1n в оценивателе SpectralClustering. Она использует граф ближай-
ших соседей для вычисления представления данных более высокой размер -
ности, после чего задает соответствие меток с помощью алгоритма k -средних
(рис. 5.117):
In[10]: from sklearn.cluster import SpectralClustering

\1ng(
\1
\1nearest_neighbors',

\1ns')

\1ng
 Метод k-средних работает довольно медленно в случае большого количества вы-
борок. Алгоритм может работать довольно медленно при росте числа выборок,
ведь при каждой итерации методу k -средних необходимо обращаться к каждой
точке в наборе данных. Интересно, можно ли смягчить это требование относи-
тельно использования всех данных при каждой итерации? Например, можно
применить лишь подмножество данных для корректировки центров кластеров
на каждом шаге. Эта идея лежит в основе пакетных алгоритмов k -средних, один
из которых реализован в классе sklearn.cluster.MiniBatchKMeans. Их интер -
фейс не отличается от обычного KMeans. В дальнейшем мы рассмотрим пример
их использования.

\1средних для распознания
схожих цифр без использования информации об исходных метках. Это напоминает

============================================================
СТРАНИЦА 526
============================================================

\1ns. Напомним, что набор данных по цифрам состоит из 1797 выборок
с 64 признаками, где каждый из признаков представляет собой яркость одного
пиксела в изображении размером 8 × 8:
In[11]: from sklearn.datasets import load_digits

\1\1
In[12]:
\1ns(
\1
\1ns.fit_predict(digits.data)
        kmeans.cluster_centers_.shape
Out[12]: (10, 64)

\1
In[13]: fig,
\1
\1ns.cluster_centers_.reshape(10, 8, 8)
        for axi, center in zip(ax.flat, centers):
            axi.set(
\1nter,
\1nearest',
\1nary)
Рис. 5.118. Центры кластеров
Как видим, алгоритм KMeans даже без меток  способен определить кластеры, чьи
центры представляют собой легко узнаваемые цифры, возможно, за исключением
1 и 8.

============================================================

\1
In[14]: from scipy.stats import mode

\1np.zeros_like(clusters)
        for i in range(10):

\1\1
In[15]: from sklearn.metrics import accuracy_score
        accuracy_score(digits.target, labels)
Out[15]: 0.79354479688369506

\1
In[16]: from sklearn.metrics import confusion_matrix

\1nfusion_matrix(digits.target, labels)
        sns.heatmap(mat.T,
\1
\1names,

\1names)
        plt.xlabel('true label')
        plt.ylabel('predicted label');

\1средних

============================================================
СТРАНИЦА 528
============================================================

\1n[17]: from sklearn.manifold import TSNE
        # Проекция данных: выполнение этого шага займет несколько секунд

\1
\1
\1
\1ne.fit_transform(digits.data)
        # Расчет кластеров

\1ns(
\1
\1ns.fit_predict(digits_proj)
        # Перестановка меток местами

\1np.zeros_like(clusters)
        for i in range(10):

\1n (для работы следующего кода у вас должен быть
установлен пакет pillow языка Python):

============================================================

\1средних  529
In[18]: # Обратите внимание: для работы этого кода
        # должен быть установлен пакет pillow
        from sklearn.datasets import load_sample_image

\1na.jpg")

\1na);

\1
In[19]: china.shape
Out[19]: (427, 640, 3)

\1вом пространстве. Изменим форму данных на [n_samples × n_features] и масшта-
бируем шкалу цветов так, чтобы они располагались между 0 и 1:
In[20]:
\1na / 255.0 # используем шкалу 0...1

\1\1
In[21]:
       def plot_pixels(data, title,
\1ne,
\1ne:

\1
\1np.random.RandomState(0)

============================================================
СТРАНИЦА 530
============================================================

\1ng.permutation(data.shape[0])[:N]

\1n',
\1n[22]: plot_pixels(data,
\1nput color space: 16 million possible
                    colors') # Исходное цветовое пространство: 16 миллионов
                             # возможных цветов

\1
In[23]: from sklearn.cluster import MiniBatchKMeans

\1niBatchKMeans(16)
        kmeans.fit(data)

\1ns.cluster_centers_[kmeans.predict(data)]
        plot_pixels(data, colors=new_colors,

\1n[24]:

\1new_colors.reshape(china.shape)
fig,
\1na)
ax[0].set_title('Original Image',
\1na_recolored)
ax[1].set_title('16-color Image',
\1\1n[1]: %matplotlib inline
       import matplotlib.pyplot as plt
       import seaborn as sns; sns.set()
       import numpy as np

\1
In[2]: # Генерируем данные
       from sklearn.datasets.samples_generator import make_blobs
       X,
\1
\1
\1
\1n[3]: # Выводим данные на график с полученными методом k-средних метками
       from sklearn.cluster import KMeans

\1ns(4,
\1ns.fit(X).predict(X)
       plt.scatter(X[:, 0], X[:, 1],
\1\1
In[4]:
from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist
def plot_kmeans(kmeans, X,
\1ne):

\1ns.fit_predict(X)
    # Выводим на рисунок входные данные

\1
\1ns.cluster_centers_

\1nter]).max()

============================================================
СТРАНИЦА 534
============================================================

\1nter in enumerate(centers)]
    for c, r in zip(centers, radii):
        ax.add_patch(plt.Circle(c, r,
\1n[5]:
\1ns(
\1
\1ns(kmeans, X)

\1средних в случае кластеров некруглой формы

============================================================
СТРАНИЦА 535
============================================================
Заглянем глубже: смеси Гауссовых распределений  535
In[6]:
\1np.random.RandomState(13)

\1np.dot(X, rng.randn(2, 2))

\1ns(
\1
\1ns(kmeans, X_stretched)

\1смеси Гауссовых распределений.

\1модели: смеси Гауссовых
распределений
Смесь Гауссовых распределений (gaussian mixture model, GMM) нацелена на поиск
многомерных Гауссовых распределений вероятностей, моделирующих наилучшим
возможным образом любой исходный набор данных.

\1
In[7]: from sklearn.mixture import GMM

\1
\1\1n это можно сделать методом predict_proba. Он возвраща-
ет матрицу размера [n_samples, n_clusters], содержащую оценки вероятностей
принадлежности точки к конкретному кластеру:
In[8]:
\1nt(probs[:5].round(3))
[[ 0. | 0. | 0.475  0.525]
[ 0. | 1. | 0. | 0. | ]
[ 0. | 1. | 0. | 0. | ]
[ 0. | 0. | 0. | 1. | ]
[ 0. | 1. | 0. | 0. | ]]

\1
In[9]:
\1\1n[10]:
from matplotlib.patches import Ellipse
def draw_ellipse(position, covariance,
\1ne, **kwargs):
    """Рисует эллипс с заданными расположением и ковариацией"""

\1nce.
\1np.linalg.svd(covariance)

\1np.degrees(np.arctan2(U[1, 0], U[0, 0]))
        width,
\1np.sqrt(s)
    else:

\1np.sqrt(covariance)

============================================================
СТРАНИЦА 538
============================================================

\1nsig in range(1, 4):
        ax.add_patch(Ellipse(position, nsig * width, nsig * height,
                             angle, **kwargs))
def plot_gmm(gmm, X,
\1ne):

\1n zip(gmm.means_, gmm.covars_, gmm.weights_):
        draw_ellipse(pos, covar,
\1\1
In[11]:
\1
\1
\1\1
In[12]:
\1
\1
\1
\1nce_
type. Этот гиперпараметр управляет степенями свободы форм кластеров. Оче нь
важно для любой задачи задавать его значения аккуратно. Значение его по
умолчанию —
\1nce_ty
\1
\1\1ns библиотеки Scikit-Learn (показанные на рис. 5.132), которые мы уже рас -
сматривали в разделе «Заглянем глубже: кластеризация методом k-средних» этой
главы:
In[13]: from sklearn.datasets import make_moons
        Xmoon,
\1ns(200,
\1
\1n[:, 0], Xmoon[:, 1]);

\1
In[14]:
\1
\1
\1
\1n)

\1
In[15]:
\1
\1
\1
\1n,
\1\1n[16]:
\1
\1new[:, 0], Xnew[:, 1]);

\1

\1порождающая модель, у нас появляется естественная
возможность определения оптимального количества компонент для заданного
набора данных. Порождающая модель, по существу, представляет собой распреде -
ление вероятности для набора данных, поэтому можно легко вычислить функцию
правдоподобия  (likelihood function) для лежащих в ее основе данных, используя
перекрестную проверку во избежание переобучения. Др угой способ введения по -
правки на переобучение — подстройка функции правдоподобия модели с помощью
некоторого аналитического критерия, например информационного критерия Акаике
(Akaike information criterion, AIC, см.: https://ru.wikipedia.org/wiki/ Информационный_кри-
терий_Акаи ке) или байесовского информационного критерия (bayesian
\1n, BIC, см.: https://ru.wikipedia.org/wiki/Информационный_критерий). Оцениватель
GMM библиотеки Scikit-Learn включает встроенные методы для вычисления этих
критериев, что сильно упрощает указанный подход.
Посмотрим на критерии AIC и BIC как функции от количества компонент GMM
для нашего набора данных moon (рис. 5.136):
Рис. 5.136. Визуализация AIC и BIC с целью выбора количества компонент GMM
In[17]:
\1np.arange(1, 21)

\1n,
\1
\1n)
                  for n in n_components]
        plt.plot(n_components, [m.bic(Xmoon) for m in models],
\1n_components, [m.aic(Xmoon) for m in models],
\1nd(
\1n_components');

\1то, которое минимизирует AIC или BIC,
в зависимости от требуемой аппроксимации. Согласно AIC, наших 16 компонент,
вероятно, слишком много, лучше взять 8–12. Как это обычно бывает в подобных
задачах, критерий BIC говорит в пользу более простой модели.

\1ризации только заведомо простых наборов данных.

============================================================
СТРАНИЦА 544
============================================================

\1n:
In[18]: from sklearn.datasets import load_digits

\1\1
In[19]: def plot_digits(data):
            fig,
\1n enumerate(ax.flat):

\1nary')
                im.set_clim(0, 16)
        plot_digits(digits.data)

\1
In[20]: from sklearn.decomposition import PCA

\1
\1nsform(digits.data)
        data.shape
Out[20]: (1797, 41)

\1
In[21]:
\1np.arange(50, 210, 10)

\1n,
\1
\1n in n_components]

\1n models]
        plt.plot(n_components, aics);
Рис. 5.137. Исходные рукописные цифры
Рис. 5.138. Кривая AIC для выбора подходящего количества компонент
\1\1n[22]:
\1
\1
\1nt(gmm.converged_)
True

\1
In[23]:
\1
\1new.shape
Out[23]: (100, 41)

\1
In[24]:
\1nverse_transform(data_new)
        plot_digits(digits_new)

\1
In[1]: %matplotlib inline
       import matplotlib.pyplot as plt
       import seaborn as sns; sns.set()
       import numpy as np

\1
In[2]:
        def make_data(N,
\1
\1np.random.RandomState(rseed)

\1nd.randn(N)
           x[int(f * N):] += 5

============================================================
СТРАНИЦА 548
============================================================

\1n x

\1normed гистограммы, мы получим норма -
лизованную гистограмму, в которой высота интервалов отражает не число точек,
а плотность вероятности (рис. 5.140):
In[3]:
\1
\1
\1\1
In[4]: density, bins,
\1ns[1:] - bins[:-1]
       (density * widths).sum()
Out[4]: 1.0

\1
In[5]:
\1
\1np.linspace(-5, 10, 10)

============================================================
СТРАНИЦА 549
============================================================
Заглянем глубже: ядерная оценка плотности распределения  549
In[6]: fig,
\1n enumerate([0.0, 0.6]):
           ax[i].hist(x,
\1ns + offset,
\1np.full_like(x, -0.01), '|k',

\1\1
In[7]: fig,
\1
\1np.arange(-3, 8)
       ax.plot(x, np.full_like(x, -0.1), '|k',

\1nt, edge in zip(*np.histogram(x, bins)):
           for i in range(count):
               ax.add_patch(plt.Rectangle((edge, i), 1, 1,

\1\1n[8]:
\1np.linspace(-4, 8, 2000)

\1n x)
       plt.fill_between(x_d, density,
\1np.full_like(x, -0.1), '|k',
\1\1
In[9]: from scipy.stats import norm

\1np.linspace(-4, 8, 1000)

\1norm(xi).pdf(x_d) for xi in x)

============================================================
СТРАНИЦА 551
============================================================
Заглянем глубже: ядерная оценка плотности распределения  551
       plt.fill_between(x_d, density,
\1np.full_like(x, -0.1), '|k',
\1\1nel), определяющее форму распределения в каждой точке, и ширина ядра (kernel
bandwidth), определяющая размер ядра в каждой точке. На практике для ядерной
оценки плотности распределения существует множество различных ядер: в частности,
реализация KDE библиотеки Scikit-Learn поддерживает использование одного из
шести ядер, о которых вы можете прочитать в посвященной оцениванию плотности
документации библиотеки Scikit-Learn ( http://scikit-learn.org/stable/modules/density.html).
Хотя в языке Python реализовано несколько вариантов ядерной оценки плотности
(особенно в пакетах SciPy и StatsModels), я предпочитаю использовать вариант из
Scikit-Learn по причине гибкости и эффективности. Он реализован в оценивателе
sklearn.neighbors.KernelDensity, умеющем работать с KDE в многомерном про-
странстве с одним из шести ядер и одной из нескольких дюжин метрик. В силу
того что метод KDE может потребовать значительных вычислительных затрат,
этот оцениватель использует «под капотом» алгоритм на основе деревьев и умеет
достигать компромисса между временем вычислений и точностью с помощью
параметров atol (absolute tolerance, допустимая абсолютная погрешность) и rtol
(relative tolerance, допустимая относительная погрешность). Определить ширину
ядра — свободный параметр — можно стандартными инструментами перекрестной
проверки библиотеки Scikit-Learn.
Рассмотрим простой пример воспроизведения предыдущего графика с помощью
оценивателя KernelDensity библиотеки Scikit-Learn (рис. 5.145):
In[10]: from sklearn.neighbors import KernelDensity
        # Создание экземпляра модели KDE и ее обучение

\1nelDensity(
\1
\1n')
        kde.fit(x[:, None])
        # score_samples возвращает логарифм плотности
        # распределения вероятности

\1ne])
        plt.fill_between(x_d, np.exp(logprob),
\1np.full_like(x, -0.01), '|k',
\1n

\1параметров зачастую производится эмпирически посредством перекрестной
проверки. Учитывая это, оцениватель KernelDensity из библиотеки Scikit-Learn
спроектирован в расчете на непосредственное использование его в стандартных
инструментах Scikit-Learn для поиска по сетке. В данном случае мы восполь -
зуемся классом GridSearchCV , чтобы выбрать оптимальную ширину ядра для
предыдущего набора данных. Поскольку наш набор данных очень невелик,
мы будем использовать перекрестную проверку по отдельным объектам, при

============================================================
СТРАНИЦА 554
============================================================

\1n[11]: from sklearn.grid_search import GridSearchCV
        from sklearn.cross_validation import LeaveOneOut

\1np.linspace(-1, 1, 100)

\1nelDensity(
\1n'),
                            {'bandwidth': bandwidths},

\1neOut(len(x)))
        grid.fit(x[:, None]);

\1
In[12]: grid.best_params_
Out[12]: {'bandwidth': 1.1233240329780276}

\1вали выше в примере, где ширина была равно 1.0 (это ширина ядра по умолчанию
объекта scipy.stats.norm).

\1пределений точек. Например, KDE встроен в библиотеку визуализации Seaborn
(которую мы обсуждали в разделе «Визуализация с пом ощью пакета Seaborn»
главы 4) и применяется там автоматически для визуализации точек в одномерном
и двумерном пространствах.

\1Learn:
географическое распределение зафиксированных наблюдений особей двух юж -
ноамериканских млекопитающих — Bradypus variegatus  (бурогорлый ленивец)
и Microryzomys minutus  (малая лесная рисовая крыса).

\1Learn следующим образом 1 :
In[13]: from sklearn.datasets import fetch_species_distributions

\1n, но
соответствующие изменения планируются к внесению в самое ближайшее время
и должны быть доступны на момент выхода данной книги.

============================================================
СТРАНИЦА 555
============================================================
Заглянем глубже: ядерная оценка плотности распределения  555

\1ns()
        # Получаем матрицы/массивы идентификаторов и местоположений животных

\1np.vstack([data.train['dd lat'],
                            data.train['dd long']]).T

\1np.array([d.decode('ascii').startswith('micro')
                            for d in data.train['species']],
\1nt')

\1
In[14]: from mpl_toolkits.basemap import Basemap
        from sklearn.datasets.species_distributions import construct_grids
        xgrid,
\1nstruct_grids(data)
        # Рисуем береговые линии с помощью Basemap

\1
\1
\1
\1n(),
\1
\1n(),
\1ndary(
\1ntinents(
\1nes(
\1ntries(
\1n[:, 1], latlon[:, 0],
\1nbow',
\1\1ne, подходящей
для адекватного отображения расстояний на криволинейной поверхности.

\1
In[15]:
# Настраиваем сетку данных для контурного графика
X,
\1np.meshgrid(xgrid[::5], ygrid[::5][::-1])

\1
\1
\1np.vstack([Y.ravel(), X.ravel()]).T
\1np.radians(xy[land_mask])
# Создаем два графика друг возле друга
fig,
\1
\1nutus']

\1n enumerate(ax):
    axi.set_title(species_names[i])
    # Рисуем береговые линии с помощью Basemap

\1
\1
\1n(),

\1
\1n(),

\1
\1ndary(
\1nes()
    m.drawcountries()
    # Формируем сферическую ядерную оценку плотности распределения

\1nelDensity(
\1ne')
    kde.fit(np.radians(latlon[
\1np.full(land_mask.shape[0], -9999.0)
    Z[land_mask] = np.exp(kde.score_samples(xy))

\1np.linspace(0, Z.max(), 25)
    axi.contourf(X, Y, Z,
\1n.

\1это просто выровненная по осям координат Гауссова функция. Алгоритм
оценки плотности, например KDE, позволяет убрать «наивную» составляющую
и произвести ту же самую классификацию с более сложными порождающими
моделями для каждого из классов. Эта классификация остается байесовской, но
уже не будет «наивной».
Общая методика порождающей классификации такова.

\1n так, чтобы воспользо-
ваться поиском по сетке и перекрестной проверкой.

\1Learn, мы после-
довательно рассмотрим его блок за блоком:
In[16]: from sklearn.base import BaseEstimator, ClassifierMixin
        class KDEClassifier(BaseEstimator, ClassifierMixin):
            """Байесовская порождающая классификация на основе метода KDE
            Параметры
            ----------
            bandwidth : float
                Ширина ядра в каждом классе
            kernel : str
                Название ядра, передаваемое функции KernelDensity
            """
            def __init__(self,
\1
\1n'):
                self.
\1ndwidth
                self.
\1nel
            def fit(self, X, y):
                self.
\1np.sort(np.unique(y))

\1n self.classes_]
                self.
\1nelDensity(
\1ndwidth,

\1nel).fit(Xi)
                                for Xi in training_sets]
                self.
\1np.log(Xi.shape[0] / X.shape[0])
                                   for Xi in training_sets]
                return self
            def predict_proba(self, X):

\1np.array([model.score_samples(X)
                                     for model in self.models_]).T

\1np.exp(logprobs + self.logpriors_)
                return result / result.sum(1,
\1n self.classes_[np.argmax(self.predict_proba(X), 1)]

============================================================

\1
from sklearn.base import BaseEstimator, ClassifierMixin
class KDEClassifier(BaseEstimator, ClassifierMixin):
    """Байесовская порождающая классификация на основе метода KDE
    Параметры
    ----------
    bandwidth : float
        Ширина ядра в каждом классе
    kernel : str
        Название ядра, передаваемое функции KernelDensity
    """

\1Learn представляет собой класс, насле -
дующий класс BaseEstimator, а также соответствующую примесь (mixin), кото -
рые обеспечивают стандартную функциональность. Например, помимо прочего,
класс BaseEstimator включает логику, необходимую для клонирования/копиро -
вания оценивателя, чтобы использовать его в процедуре перекрестной проверки,
а ClassifierMixin определяет используемый по умолчанию метод score(). Мы так-
же задали docstring, который будет собран справочной системой языка Python
(см. раздел «Справка и документация в оболочке Python» главы 1).

\1
    def __init__(self,
\1
\1n'):
        self.
\1ndwidth
        self.
\1nel

\1Learn важно, чтобы в методе
инициализации не содержалось  никаких  команд, кроме  присваивания объекту
self переданных значений по имени. Причина в том, что содержащаяся в классе
BaseEstimator логика необходима для клонирования и модификации оценивателей
для перекрестной проверки, поиска по сетке и других целей. Аналогично все аргу-
менты метода __init__ должны быть объявлены явным образом, то есть следует
избегать аргументов *args или **kwargs, так как они не могут быть корректно об-
работаны внутри процедур перекрестной проверки.

\1
        self.
\1np.sort(np.unique(y))

\1n self.classes_]
        self.
\1nelDensity(
\1ndwidth,

\1nel).fit(Xi)
                        for Xi in training_sets]

============================================================
СТРАНИЦА 560
============================================================

\1np.log(Xi.shape[0] / X.shape[0])
                           for Xi in training_sets]
        return self
В нем мы находим в обучающих данных уникальные классы, обучаем модель
KernelDensity для всех классов и вычисляем априорные вероятности на основе
количеств исходных выборок. Наконец, метод fit() должен всегда возвращать
объект self, чтобы можно было связывать команды в цепочку. Например:

\1n, чтобы можно было быстро про -
смотреть список членов оценивателя (с помощью TAB-автодополнения оболочки
IPython) и выяснить, какие именно члены были обучены на обучающих данных.

\1

\1np.vstack([model.score_samples(X)
                              for model in self.models_]).T

\1np.exp(logprobs + self.logpriors_)
        return result / result.sum(1,
\1n self.classes_[np.argmax(self.predict_proba(X), 1)]

\1ализовали метод predict_proba() , возвращающий массив формы [n_samples,
n_classes] вероятностей классов. Элемент [i, j]  этого массива представляет
собой апостериорную вероятность того, что выборка i — член класса j, вычис -
ленная путем умножения функции правдоподобия на априорную вероятность
и нормализации.

\1
In[17]: from sklearn.datasets import load_digits
        from sklearn.grid_search import GridSearchCV

\1
\1np.linspace(0, 2, 100)

\1ndwidth': bandwidths})
        grid.fit(digits.data, digits.target)

\1n_validation_score for val in grid.grid_scores_]

\1
In[18]: plt.semilogx(bandwidths, scores)
plt.xlabel('bandwidth') | # Ширина ядра
plt.ylabel('accuracy') | # Точность
        plt.title('KDE Model Performance') # Эффективность модели KDE
        print(grid.best_params_)
        print('
\1ndwidth': 7.0548023107186433}

\1\1
In[19]: from sklearn.naive_bayes import GaussianNB
        from sklearn.cross_validation import cross_val_score
        cross_val_score(GaussianNB(), digits.data, digits.target).mean()
Out[19]: 0.81860038035501381
Рис. 5.148. Кривая проверки для основанного на KDE байесовского классификатора

============================================================
СТРАНИЦА 562
============================================================

\1nelDensity, а не общую оценку точности пред -
сказания.

\1жать данные в таком виде, который сложно преобразовать в аккуратную матрицу
[n_samples, n_features]. Вам придется, прежде чем воспользоваться любым из
изложенных здесь методов, сначала извлечь эти признаки из данных. Не существу-
ет готового единого шаблона, подходящего для всех предметных областей. В этом
вопросе вам как исследователю данных придется использовать ваши собственные
интуицию и накопленный опыт.

\1анализ изобра -
жений, и мы уже видели несколько примеров его с использованием пиксельных
признаков для классификации. На практике данные редко оказываются настолько
однородными, и простых пикселов будет недостаточно. Это привело к появле -
нию обширной литературы, посвященной методам выделения признаков  (feature
extraction) для изображений (см. раздел «Проектирование признаков» данной
главы).
В этом разделе мы рассмотрим одну из подобных методик выделения признаков,
гистограмму направленных градиентов (histogram of oriented gradients, HOG,
см. https://ru.wikipedia.org/wiki/Гистограмма_направленных_градиентов), которая преобра -
зует пикселы изображения в векторное представление, чувствительное к несущим

============================================================

\1
In[1]: %matplotlib inline
       import matplotlib.pyplot as plt
       import seaborn as sns; sns.set()
       import numpy as np

\1простая процедура выделения призна-
ков, разработанная для идентификации пешеходов на изображениях. Метод HOG
включает следующие этапы.

\1n[2]: from skimage import data, color, feature
       import skimage.data

\1nput image')

\1nda install scikit-
\1\1n of HOG features');

\1Learn. Мы воспользу-
емся линейным методом опорных векторов (см. раздел «Заглянем глубже: метод
опорных векторов» данной главы). Алгоритм включает следующие шаги.

\1n the Wild (LFW), который
можно скачать с помощью библиотеки Scikit-Learn:
In[3]: from sklearn.datasets import fetch_lfw_people

\1\1n:
In[4]: from skimage import data, transform

\1ns', 'moon',
                      'page', 'clock', 'immunohistochemistry',
                      'chelsea', 'coffee', 'hubble_deep_field']

\1name)())
                 for name in imgs_to_use]
In[5]:
from sklearn.feature_extraction.image import PatchExtractor
def extract_patches(img, N,
\1\
    tuple((scale * np.array(patch_size)).astype(int))

\1
\1nsform(img[np.newaxis])
    if
\1np.array([transform.resize(patch, patch_size)
                            for patch in patches])
    return patches

\1np.vstack([extract_patches(im, 1000, scale)
                       for im in images for scale in [0.5, 1.0, 2.0]])
negative_patches.shape
Out[5]: (30000, 62, 47)

\1
In[6]: fig,
\1n enumerate(ax.flat):
           axi.imshow(negative_patches[500 * i],
\1\1n[7]: from itertools import chain

\1np.array([feature.hog(im)
                           for im in chain(positive_patches,
                                           negative_patches)])

\1np.zeros(X_train.shape[0])
       y_train[:positive_patches.shape[0]] = 1
In[8]: X_train.shape
Out[8]: (43233, 1215)

\1Learn виде!

\1nearSVC, поскольку он обычно
лучше масштабируется при росте числа выборок по сравнению с SVC.

============================================================

\1
In[9]: from sklearn.naive_bayes import GaussianNB
       from sklearn.cross_validation import cross_val_score
       cross_val_score(GaussianNB(), X_train, y_train)
Out[9]: array([ 0.94 08785 ,  0.87 52342 ,  0.939 76823])

\1
In[10]: from sklearn.svm import LinearSVC
        from sklearn.grid_search import GridSearchCV

\1nearSVC(), {'C': [1.0, 2.0, 4.0, 8.0]})
        grid.fit(X_train, y_train)
        grid.best_score_
Out[10]: 0.98 66768 44077 44083
In[11]: grid.best_params_
Out[11]: {'C': 4.0}

\1
In[12]:
\1n, y_train)
Out[12]: LinearSVC(
\1ne,
\1
\1
\1nge',
\1
\1
\1ne,
\1\1n[13]:
\1naut()

\1nsform.rescale(test_image, 0.5)

\1\1n[14]: def sliding_window(img,
\1nt(scale * s) for s in patch_size)
            for i in range(0, img.shape[0] - Ni, istep):
                for j in range(0, img.shape[1] - Ni, jstep):

\1nsform.resize(patch, patch_size)
                    yield (i, j), patch
        indices,
\1ng_window(test_image))

\1np.array([feature.hog(patch) for patch in patches])
        patches_hog.shape
Out[14]: (1911, 1215)

\1
In[15]:
\1\1
In[16]: fig,
\1
\1np.array(indices)
        for i, j in indices[
\1ngle((j, i), Nj, Ni,
\1none'))

\1бражении лицо! Отличный результат для всего нескольких строк кода на языке
Python.

\1жеству ложных обнаружений лиц в других областях изображения.

============================================================
СТРАНИЦА 570
============================================================

\1negative mining . При подходе hard negative mining, берется новый,
еще не виденный классификатором набор изображений и все фрагменты в нем,
соответствующие ложноположительным результатам, явным образом добавля-
ются в качестве отрицательных примеров в обучающую последовательность до
повторного обучения классификатора.
 Текущий конвейер выполняет поиск только при одном з начении масштаба.
В текущем виде наш алгоритм будет распознавать только те лица, чей размер
примерно равен 62 × 47 пикселов. Эту проблему можно решить довольно просто
путем применения скользящих окон различных размеров и изменения размера
каждого из фрагментов с помощью функции skimage.transform.resize до по-
дачи его на вход модели. На самом деле используемая здесь вспомогательная
функция sliding_window() уже учитывает этот нюанс.
 Желательно комбинировать перекрывающиеся фрагменты, на которых обнару-
жены лица. В случае готового к промышленной эксплуатации конвейера полу-
чение 30 обнаружений одного и того же лица представляется нежелательным.

\1кластеризация путем сдвига среднего значе-
ния (meanshift clustering)) или посредством процедурного подхода, например
алгоритма подавления немаксимумов (nonmaximum suppression), часто исполь-
зуемого в сфере машинного зрения.
 Конвейер должен быть более продвинутым. После решение вышеописанных
проблем неплохо было бы создать более продвинутый конвейер, который бы
получал на входе обучающие изображения и выдавал предсказания на основе
скользящих окон. Именно в этом вопросе язык Python как инструмент науки
о данных демонстрирует все свои возможности: приложив немного труда, мы
сможем скомпоновать наш предварительный код с качественно спроектиро -
ванным объектно-ориентированным API, обеспечивающим для пользователя
легкость в использовании. Оставлю это в качестве упражнения читателю.
 Желательно обдумать возможность применения более современных средств
предварительной обработки, таких как глубокое обучение. Наконец, мне хоте-
лось бы добавить, что HOG и другие процедурные методы выделения призна-
ков для изображений более не считаются современными. Вместо них многие
современные конвейеры обнаружения объектов используют различные вари -
анты глубоких нейронных сетей. Нейронные сети можно рассматривать как
оцениватель, определяющий оптимальную стратегию выделения признаков
на основе самих данных, а не полагающийся на интуицию пользователя. Зна-
комство с методами глубоких нейронных сетей выходит за рамки этого раздела
концептуально (и вычислительно!), хотя некоторые инструменты с открытым

============================================================
СТРАНИЦА 571
============================================================
Дополнительные источники информации по машинному обучению  571
исходным кодом, такие как TensorFlow ( https://www.tensorflow.org/), выпущенный
корпорацией Google, сделали в последнее время подход глубокого обучения
значительно более доступным. На момент написания книги глубокое обучение
в языке Python остается еще довольно «незрелой» концепцией, поэтому я не
могу рекомендовать вам какие-либо авторитетные источники информации по
этому вопросу. Тем не менее список литературы в следующем разделе покажет
вам, с чего можно начать.
Дополнительные источники информации
по машинному обучению
В этой главе мы кратко рассмотрели машинное обучение в языке Python, в ос -
новном используя инструменты из библиотеки Scikit-Learn. Как бы объемна ни
была эта глава, в ней все равно невозможно было охватить многие интересные
и важные алгоритмы, подходы и вопросы. Я хотел бы предложить тем, кто желает
узнать больше о машинном обучении, некоторые дополнительные источники ин-
формации.
Машинное обучение в языке Python

\1Learn. На сайте библиотеки Scikit-Learn содержатся
поразительные объемы документации и примеров, охватывающие не только
некоторые из рассмотренных в книге моделей, но и многое другое. Если вам
необходим краткий обзор наиболее важных и часто используемых алгоритмов
машинного обучения, этот сайт будет для вас отличной отправной точкой.
 Обучающие видео с таких конференций, как SciPy, PyCon и PyD ata. Библиотека
Scikit-Learn и другие вопросы машинного обучения — неизменные фавориты
учебных пособий ежегодных конференций, посвященных языку Python, в част-
ности PyCon, SciPy и PyData. Найти наиболее свежие материалы можно путем
поиска в Интернете.
 Книга Introduction to Machine Learning with Python ( «Введение в машинное обу чение
с помощью Python», http://shop.oreilly.com/product/063 69200 30515.do1 ) . Написанная

\1Learn на все 100 %, эта книга — отличный источник информа -
ции, написанный одним из разработчиков команды Scikit-Learn.
1 http://www.williamspublishing.com/Books/978-5-99 08910-8-1.html.

============================================================
СТРАНИЦА 572
============================================================

\1n Machine Learning ( «Python и машинное обучение», https://www.packt-
pub.com/big-data-and-business-intelligence/python-machine-learning1 ).  Книга Себастьяна

\1Learn, и в большей — на диапазоне имеющихся в языке  Python инструментов
машинного обучения. В ней приведено очень полезное обсуждение масштабиро-
вания основанных на языке Python подходов машинного обучения на большие
и сложные наборы данных.
Машинное обучение в целом
Машинное обучение не ограничивается только миром языка Python. Существует
множество отличных источников информации, с помощью которых вы сможете
расширить свои познания в этом вопросе. Я отмечу здесь несколько, на мой взгляд,
наиболее полезных.
 Машинное обучение ( https://www.coursera.org/learn/machine-learning).  Этот бесплат-
ный онлайн-курс, преподаваемый Эндрю Энгом из проекта Coursera, представ -
ляет собой исключительно ясно изложенный материал по основам машинного
обучения с алгоритмической точки зрения. Он предполагает знания математики
и программирования на уровне старших курсов университета и последовательно
и подробно обсуждает некоторые из наиболее важных алгоритмов машинного
обучения. Алгоритмически ранжированные домашние задания позволят вам
реализовать некоторые из этих моделей самостоятельно.
 Книга Pattern Recognition and Machine Learning ( «Распознавание образов и машин-
ное обучение», https://www.springer.com/us/book/978 03873 10732) . Написанная Кри -
стофером Бишопом, эта классическая книга предназначена для специалистов.
Она охватывает во всех подробностях рассмотренные в данной главе понятия
машинного обучения. Если вы хотите продвинуться в вопросе дальше, вам не
обойтись без этой книги на полке.
 Machine Learning: A Probabilistic Perspective  («Машинное обучение: вероятностная
точка зрения», https://mitpress.mit.edu/books/machine-learning-0). В пособии уровня
выпускников университета, написанном Кевином Мерфи, исследуются прак -
тически все важные алгоритмы машинного обучения с единой вероятностной
точки зрения.

\1давнишний пользователь и разработчик стека научных ин-
струментов языка Python. В настоящее время он является руководителем группы
по междисциплинарным исследованиям Вашингтонского университета, занима -
ется собственными астрономическими исследованиями, а также консультирует
и консультируется с учеными в разнообразных областях науки.

============================================================
СТРАНИЦА 574
============================================================
 Дж. Вандер Плас
Python	для	сложных	задач:
наука	о	данных	и	машинное	обучение

\1Книги печатные профессиональные, технические и научные.

\1mail: marketing@chpk.ru

\187